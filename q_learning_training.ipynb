{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "\n",
    "env_config = {\n",
    "    \"fuel_limit\": 5000\n",
    "}\n",
    "render = False\n",
    "hyperparameters = {\n",
    "    \"alpha\": 0.001,\n",
    "\t\"gamma\": 0.99,\n",
    "\t\"epsilon_start\": 1.0, \n",
    "\t\"epsilon_end\": 0.1,\n",
    "\t\"decay_rate\": 0.9999,\n",
    "\t\"episodes\": 10000,\n",
    " \t\"max_steps\": 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20/3000, Avg Reward: -15969.9450, Epsilon: 0.980\n",
      "72\n",
      "Episode 40/3000, Avg Reward: -14950.1600, Epsilon: 0.961\n",
      "72\n",
      "Episode 60/3000, Avg Reward: -17227.2950, Epsilon: 0.942\n",
      "72\n",
      "Episode 80/3000, Avg Reward: -10645.5400, Epsilon: 0.923\n",
      "72\n",
      "Episode 100/3000, Avg Reward: -15481.8200, Epsilon: 0.905\n",
      "72\n",
      "Episode 120/3000, Avg Reward: -14976.8300, Epsilon: 0.887\n",
      "72\n",
      "Episode 140/3000, Avg Reward: -16638.0800, Epsilon: 0.869\n",
      "72\n",
      "Episode 160/3000, Avg Reward: -16447.6900, Epsilon: 0.852\n",
      "72\n",
      "Episode 180/3000, Avg Reward: -16516.0600, Epsilon: 0.835\n",
      "72\n",
      "Episode 200/3000, Avg Reward: -12937.3650, Epsilon: 0.819\n",
      "72\n",
      "Episode 220/3000, Avg Reward: -15868.6200, Epsilon: 0.802\n",
      "72\n",
      "Episode 240/3000, Avg Reward: -17395.5700, Epsilon: 0.787\n",
      "72\n",
      "Episode 260/3000, Avg Reward: -14709.8700, Epsilon: 0.771\n",
      "72\n",
      "Episode 280/3000, Avg Reward: -16777.3050, Epsilon: 0.756\n",
      "72\n",
      "Episode 300/3000, Avg Reward: -15717.0450, Epsilon: 0.741\n",
      "72\n",
      "Episode 320/3000, Avg Reward: -16176.1650, Epsilon: 0.726\n",
      "72\n",
      "Episode 340/3000, Avg Reward: -16595.7350, Epsilon: 0.712\n",
      "72\n",
      "Episode 360/3000, Avg Reward: -16181.6250, Epsilon: 0.698\n",
      "72\n",
      "Episode 380/3000, Avg Reward: -17594.0550, Epsilon: 0.684\n",
      "72\n",
      "Episode 400/3000, Avg Reward: -15217.6150, Epsilon: 0.670\n",
      "72\n",
      "Episode 420/3000, Avg Reward: -15173.7450, Epsilon: 0.657\n",
      "72\n",
      "Episode 440/3000, Avg Reward: -14799.8800, Epsilon: 0.644\n",
      "72\n",
      "Episode 460/3000, Avg Reward: -15943.2100, Epsilon: 0.631\n",
      "72\n",
      "Episode 480/3000, Avg Reward: -15981.4550, Epsilon: 0.619\n",
      "72\n",
      "Episode 500/3000, Avg Reward: -16391.2550, Epsilon: 0.606\n",
      "72\n",
      "Episode 520/3000, Avg Reward: -15108.0500, Epsilon: 0.594\n",
      "72\n",
      "Episode 540/3000, Avg Reward: -13752.8750, Epsilon: 0.583\n",
      "72\n",
      "Episode 560/3000, Avg Reward: -15360.6700, Epsilon: 0.571\n",
      "72\n",
      "Episode 580/3000, Avg Reward: -15551.7950, Epsilon: 0.560\n",
      "72\n",
      "Episode 600/3000, Avg Reward: -15368.2800, Epsilon: 0.549\n",
      "72\n",
      "Episode 620/3000, Avg Reward: -15398.1500, Epsilon: 0.538\n",
      "72\n",
      "Episode 640/3000, Avg Reward: -13940.3900, Epsilon: 0.527\n",
      "72\n",
      "Episode 660/3000, Avg Reward: -14468.8900, Epsilon: 0.517\n",
      "72\n",
      "Episode 680/3000, Avg Reward: -15656.0000, Epsilon: 0.506\n",
      "72\n",
      "Episode 700/3000, Avg Reward: -14413.7750, Epsilon: 0.496\n",
      "72\n",
      "Episode 720/3000, Avg Reward: -12330.6600, Epsilon: 0.487\n",
      "72\n",
      "Episode 740/3000, Avg Reward: -13410.3700, Epsilon: 0.477\n",
      "72\n",
      "Episode 760/3000, Avg Reward: -13149.3750, Epsilon: 0.467\n",
      "72\n",
      "Episode 780/3000, Avg Reward: -14893.3750, Epsilon: 0.458\n",
      "72\n",
      "Episode 800/3000, Avg Reward: -13721.2450, Epsilon: 0.449\n",
      "72\n",
      "Episode 820/3000, Avg Reward: -14201.3200, Epsilon: 0.440\n",
      "72\n",
      "Episode 840/3000, Avg Reward: -13543.6800, Epsilon: 0.432\n",
      "72\n",
      "Episode 860/3000, Avg Reward: -13732.5400, Epsilon: 0.423\n",
      "72\n",
      "Episode 880/3000, Avg Reward: -14625.2500, Epsilon: 0.415\n",
      "72\n",
      "Episode 900/3000, Avg Reward: -14364.2500, Epsilon: 0.406\n",
      "72\n",
      "Episode 920/3000, Avg Reward: -14007.7800, Epsilon: 0.398\n",
      "72\n",
      "Episode 940/3000, Avg Reward: -14309.0000, Epsilon: 0.390\n",
      "72\n",
      "Episode 960/3000, Avg Reward: -12357.0750, Epsilon: 0.383\n",
      "72\n",
      "Episode 980/3000, Avg Reward: -12818.3750, Epsilon: 0.375\n",
      "72\n",
      "Episode 1000/3000, Avg Reward: -12370.0550, Epsilon: 0.368\n",
      "72\n",
      "Episode 1020/3000, Avg Reward: -12932.4050, Epsilon: 0.360\n",
      "72\n",
      "Episode 1040/3000, Avg Reward: -12896.9850, Epsilon: 0.353\n",
      "72\n",
      "Episode 1060/3000, Avg Reward: -13845.0000, Epsilon: 0.346\n",
      "72\n",
      "Episode 1080/3000, Avg Reward: -12357.3900, Epsilon: 0.339\n",
      "72\n",
      "Episode 1100/3000, Avg Reward: -13444.7050, Epsilon: 0.333\n",
      "72\n",
      "Episode 1120/3000, Avg Reward: -13081.1700, Epsilon: 0.326\n",
      "72\n",
      "Episode 1140/3000, Avg Reward: -12961.2300, Epsilon: 0.320\n",
      "72\n",
      "Episode 1160/3000, Avg Reward: -13028.7500, Epsilon: 0.313\n",
      "72\n",
      "Episode 1180/3000, Avg Reward: -12420.0950, Epsilon: 0.307\n",
      "72\n",
      "Episode 1200/3000, Avg Reward: -12334.6750, Epsilon: 0.301\n",
      "72\n",
      "Episode 1220/3000, Avg Reward: -12481.4000, Epsilon: 0.295\n",
      "72\n",
      "Episode 1240/3000, Avg Reward: -12777.8950, Epsilon: 0.289\n",
      "72\n",
      "Episode 1260/3000, Avg Reward: -12437.3700, Epsilon: 0.283\n",
      "72\n",
      "Episode 1280/3000, Avg Reward: -11847.8950, Epsilon: 0.278\n",
      "72\n",
      "Episode 1300/3000, Avg Reward: -11761.4150, Epsilon: 0.272\n",
      "72\n",
      "Episode 1320/3000, Avg Reward: -12582.2500, Epsilon: 0.267\n",
      "72\n",
      "Episode 1340/3000, Avg Reward: -12368.1500, Epsilon: 0.262\n",
      "72\n",
      "Episode 1360/3000, Avg Reward: -11634.0250, Epsilon: 0.256\n",
      "72\n",
      "Episode 1380/3000, Avg Reward: -10916.2800, Epsilon: 0.251\n",
      "72\n",
      "Episode 1400/3000, Avg Reward: -10578.3950, Epsilon: 0.246\n",
      "72\n",
      "Episode 1420/3000, Avg Reward: -12317.2500, Epsilon: 0.242\n",
      "72\n",
      "Episode 1440/3000, Avg Reward: -11798.2150, Epsilon: 0.237\n",
      "72\n",
      "Episode 1460/3000, Avg Reward: -10396.8550, Epsilon: 0.232\n",
      "72\n",
      "Episode 1480/3000, Avg Reward: -11948.3500, Epsilon: 0.227\n",
      "72\n",
      "Episode 1500/3000, Avg Reward: -10675.3450, Epsilon: 0.223\n",
      "72\n",
      "Episode 1520/3000, Avg Reward: -11438.9300, Epsilon: 0.219\n",
      "72\n",
      "Episode 1540/3000, Avg Reward: -10940.3000, Epsilon: 0.214\n",
      "72\n",
      "Episode 1560/3000, Avg Reward: -11396.2500, Epsilon: 0.210\n",
      "72\n",
      "Episode 1580/3000, Avg Reward: -11841.2500, Epsilon: 0.206\n",
      "72\n",
      "Episode 1600/3000, Avg Reward: -11019.6550, Epsilon: 0.202\n",
      "72\n",
      "Episode 1620/3000, Avg Reward: -11041.7500, Epsilon: 0.198\n",
      "72\n",
      "Episode 1640/3000, Avg Reward: -10690.9900, Epsilon: 0.194\n",
      "72\n",
      "Episode 1660/3000, Avg Reward: -11554.7500, Epsilon: 0.190\n",
      "72\n",
      "Episode 1680/3000, Avg Reward: -10468.3450, Epsilon: 0.186\n",
      "72\n",
      "Episode 1700/3000, Avg Reward: -11400.7300, Epsilon: 0.183\n",
      "72\n",
      "Episode 1720/3000, Avg Reward: -10871.4850, Epsilon: 0.179\n",
      "72\n",
      "Episode 1740/3000, Avg Reward: -11528.0000, Epsilon: 0.175\n",
      "72\n",
      "Episode 1760/3000, Avg Reward: -10570.3600, Epsilon: 0.172\n",
      "72\n",
      "Episode 1780/3000, Avg Reward: -11191.2500, Epsilon: 0.168\n",
      "72\n",
      "Episode 1800/3000, Avg Reward: -11031.5750, Epsilon: 0.165\n",
      "72\n",
      "Episode 1820/3000, Avg Reward: -10336.7500, Epsilon: 0.162\n",
      "72\n",
      "Episode 1840/3000, Avg Reward: -10965.9850, Epsilon: 0.159\n",
      "72\n",
      "Episode 1860/3000, Avg Reward: -11228.2500, Epsilon: 0.156\n",
      "72\n",
      "Episode 1880/3000, Avg Reward: -10166.3750, Epsilon: 0.152\n",
      "72\n",
      "Episode 1900/3000, Avg Reward: -10267.1700, Epsilon: 0.149\n",
      "72\n",
      "Episode 1920/3000, Avg Reward: -11018.0000, Epsilon: 0.146\n",
      "72\n",
      "Episode 1940/3000, Avg Reward: -9703.8800, Epsilon: 0.144\n",
      "72\n",
      "Episode 1960/3000, Avg Reward: -10648.2500, Epsilon: 0.141\n",
      "72\n",
      "Episode 1980/3000, Avg Reward: -10623.0000, Epsilon: 0.138\n",
      "72\n",
      "Episode 2000/3000, Avg Reward: -10607.2500, Epsilon: 0.135\n",
      "72\n",
      "Episode 2020/3000, Avg Reward: -10461.7500, Epsilon: 0.133\n",
      "72\n",
      "Episode 2040/3000, Avg Reward: -9044.8600, Epsilon: 0.130\n",
      "72\n",
      "Episode 2060/3000, Avg Reward: -9770.7800, Epsilon: 0.127\n",
      "72\n",
      "Episode 2080/3000, Avg Reward: -10780.2500, Epsilon: 0.125\n",
      "72\n",
      "Episode 2100/3000, Avg Reward: -9851.3500, Epsilon: 0.122\n",
      "72\n",
      "Episode 2120/3000, Avg Reward: -10454.5050, Epsilon: 0.120\n",
      "72\n",
      "Episode 2140/3000, Avg Reward: -8926.1950, Epsilon: 0.118\n",
      "72\n",
      "Episode 2160/3000, Avg Reward: -10707.4550, Epsilon: 0.115\n",
      "72\n",
      "Episode 2180/3000, Avg Reward: -10320.0000, Epsilon: 0.113\n",
      "72\n",
      "Episode 2200/3000, Avg Reward: -10533.1000, Epsilon: 0.111\n",
      "72\n",
      "Episode 2220/3000, Avg Reward: -10459.5000, Epsilon: 0.108\n",
      "72\n",
      "Episode 2240/3000, Avg Reward: -10223.5000, Epsilon: 0.106\n",
      "72\n",
      "Episode 2260/3000, Avg Reward: -10328.1950, Epsilon: 0.104\n",
      "72\n",
      "Episode 2280/3000, Avg Reward: -10027.7900, Epsilon: 0.102\n",
      "72\n",
      "Episode 2300/3000, Avg Reward: -9929.7550, Epsilon: 0.100\n",
      "72\n",
      "Episode 2320/3000, Avg Reward: -10191.5000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2340/3000, Avg Reward: -9530.2500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2360/3000, Avg Reward: -10374.4650, Epsilon: 0.100\n",
      "72\n",
      "Episode 2380/3000, Avg Reward: -9793.7600, Epsilon: 0.100\n",
      "72\n",
      "Episode 2400/3000, Avg Reward: -10148.2500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2420/3000, Avg Reward: -9766.8750, Epsilon: 0.100\n",
      "72\n",
      "Episode 2440/3000, Avg Reward: -9500.9250, Epsilon: 0.100\n",
      "72\n",
      "Episode 2460/3000, Avg Reward: -9696.9450, Epsilon: 0.100\n",
      "72\n",
      "Episode 2480/3000, Avg Reward: -9809.3150, Epsilon: 0.100\n",
      "72\n",
      "Episode 2500/3000, Avg Reward: -9863.1050, Epsilon: 0.100\n",
      "72\n",
      "Episode 2520/3000, Avg Reward: -9880.7500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2540/3000, Avg Reward: -10364.3100, Epsilon: 0.100\n",
      "72\n",
      "Episode 2560/3000, Avg Reward: -10418.7600, Epsilon: 0.100\n",
      "72\n",
      "Episode 2580/3000, Avg Reward: -9964.3550, Epsilon: 0.100\n",
      "72\n",
      "Episode 2600/3000, Avg Reward: -10411.7500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2620/3000, Avg Reward: -10105.9450, Epsilon: 0.100\n",
      "72\n",
      "Episode 2640/3000, Avg Reward: -8809.7550, Epsilon: 0.100\n",
      "72\n",
      "Episode 2660/3000, Avg Reward: -10219.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2680/3000, Avg Reward: -9034.5500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2700/3000, Avg Reward: -10106.2500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2720/3000, Avg Reward: -8687.7500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2740/3000, Avg Reward: -9384.3550, Epsilon: 0.100\n",
      "72\n",
      "Episode 2760/3000, Avg Reward: -10332.2500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2780/3000, Avg Reward: -9531.7850, Epsilon: 0.100\n",
      "72\n",
      "Episode 2800/3000, Avg Reward: -10048.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2820/3000, Avg Reward: -10174.5000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2840/3000, Avg Reward: -9996.7500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2860/3000, Avg Reward: -9637.3050, Epsilon: 0.100\n",
      "72\n",
      "Episode 2880/3000, Avg Reward: -10393.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2900/3000, Avg Reward: -8121.3900, Epsilon: 0.100\n",
      "72\n",
      "Episode 2920/3000, Avg Reward: -10083.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2940/3000, Avg Reward: -10387.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2960/3000, Avg Reward: -9652.5050, Epsilon: 0.100\n",
      "72\n",
      "Episode 2980/3000, Avg Reward: -9809.1700, Epsilon: 0.100\n",
      "72\n",
      "Episode 3000/3000, Avg Reward: -9637.0650, Epsilon: 0.100\n",
      "72\n",
      "Q-table 已儲存至 q_table.pkl\n"
     ]
    }
   ],
   "source": [
    "q_table = {}\n",
    "epsilon = hyperparameters[\"epsilon_start\"]\n",
    "\n",
    "def get_state(obs, target_loc=None):\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col, stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\t\n",
    "\n",
    "\tassert target_loc is not None\n",
    "\tx_dir = target_loc[0] - taxi_row\n",
    "\ty_dir = target_loc[1] - taxi_col\n",
    "\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\treturn (x_dir, y_dir, obstacle_north, obstacle_south, obstacle_east, obstacle_west, has_picked_up)\n",
    "  \n",
    "def get_action(obs):\n",
    "\t\"\"\"\n",
    "\t# Selects the best action using the trained Q-table.\n",
    "\t\"\"\"\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = np.random.choice(action_nums)  # Random action\n",
    "\telse:\n",
    "\t\taction = np.argmax(q_table[get_state(obs)])  # Greedy action\n",
    "\treturn action\n",
    "\n",
    "def is_in_station(obs):\n",
    "\t\"\"\"\n",
    "\t# Checks if the taxi is in a station.\n",
    "\t\"\"\"\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col,stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\n",
    "\treturn (taxi_row, taxi_col) in stations\n",
    "\n",
    "env = SimpleTaxiEnv(**env_config)\n",
    "action_nums = 6\n",
    "rewards_per_episode = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "step_count = 0\n",
    "stations = [(0, 0), (0, 4), (4, 0), (4,4)]\n",
    "\n",
    "taxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\n",
    "if render:\n",
    "\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\taction=None, step=step_count, fuel=env.current_fuel)\n",
    "\ttime.sleep(0.5)\n",
    " \n",
    "\n",
    "for episode in range(hyperparameters[\"episodes\"]):\n",
    "\tget_state.passenger_loc, get_state.destination_loc = None, None\n",
    "\tobs, _ = env.reset()\n",
    "\tdone = False\n",
    "\ttotal_reward = 0\n",
    "\tstep_count = 0\n",
    " \n",
    "\tdestination = None\n",
    "\tvisited = []\n",
    "\thas_picked_up = False\n",
    " \n",
    "\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [(obs[2], obs[3]), (obs[4], obs[5]), (obs[6], obs[7]), (obs[8], obs[9])]\n",
    "\ttarget_loc = stations[0]\n",
    "\tstate = get_state(obs, target_loc)\n",
    "\t\n",
    "\twhile not done:\t\n",
    "\t\tif state not in q_table:\n",
    "\t\t\tq_table[state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\t\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\t\taction_probs = np.ones(action_nums) / action_nums\n",
    "\t\t\t# if obstacle_south:\n",
    "\t\t\t# \taction_probs[0] = 0\n",
    "\t\t\t# if obstacle_north:\n",
    "\t\t\t# \taction_probs[1] = 0\n",
    "\t\t\t# if obstacle_east:\n",
    "\t\t\t# \taction_probs[2] = 0\n",
    "\t\t\t# if obstacle_west:\n",
    "\t\t\t# \taction_probs[3] = 0\t\n",
    "\t\t\t# if not passenger_look or has_picked_up or not is_in_station(obs):\n",
    "\t\t\t# \taction_probs[4] = 0\n",
    "\t\t\t# if not destination_look or not has_picked_up or not is_in_station(obs):\n",
    "\t\t\t# \taction_probs[5] = 0\n",
    "\t\t\t# action_probs = action_probs / np.sum(action_probs)\n",
    "\t\t\taction = np.random.choice(action_nums, p=action_probs)  # Random action\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(q_table[state])  # Greedy action\n",
    "\t\t\n",
    "\t\tif not has_picked_up and passenger_look and is_in_station(obs) and action == 4:\n",
    "\t\t\thas_picked_up = True\n",
    "\t\tif has_picked_up and destination_look and is_in_station(obs) and action == 5:\n",
    "\t\t\tdone = True\n",
    "   \n",
    "\t\tshaped_reward = 0\n",
    "\t\tx_dir = target_loc[0] - taxi_row\n",
    "\t\ty_dir = target_loc[1] - taxi_col\n",
    "\t\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\t\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\t\tif x_dir == 1 and action == 1 and not obstacle_north:\n",
    "\t\t\tshaped_reward = 1000\n",
    "\t\tif x_dir == -1 and action == 0 and not obstacle_south:\n",
    "\t\t\tshaped_reward = 1000\n",
    "\t\tif y_dir == 1 and action == 3 and not obstacle_west:\n",
    "\t\t\tshaped_reward = 1000\n",
    "\t\tif y_dir == -1 and action == 2 and not obstacle_east:\n",
    "\t\t\tshaped_reward = 1000\n",
    "\t\tif action == 4 and passenger_look and not has_picked_up:\n",
    "\t\t\tshaped_reward = 5000\n",
    "\t\tif action == 5 and destination_look and has_picked_up:\n",
    "\t\t\tshaped_reward = 10000\n",
    "\t\tif obstacle_south and action == 0:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\tif obstacle_north and action == 1:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\tif obstacle_east and action == 2:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\tif obstacle_west and action == 3:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\tif (not passenger_look or has_picked_up or not is_in_station(obs)) and action == 4:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\tif (not destination_look or not has_picked_up or not is_in_station(obs)) and action == 5:\n",
    "\t\t\tshaped_reward = -10000\n",
    "   \n",
    "\t\tnext_obs, reward, done, _ = env.step(action)\n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = next_obs\n",
    "  \n",
    "\t\tif is_in_station(next_obs) and (taxi_row, taxi_col) not in visited:\n",
    "\t\t\tvisited.append((taxi_row, taxi_col))\n",
    "\t\t\tif destination_look:\n",
    "\t\t\t\tdestination = (taxi_row, taxi_col)\n",
    "\t\t\tif has_picked_up and destination is not None:\n",
    "\t\t\t\ttarget_loc = destination\n",
    "\t\t\telse:\n",
    "\t\t\t\t# choose a station that has not been visited yet\n",
    "\t\t\t\t# print (visited, (taxi_row, taxi_col))\n",
    "\t\t\t\tfor station in stations:\n",
    "\t\t\t\t\tif station not in visited:\n",
    "\t\t\t\t\t\ttarget_loc = station\n",
    "\t\t\t\t\t\tbreak\t\n",
    "\t\t\t\t# print (target_loc)\n",
    "\n",
    "\t\t# if done:\n",
    "\t\t# \tif not has_picked_up:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not picked up\", next_obs, action, has_picked_up, destination)\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and not destination_look:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not destination\")\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and destination_look:\n",
    "\t\t# \t\t# print (\"done\")\n",
    "\t\t# \t\tshaped_reward = 2000\n",
    "\t\t\n",
    "\t\ttotal_reward += reward\n",
    "\t\treward += shaped_reward\n",
    "\t\tif next_obs not in q_table:\n",
    "\t\t\tnext_state = get_state(next_obs, target_loc)\n",
    "\t\t\tq_table[next_state] = np.zeros(action_nums)\n",
    "\t\tq_table[state][action] += hyperparameters[\"alpha\"] * (reward + hyperparameters[\"gamma\"] * np.max(q_table[next_state]) - q_table[state][action])\n",
    "\t\t\n",
    "\t\tstep_count += 1\n",
    "\t\tobs = next_obs\n",
    "\t\tstate = next_state\n",
    "   \n",
    "\t\n",
    "\t\tif render:\n",
    "\t\t\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\t\t\taction=action, step=step_count, fuel=env.current_fuel)\n",
    "\t# print (step_count)\n",
    "\trewards_per_episode.append(total_reward)\n",
    "\tepsilon = max(hyperparameters[\"epsilon_end\"], epsilon * hyperparameters[\"decay_rate\"])\n",
    "\tif (episode + 1) % 20 == 0:\n",
    "\t\tavg_reward = np.mean(rewards_per_episode[-20:])\n",
    "\t\tprint(f'Episode {episode + 1}/{hyperparameters[\"episodes\"]}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.3f}')\n",
    "\t\t# print ([np.argmax(i) for i in q_table.values()])\n",
    "\t\tprint (len(q_table))\n",
    "  \n",
    "filename = \"q_table.pkl\"\n",
    "\n",
    "# 儲存 Q-table\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "print(f\"Q-table 已儲存至 {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
