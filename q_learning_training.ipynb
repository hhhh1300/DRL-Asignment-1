{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "\n",
    "env_config = {\n",
    "    \"fuel_limit\": 5000\n",
    "}\n",
    "render = False\n",
    "hyperparameters = {\n",
    "    \"alpha\": 0.1,\n",
    "\t\"gamma\": 0.99,\n",
    "\t\"epsilon_start\": 1.0, \n",
    "\t\"epsilon_end\": 0.01,\n",
    "\t\"decay_rate\": 0.9995,\n",
    "\t\"episodes\": 10000,\n",
    " \t\"max_steps\": 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20/10000, Avg Reward: -12749.0400, Epsilon: 0.990\n",
      "Episode 40/10000, Avg Reward: -15747.1600, Epsilon: 0.980\n",
      "Episode 60/10000, Avg Reward: -14763.0850, Epsilon: 0.970\n",
      "Episode 80/10000, Avg Reward: -12597.1100, Epsilon: 0.961\n",
      "Episode 100/10000, Avg Reward: -12570.9650, Epsilon: 0.951\n",
      "Episode 120/10000, Avg Reward: -10800.2000, Epsilon: 0.942\n",
      "Episode 140/10000, Avg Reward: -11425.7700, Epsilon: 0.932\n",
      "Episode 160/10000, Avg Reward: -11538.5100, Epsilon: 0.923\n",
      "Episode 180/10000, Avg Reward: -14182.6400, Epsilon: 0.914\n",
      "Episode 200/10000, Avg Reward: -12031.8200, Epsilon: 0.905\n",
      "Episode 220/10000, Avg Reward: -10902.6550, Epsilon: 0.896\n",
      "Episode 240/10000, Avg Reward: -11099.9950, Epsilon: 0.887\n",
      "Episode 260/10000, Avg Reward: -10231.4200, Epsilon: 0.878\n",
      "Episode 280/10000, Avg Reward: -12626.4750, Epsilon: 0.869\n",
      "Episode 300/10000, Avg Reward: -12458.9750, Epsilon: 0.861\n",
      "Episode 320/10000, Avg Reward: -14195.2300, Epsilon: 0.852\n",
      "Episode 340/10000, Avg Reward: -10538.0300, Epsilon: 0.844\n",
      "Episode 360/10000, Avg Reward: -12019.1000, Epsilon: 0.835\n",
      "Episode 380/10000, Avg Reward: -10105.6850, Epsilon: 0.827\n",
      "Episode 400/10000, Avg Reward: -12416.5800, Epsilon: 0.819\n",
      "Episode 420/10000, Avg Reward: -12098.2350, Epsilon: 0.811\n",
      "Episode 440/10000, Avg Reward: -10150.1200, Epsilon: 0.802\n",
      "Episode 460/10000, Avg Reward: -10514.5150, Epsilon: 0.794\n",
      "Episode 480/10000, Avg Reward: -9645.0900, Epsilon: 0.787\n",
      "Episode 500/10000, Avg Reward: -11706.9950, Epsilon: 0.779\n",
      "Episode 520/10000, Avg Reward: -9471.9350, Epsilon: 0.771\n",
      "Episode 540/10000, Avg Reward: -12960.7300, Epsilon: 0.763\n",
      "Episode 560/10000, Avg Reward: -13219.0100, Epsilon: 0.756\n",
      "Episode 580/10000, Avg Reward: -13474.1600, Epsilon: 0.748\n",
      "Episode 600/10000, Avg Reward: -13558.6200, Epsilon: 0.741\n",
      "Episode 620/10000, Avg Reward: -13991.4350, Epsilon: 0.733\n",
      "Episode 640/10000, Avg Reward: -14792.8550, Epsilon: 0.726\n",
      "Episode 660/10000, Avg Reward: -12192.7000, Epsilon: 0.719\n",
      "Episode 680/10000, Avg Reward: -13494.1200, Epsilon: 0.712\n",
      "Episode 700/10000, Avg Reward: -14575.3400, Epsilon: 0.705\n",
      "Episode 720/10000, Avg Reward: -11089.7150, Epsilon: 0.698\n",
      "Episode 740/10000, Avg Reward: -16855.1350, Epsilon: 0.691\n",
      "Episode 760/10000, Avg Reward: -16389.5050, Epsilon: 0.684\n",
      "Episode 780/10000, Avg Reward: -15186.6450, Epsilon: 0.677\n",
      "Episode 800/10000, Avg Reward: -13409.8950, Epsilon: 0.670\n",
      "Episode 820/10000, Avg Reward: -14475.9300, Epsilon: 0.664\n",
      "Episode 840/10000, Avg Reward: -12855.6150, Epsilon: 0.657\n",
      "Episode 860/10000, Avg Reward: -16521.2450, Epsilon: 0.650\n",
      "Episode 880/10000, Avg Reward: -17754.8200, Epsilon: 0.644\n",
      "Episode 900/10000, Avg Reward: -13619.9900, Epsilon: 0.638\n",
      "Episode 920/10000, Avg Reward: -12433.7150, Epsilon: 0.631\n",
      "Episode 940/10000, Avg Reward: -15331.4500, Epsilon: 0.625\n",
      "Episode 960/10000, Avg Reward: -15633.7200, Epsilon: 0.619\n",
      "Episode 980/10000, Avg Reward: -15360.4150, Epsilon: 0.613\n",
      "Episode 1000/10000, Avg Reward: -14533.5900, Epsilon: 0.606\n",
      "Episode 1020/10000, Avg Reward: -14396.2200, Epsilon: 0.600\n",
      "Episode 1040/10000, Avg Reward: -14773.1900, Epsilon: 0.594\n",
      "Episode 1060/10000, Avg Reward: -16656.3650, Epsilon: 0.589\n",
      "Episode 1080/10000, Avg Reward: -19375.1600, Epsilon: 0.583\n",
      "Episode 1100/10000, Avg Reward: -15949.5300, Epsilon: 0.577\n",
      "Episode 1120/10000, Avg Reward: -15561.0450, Epsilon: 0.571\n",
      "Episode 1140/10000, Avg Reward: -19455.7200, Epsilon: 0.565\n",
      "Episode 1160/10000, Avg Reward: -14103.3500, Epsilon: 0.560\n",
      "Episode 1180/10000, Avg Reward: -15600.3450, Epsilon: 0.554\n",
      "Episode 1200/10000, Avg Reward: -17052.6050, Epsilon: 0.549\n",
      "Episode 1220/10000, Avg Reward: -16881.7050, Epsilon: 0.543\n",
      "Episode 1240/10000, Avg Reward: -17770.5050, Epsilon: 0.538\n",
      "Episode 1260/10000, Avg Reward: -18135.9800, Epsilon: 0.533\n",
      "Episode 1280/10000, Avg Reward: -15702.6750, Epsilon: 0.527\n",
      "Episode 1300/10000, Avg Reward: -16518.6500, Epsilon: 0.522\n",
      "Episode 1320/10000, Avg Reward: -17745.4750, Epsilon: 0.517\n",
      "Episode 1340/10000, Avg Reward: -13952.3050, Epsilon: 0.512\n",
      "Episode 1360/10000, Avg Reward: -17356.1650, Epsilon: 0.507\n",
      "Episode 1380/10000, Avg Reward: -16065.9950, Epsilon: 0.501\n",
      "Episode 1400/10000, Avg Reward: -15109.0650, Epsilon: 0.496\n",
      "Episode 1420/10000, Avg Reward: -19823.4950, Epsilon: 0.492\n",
      "Episode 1440/10000, Avg Reward: -14828.8900, Epsilon: 0.487\n",
      "Episode 1460/10000, Avg Reward: -17353.7050, Epsilon: 0.482\n",
      "Episode 1480/10000, Avg Reward: -15862.0950, Epsilon: 0.477\n",
      "Episode 1500/10000, Avg Reward: -12906.7000, Epsilon: 0.472\n",
      "Episode 1520/10000, Avg Reward: -18942.1950, Epsilon: 0.468\n",
      "Episode 1540/10000, Avg Reward: -18575.4150, Epsilon: 0.463\n",
      "Episode 1560/10000, Avg Reward: -17334.0950, Epsilon: 0.458\n",
      "Episode 1580/10000, Avg Reward: -17019.5100, Epsilon: 0.454\n",
      "Episode 1600/10000, Avg Reward: -21768.0700, Epsilon: 0.449\n",
      "Episode 1620/10000, Avg Reward: -16572.1400, Epsilon: 0.445\n",
      "Episode 1640/10000, Avg Reward: -19031.8850, Epsilon: 0.440\n",
      "Episode 1660/10000, Avg Reward: -15907.1300, Epsilon: 0.436\n",
      "Episode 1680/10000, Avg Reward: -19020.5400, Epsilon: 0.432\n",
      "Episode 1700/10000, Avg Reward: -13615.2950, Epsilon: 0.427\n",
      "Episode 1720/10000, Avg Reward: -21588.3550, Epsilon: 0.423\n",
      "Episode 1740/10000, Avg Reward: -15734.4900, Epsilon: 0.419\n",
      "Episode 1760/10000, Avg Reward: -19032.6600, Epsilon: 0.415\n",
      "Episode 1780/10000, Avg Reward: -17876.3150, Epsilon: 0.411\n",
      "Episode 1800/10000, Avg Reward: -18972.4600, Epsilon: 0.406\n",
      "Episode 1820/10000, Avg Reward: -15330.5450, Epsilon: 0.402\n",
      "Episode 1840/10000, Avg Reward: -18536.8350, Epsilon: 0.398\n",
      "Episode 1860/10000, Avg Reward: -14865.7100, Epsilon: 0.394\n",
      "Episode 1880/10000, Avg Reward: -15450.3250, Epsilon: 0.391\n",
      "Episode 1900/10000, Avg Reward: -14494.8200, Epsilon: 0.387\n",
      "Episode 1920/10000, Avg Reward: -18123.0750, Epsilon: 0.383\n",
      "Episode 1940/10000, Avg Reward: -15151.9850, Epsilon: 0.379\n",
      "Episode 1960/10000, Avg Reward: -16994.1250, Epsilon: 0.375\n",
      "Episode 1980/10000, Avg Reward: -18198.0750, Epsilon: 0.371\n",
      "Episode 2000/10000, Avg Reward: -19951.2600, Epsilon: 0.368\n",
      "Episode 2020/10000, Avg Reward: -14091.4950, Epsilon: 0.364\n",
      "Episode 2040/10000, Avg Reward: -18729.3400, Epsilon: 0.361\n",
      "Episode 2060/10000, Avg Reward: -16890.6150, Epsilon: 0.357\n",
      "Episode 2080/10000, Avg Reward: -21540.9250, Epsilon: 0.353\n",
      "Episode 2100/10000, Avg Reward: -17519.7350, Epsilon: 0.350\n",
      "Episode 2120/10000, Avg Reward: -16340.5700, Epsilon: 0.346\n",
      "Episode 2140/10000, Avg Reward: -14912.3800, Epsilon: 0.343\n",
      "Episode 2160/10000, Avg Reward: -12942.5600, Epsilon: 0.340\n",
      "Episode 2180/10000, Avg Reward: -17972.0500, Epsilon: 0.336\n",
      "Episode 2200/10000, Avg Reward: -15404.0000, Epsilon: 0.333\n",
      "Episode 2220/10000, Avg Reward: -16490.2800, Epsilon: 0.329\n",
      "Episode 2240/10000, Avg Reward: -17815.9800, Epsilon: 0.326\n",
      "Episode 2260/10000, Avg Reward: -15236.8050, Epsilon: 0.323\n",
      "Episode 2280/10000, Avg Reward: -17111.8900, Epsilon: 0.320\n",
      "Episode 2300/10000, Avg Reward: -10162.2950, Epsilon: 0.317\n",
      "Episode 2320/10000, Avg Reward: -15499.8800, Epsilon: 0.313\n",
      "Episode 2340/10000, Avg Reward: -16828.6050, Epsilon: 0.310\n",
      "Episode 2360/10000, Avg Reward: -16907.3000, Epsilon: 0.307\n",
      "Episode 2380/10000, Avg Reward: -14743.6000, Epsilon: 0.304\n",
      "Episode 2400/10000, Avg Reward: -13567.7800, Epsilon: 0.301\n",
      "Episode 2420/10000, Avg Reward: -20800.4000, Epsilon: 0.298\n",
      "Episode 2440/10000, Avg Reward: -19959.9750, Epsilon: 0.295\n",
      "Episode 2460/10000, Avg Reward: -18608.2650, Epsilon: 0.292\n",
      "Episode 2480/10000, Avg Reward: -14290.6750, Epsilon: 0.289\n",
      "Episode 2500/10000, Avg Reward: -18235.7600, Epsilon: 0.286\n",
      "Episode 2520/10000, Avg Reward: -16008.4200, Epsilon: 0.284\n",
      "Episode 2540/10000, Avg Reward: -17127.9350, Epsilon: 0.281\n",
      "Episode 2560/10000, Avg Reward: -16442.0300, Epsilon: 0.278\n",
      "Episode 2580/10000, Avg Reward: -15896.5350, Epsilon: 0.275\n",
      "Episode 2600/10000, Avg Reward: -15900.9150, Epsilon: 0.272\n",
      "Episode 2620/10000, Avg Reward: -14871.2450, Epsilon: 0.270\n",
      "Episode 2640/10000, Avg Reward: -11703.4350, Epsilon: 0.267\n",
      "Episode 2660/10000, Avg Reward: -17738.7200, Epsilon: 0.264\n",
      "Episode 2680/10000, Avg Reward: -12140.4500, Epsilon: 0.262\n",
      "Episode 2700/10000, Avg Reward: -14936.7150, Epsilon: 0.259\n",
      "Episode 2720/10000, Avg Reward: -10144.9700, Epsilon: 0.257\n",
      "Episode 2740/10000, Avg Reward: -15123.8100, Epsilon: 0.254\n",
      "Episode 2760/10000, Avg Reward: -21420.1850, Epsilon: 0.251\n",
      "Episode 2780/10000, Avg Reward: -16644.3150, Epsilon: 0.249\n",
      "Episode 2800/10000, Avg Reward: -14208.4200, Epsilon: 0.247\n",
      "Episode 2820/10000, Avg Reward: -10973.7650, Epsilon: 0.244\n",
      "Episode 2840/10000, Avg Reward: -12383.3650, Epsilon: 0.242\n",
      "Episode 2860/10000, Avg Reward: -14430.9700, Epsilon: 0.239\n",
      "Episode 2880/10000, Avg Reward: -11134.5000, Epsilon: 0.237\n",
      "Episode 2900/10000, Avg Reward: -13499.1150, Epsilon: 0.234\n",
      "Episode 2920/10000, Avg Reward: -15845.7450, Epsilon: 0.232\n",
      "Episode 2940/10000, Avg Reward: -18795.7800, Epsilon: 0.230\n",
      "Episode 2960/10000, Avg Reward: -17609.7550, Epsilon: 0.228\n",
      "Episode 2980/10000, Avg Reward: -16854.5700, Epsilon: 0.225\n",
      "Episode 3000/10000, Avg Reward: -16242.3650, Epsilon: 0.223\n",
      "Episode 3020/10000, Avg Reward: -10174.0250, Epsilon: 0.221\n",
      "Episode 3040/10000, Avg Reward: -15659.3450, Epsilon: 0.219\n",
      "Episode 3060/10000, Avg Reward: -15702.3000, Epsilon: 0.216\n",
      "Episode 3080/10000, Avg Reward: -17738.0600, Epsilon: 0.214\n",
      "Episode 3100/10000, Avg Reward: -14750.7100, Epsilon: 0.212\n",
      "Episode 3120/10000, Avg Reward: -15296.7150, Epsilon: 0.210\n",
      "Episode 3140/10000, Avg Reward: -15793.6850, Epsilon: 0.208\n",
      "Episode 3160/10000, Avg Reward: -14312.9700, Epsilon: 0.206\n",
      "Episode 3180/10000, Avg Reward: -10012.7550, Epsilon: 0.204\n",
      "Episode 3200/10000, Avg Reward: -15447.1550, Epsilon: 0.202\n",
      "Episode 3220/10000, Avg Reward: -20023.0000, Epsilon: 0.200\n",
      "Episode 3240/10000, Avg Reward: -14600.5050, Epsilon: 0.198\n",
      "Episode 3260/10000, Avg Reward: -16192.3650, Epsilon: 0.196\n",
      "Episode 3280/10000, Avg Reward: -15349.5950, Epsilon: 0.194\n",
      "Episode 3300/10000, Avg Reward: -17889.1800, Epsilon: 0.192\n",
      "Episode 3320/10000, Avg Reward: -9499.1350, Epsilon: 0.190\n",
      "Episode 3340/10000, Avg Reward: -13070.8950, Epsilon: 0.188\n",
      "Episode 3360/10000, Avg Reward: -10442.6450, Epsilon: 0.186\n",
      "Episode 3380/10000, Avg Reward: -11708.1650, Epsilon: 0.184\n",
      "Episode 3400/10000, Avg Reward: -10122.3500, Epsilon: 0.183\n",
      "Episode 3420/10000, Avg Reward: -16589.9850, Epsilon: 0.181\n",
      "Episode 3440/10000, Avg Reward: -10878.6750, Epsilon: 0.179\n",
      "Episode 3460/10000, Avg Reward: -10792.6350, Epsilon: 0.177\n",
      "Episode 3480/10000, Avg Reward: -9181.5600, Epsilon: 0.175\n",
      "Episode 3500/10000, Avg Reward: -10694.4100, Epsilon: 0.174\n",
      "Episode 3520/10000, Avg Reward: -16984.3350, Epsilon: 0.172\n",
      "Episode 3540/10000, Avg Reward: -13805.2050, Epsilon: 0.170\n",
      "Episode 3560/10000, Avg Reward: -12987.2500, Epsilon: 0.169\n",
      "Episode 3580/10000, Avg Reward: -14560.2500, Epsilon: 0.167\n",
      "Episode 3600/10000, Avg Reward: -10308.9850, Epsilon: 0.165\n",
      "Episode 3620/10000, Avg Reward: -13787.9700, Epsilon: 0.164\n",
      "Episode 3640/10000, Avg Reward: -7187.7500, Epsilon: 0.162\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m \taction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(action_nums, p\u001b[38;5;241m=\u001b[39maction_probs)  \u001b[38;5;66;03m# Random action\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m \taction \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Greedy action\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_picked_up \u001b[38;5;129;01mand\u001b[39;00m passenger_look \u001b[38;5;129;01mand\u001b[39;00m is_in_station(obs) \u001b[38;5;129;01mand\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m     94\u001b[0m \thas_picked_up \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\楊中\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\楊中\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_table = {}\n",
    "epsilon = hyperparameters[\"epsilon_start\"]\n",
    "\n",
    "def get_state(obs, target_loc=None, has_picked_up=False):\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col, stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\t\n",
    "\n",
    "\tassert target_loc is not None\n",
    "\tx_dir = target_loc[0] - taxi_row\n",
    "\ty_dir = target_loc[1] - taxi_col\n",
    "\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\treturn (x_dir, y_dir, obstacle_north, obstacle_south, obstacle_east, obstacle_west, has_picked_up)\n",
    "  \n",
    "def get_action(obs):\n",
    "\t\"\"\"\n",
    "\t# Selects the best action using the trained Q-table.\n",
    "\t\"\"\"\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = np.random.choice(action_nums)  # Random action\n",
    "\telse:\n",
    "\t\taction = np.argmax(q_table[get_state(obs)])  # Greedy action\n",
    "\treturn action\n",
    "\n",
    "def is_in_station(obs):\n",
    "\t\"\"\"\n",
    "\t# Checks if the taxi is in a station.\n",
    "\t\"\"\"\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col,stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\n",
    "\treturn (taxi_row, taxi_col) in stations\n",
    "\n",
    "env = SimpleTaxiEnv(**env_config)\n",
    "action_nums = 6\n",
    "rewards_per_episode = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "step_count = 0\n",
    "stations = [(0, 0), (0, 4), (4, 0), (4,4)]\n",
    "\n",
    "taxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\n",
    "if render:\n",
    "\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\taction=None, step=step_count, fuel=env.current_fuel)\n",
    "\ttime.sleep(0.5)\n",
    " \n",
    "\n",
    "for episode in range(hyperparameters[\"episodes\"]):\n",
    "\tget_state.passenger_loc, get_state.destination_loc = None, None\n",
    "\tobs, _ = env.reset()\n",
    "\tdone = False\n",
    "\ttotal_reward = 0\n",
    "\tstep_count = 0\n",
    " \n",
    "\tdestination = None\n",
    "\tvisited = []\n",
    "\thas_picked_up = False\n",
    " \n",
    "\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [(obs[2], obs[3]), (obs[4], obs[5]), (obs[6], obs[7]), (obs[8], obs[9])]\n",
    "\ttarget_loc = stations[0]\n",
    "\tstate = get_state(obs, target_loc, has_picked_up)\n",
    "\t\n",
    "\twhile not done:\t\n",
    "\t\tif state not in q_table:\n",
    "\t\t\tq_table[state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\t\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\t\taction_probs = np.ones(action_nums) / action_nums\n",
    "\t\t\t# if obstacle_south:\n",
    "\t\t\t# \taction_probs[0] = 0\n",
    "\t\t\t# if obstacle_north:\n",
    "\t\t\t# \taction_probs[1] = 0\n",
    "\t\t\t# if obstacle_east:\n",
    "\t\t\t# \taction_probs[2] = 0\n",
    "\t\t\t# if obstacle_west:\n",
    "\t\t\t# \taction_probs[3] = 0\t\n",
    "\t\t\t# # if not passenger_look or has_picked_up or not is_in_station(obs):\n",
    "\t\t\t# action_probs[4] = 0\n",
    "\t\t\t# # if not destination_look or not has_picked_up or not is_in_station(obs):\n",
    "\t\t\t# action_probs[5] = 0\n",
    "\t\t\taction_probs = action_probs / np.sum(action_probs)\n",
    "\t\t\taction = np.random.choice(action_nums, p=action_probs)  # Random action\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(q_table[state])  # Greedy action\n",
    "\t\t\n",
    "\t\tif not has_picked_up and passenger_look and is_in_station(obs) and action == 4:\n",
    "\t\t\thas_picked_up = True\n",
    "\t\tif has_picked_up and destination_look and is_in_station(obs) and action == 5:\n",
    "\t\t\tdone = True\n",
    "   \n",
    "\t\tshaped_reward = 0\n",
    "\t\tx_dir = target_loc[0] - taxi_row\n",
    "\t\ty_dir = target_loc[1] - taxi_col\n",
    "\t\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\t\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\t\t# if action == 0 :  # Move Down\n",
    "        #     next_row += 1\n",
    "        # elif action == 1:  # Move Up\n",
    "        #     next_row -= 1\n",
    "        # elif action == 2:  # Move Right\n",
    "        #     next_col += 1\n",
    "        # elif action == 3:  # Move Left\n",
    "        #     next_col -= 1\n",
    "\t\tif y_dir == 1 and action == 2 and not obstacle_east:\n",
    "\t\t\tshaped_reward = 10\n",
    "\t\tif y_dir == -1 and action == 3 and not obstacle_west:\n",
    "\t\t\tshaped_reward = 10\n",
    "\t\tif x_dir == 1 and action == 0 and not obstacle_south:\n",
    "\t\t\t# print (target_loc, (taxi_row, taxi_col), (x_dir, y_dir), action)\n",
    "\t\t\tshaped_reward = 10\n",
    "\t\tif x_dir == -1 and action == 1 and not obstacle_north:\n",
    "\t\t\tshaped_reward = 10\n",
    "\t\t# if action == 4 and passenger_look and not has_picked_up and is_in_station(obs): \n",
    "\t\t# \tshaped_reward = 50\n",
    "\t\t# if action == 5 and destination_look and has_picked_up and is_in_station(obs):\n",
    "\t\t# \tshaped_reward = 100\n",
    "   \n",
    "\t\tif obstacle_south and action == 0:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"south\")\n",
    "\t\tif obstacle_north and action == 1:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"north\")\n",
    "\t\tif obstacle_east and action == 2:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"east\")\n",
    "\t\tif obstacle_west and action == 3:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"west\")\n",
    "\t\tif (not passenger_look or has_picked_up or not is_in_station(obs)) and action == 4:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"not passenger\")\n",
    "\t\tif (not destination_look or not has_picked_up or not is_in_station(obs)) and action == 5:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"not destination\")\n",
    "   \n",
    "\t\tnext_obs, reward, done, _ = env.step(action)\n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = next_obs\n",
    "\t\t# print (taxi_row, taxi_col)\n",
    "  \n",
    "\t\tif is_in_station(next_obs) and (taxi_row, taxi_col) not in visited:\n",
    "\t\t\tvisited.append((taxi_row, taxi_col))\n",
    "\t\t\tif destination_look:\n",
    "\t\t\t\tdestination = (taxi_row, taxi_col)\n",
    "\t\t\tif has_picked_up and destination is not None:\n",
    "\t\t\t\ttarget_loc = destination\n",
    "\t\t\telse:\n",
    "\t\t\t\t# choose a station that has not been visited yet\n",
    "\t\t\t\t# print (visited, (taxi_row, taxi_col))\n",
    "\t\t\t\tfor station in stations:\n",
    "\t\t\t\t\tif station not in visited:\n",
    "\t\t\t\t\t\ttarget_loc = station\n",
    "\t\t\t\t\t\tbreak\t\n",
    "\t\t\t\t# print (target_loc)\n",
    "\n",
    "\t\t# if done:\n",
    "\t\t# \tif not has_picked_up:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not picked up\", next_obs, action, has_picked_up, destination)\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and not destination_look:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not destination\")\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and destination_look:\n",
    "\t\t# \t\t# print (\"done\")\n",
    "\t\t# \t\tshaped_reward = 2000\n",
    "\t\t\n",
    "\t\ttotal_reward += reward\n",
    "\t\treward = 0\n",
    "\t\treward += shaped_reward\n",
    "\t\tnext_state = get_state(next_obs, target_loc, has_picked_up)\n",
    "\t\tif next_state not in q_table:\n",
    "\t\t\tq_table[next_state] = np.zeros(action_nums)\n",
    "\t\tq_table[state][action] += hyperparameters[\"alpha\"] * (reward + hyperparameters[\"gamma\"] * np.max(q_table[next_state]) - q_table[state][action])\n",
    "\t\t\n",
    "\t\tstep_count += 1\n",
    "\t\tobs = next_obs\n",
    "\t\tstate = next_state\n",
    "   \n",
    "\t\n",
    "\t\tif render:\n",
    "\t\t\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\t\t\taction=action, step=step_count, fuel=env.current_fuel)\n",
    "\t# print (step_count)\n",
    "\trewards_per_episode.append(total_reward)\n",
    "\tepsilon = max(hyperparameters[\"epsilon_end\"], epsilon * hyperparameters[\"decay_rate\"])\n",
    "\tif (episode + 1) % 20 == 0:\n",
    "\t\tavg_reward = np.mean(rewards_per_episode[-20:])\n",
    "\t\tprint(f'Episode {episode + 1}/{hyperparameters[\"episodes\"]}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.3f}')\n",
    "\t\t# print ([np.argmax(i) for i in q_table.values()])\n",
    "\t\t# print ((q_table))\n",
    "  \n",
    "filename = \"q_table.pkl\"\n",
    "\n",
    "# 儲存 Q-table\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "print(f\"Q-table 已儲存至 {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
