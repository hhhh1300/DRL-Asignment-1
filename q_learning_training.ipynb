{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\楊中\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "\n",
    "env_config = {\n",
    "    \"fuel_limit\": 5000\n",
    "}\n",
    "render = False\n",
    "hyperparameters = {\n",
    "    \"alpha\": 0.1,\n",
    "\t\"gamma\": 0.99,\n",
    "\t\"epsilon_start\": 1.0, \n",
    " \t\"epsilon_end\": 0.1,\n",
    "\t\"decay_rate\": 0.9999,\n",
    "\t\"episodes\": 10000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20/10000, Avg Reward: -92400.7800, Epsilon: 0.998\n",
      "140\n",
      "Episode 40/10000, Avg Reward: -97323.0000, Epsilon: 0.996\n",
      "148\n",
      "Episode 60/10000, Avg Reward: -105803.9900, Epsilon: 0.994\n",
      "150\n",
      "Episode 80/10000, Avg Reward: -130773.4000, Epsilon: 0.992\n",
      "151\n",
      "Episode 100/10000, Avg Reward: -113441.3300, Epsilon: 0.990\n",
      "151\n",
      "Episode 120/10000, Avg Reward: -163508.5600, Epsilon: 0.988\n",
      "151\n",
      "Episode 140/10000, Avg Reward: -130267.6100, Epsilon: 0.986\n",
      "151\n",
      "Episode 160/10000, Avg Reward: -120973.0800, Epsilon: 0.984\n",
      "151\n",
      "Episode 180/10000, Avg Reward: -202151.6400, Epsilon: 0.982\n",
      "151\n",
      "Episode 200/10000, Avg Reward: -185085.4200, Epsilon: 0.980\n",
      "152\n",
      "Episode 220/10000, Avg Reward: -121345.4800, Epsilon: 0.978\n",
      "152\n",
      "Episode 240/10000, Avg Reward: -203383.7900, Epsilon: 0.976\n",
      "152\n",
      "Episode 260/10000, Avg Reward: -136454.1600, Epsilon: 0.974\n",
      "152\n",
      "Episode 280/10000, Avg Reward: -100250.0800, Epsilon: 0.972\n",
      "152\n",
      "Episode 300/10000, Avg Reward: -161351.8800, Epsilon: 0.970\n",
      "152\n",
      "Episode 320/10000, Avg Reward: -170032.4500, Epsilon: 0.969\n",
      "153\n",
      "Episode 340/10000, Avg Reward: -149314.8900, Epsilon: 0.967\n",
      "153\n",
      "Episode 360/10000, Avg Reward: -190881.8700, Epsilon: 0.965\n",
      "155\n",
      "Episode 380/10000, Avg Reward: -110336.9500, Epsilon: 0.963\n",
      "156\n",
      "Episode 400/10000, Avg Reward: -160534.7900, Epsilon: 0.961\n",
      "157\n",
      "Episode 420/10000, Avg Reward: -179714.0200, Epsilon: 0.959\n",
      "157\n",
      "Episode 440/10000, Avg Reward: -136489.8700, Epsilon: 0.957\n",
      "157\n",
      "Episode 460/10000, Avg Reward: -129953.1800, Epsilon: 0.955\n",
      "157\n",
      "Episode 480/10000, Avg Reward: -175056.8100, Epsilon: 0.953\n",
      "157\n",
      "Episode 500/10000, Avg Reward: -212660.0300, Epsilon: 0.951\n",
      "157\n",
      "Episode 520/10000, Avg Reward: -336064.4000, Epsilon: 0.949\n",
      "157\n",
      "Episode 540/10000, Avg Reward: -204127.6800, Epsilon: 0.947\n",
      "157\n",
      "Episode 560/10000, Avg Reward: -144279.5600, Epsilon: 0.946\n",
      "157\n",
      "Episode 580/10000, Avg Reward: -123946.8700, Epsilon: 0.944\n",
      "157\n",
      "Episode 600/10000, Avg Reward: -160091.6000, Epsilon: 0.942\n",
      "157\n",
      "Episode 620/10000, Avg Reward: -258815.8200, Epsilon: 0.940\n",
      "157\n",
      "Episode 640/10000, Avg Reward: -269220.0300, Epsilon: 0.938\n",
      "158\n",
      "Episode 660/10000, Avg Reward: -512711.8900, Epsilon: 0.936\n",
      "158\n",
      "Episode 680/10000, Avg Reward: -173163.6200, Epsilon: 0.934\n",
      "158\n",
      "Episode 700/10000, Avg Reward: -314055.4500, Epsilon: 0.932\n",
      "158\n",
      "Episode 720/10000, Avg Reward: -82041.1500, Epsilon: 0.931\n",
      "158\n",
      "Episode 740/10000, Avg Reward: -192863.5800, Epsilon: 0.929\n",
      "158\n",
      "Episode 760/10000, Avg Reward: -218804.0500, Epsilon: 0.927\n",
      "158\n",
      "Episode 780/10000, Avg Reward: -224221.5600, Epsilon: 0.925\n",
      "158\n",
      "Episode 800/10000, Avg Reward: -173834.5800, Epsilon: 0.923\n",
      "158\n",
      "Episode 820/10000, Avg Reward: -268539.5500, Epsilon: 0.921\n",
      "158\n",
      "Episode 840/10000, Avg Reward: -208750.3600, Epsilon: 0.919\n",
      "158\n",
      "Episode 860/10000, Avg Reward: -207180.4700, Epsilon: 0.918\n",
      "158\n",
      "Episode 880/10000, Avg Reward: -186174.7400, Epsilon: 0.916\n",
      "158\n",
      "Episode 900/10000, Avg Reward: -191026.9200, Epsilon: 0.914\n",
      "158\n",
      "Episode 920/10000, Avg Reward: -170041.6600, Epsilon: 0.912\n",
      "158\n",
      "Episode 940/10000, Avg Reward: -267935.2900, Epsilon: 0.910\n",
      "158\n",
      "Episode 960/10000, Avg Reward: -177030.3200, Epsilon: 0.908\n",
      "158\n",
      "Episode 980/10000, Avg Reward: -341166.9800, Epsilon: 0.907\n",
      "158\n",
      "Episode 1000/10000, Avg Reward: -301612.6900, Epsilon: 0.905\n",
      "158\n",
      "Episode 1020/10000, Avg Reward: -285951.5900, Epsilon: 0.903\n",
      "158\n",
      "Episode 1040/10000, Avg Reward: -326181.2000, Epsilon: 0.901\n",
      "158\n",
      "Episode 1060/10000, Avg Reward: -245353.3500, Epsilon: 0.899\n",
      "158\n",
      "Episode 1080/10000, Avg Reward: -598740.7800, Epsilon: 0.898\n",
      "158\n",
      "Episode 1100/10000, Avg Reward: -373275.7300, Epsilon: 0.896\n",
      "158\n",
      "Episode 1120/10000, Avg Reward: -390640.6300, Epsilon: 0.894\n",
      "158\n",
      "Episode 1140/10000, Avg Reward: -401728.3800, Epsilon: 0.892\n",
      "158\n",
      "Episode 1160/10000, Avg Reward: -269928.9800, Epsilon: 0.890\n",
      "158\n",
      "Episode 1180/10000, Avg Reward: -303882.2300, Epsilon: 0.889\n",
      "158\n",
      "Episode 1200/10000, Avg Reward: -572878.0300, Epsilon: 0.887\n",
      "158\n",
      "Episode 1220/10000, Avg Reward: -209609.8900, Epsilon: 0.885\n",
      "158\n",
      "Episode 1240/10000, Avg Reward: -340634.8600, Epsilon: 0.883\n",
      "158\n",
      "Episode 1260/10000, Avg Reward: -250036.1900, Epsilon: 0.882\n",
      "158\n",
      "Episode 1280/10000, Avg Reward: -430583.9500, Epsilon: 0.880\n",
      "158\n",
      "Episode 1300/10000, Avg Reward: -287148.4300, Epsilon: 0.878\n",
      "158\n",
      "Episode 1320/10000, Avg Reward: -329086.6100, Epsilon: 0.876\n",
      "158\n",
      "Episode 1340/10000, Avg Reward: -263858.9800, Epsilon: 0.875\n",
      "158\n",
      "Episode 1360/10000, Avg Reward: -374398.4200, Epsilon: 0.873\n",
      "158\n",
      "Episode 1380/10000, Avg Reward: -512587.2300, Epsilon: 0.871\n",
      "158\n",
      "Episode 1400/10000, Avg Reward: -796076.8900, Epsilon: 0.869\n",
      "158\n",
      "Episode 1420/10000, Avg Reward: -465290.8600, Epsilon: 0.868\n",
      "158\n",
      "Episode 1440/10000, Avg Reward: -832747.4400, Epsilon: 0.866\n",
      "158\n",
      "Episode 1460/10000, Avg Reward: -608752.5400, Epsilon: 0.864\n",
      "158\n",
      "Episode 1480/10000, Avg Reward: -493101.9400, Epsilon: 0.862\n",
      "158\n",
      "Episode 1500/10000, Avg Reward: -361407.4000, Epsilon: 0.861\n",
      "158\n",
      "Episode 1520/10000, Avg Reward: -350545.6700, Epsilon: 0.859\n",
      "158\n",
      "Episode 1540/10000, Avg Reward: -212225.9800, Epsilon: 0.857\n",
      "158\n",
      "Episode 1560/10000, Avg Reward: -779894.4300, Epsilon: 0.856\n",
      "158\n",
      "Episode 1580/10000, Avg Reward: -744763.5600, Epsilon: 0.854\n",
      "158\n",
      "Episode 1600/10000, Avg Reward: -723598.1600, Epsilon: 0.852\n",
      "158\n",
      "Episode 1620/10000, Avg Reward: -882082.3200, Epsilon: 0.850\n",
      "158\n",
      "Episode 1640/10000, Avg Reward: -635649.9200, Epsilon: 0.849\n",
      "158\n",
      "Episode 1660/10000, Avg Reward: -798767.5300, Epsilon: 0.847\n",
      "158\n",
      "Episode 1680/10000, Avg Reward: -839287.1100, Epsilon: 0.845\n",
      "158\n",
      "Episode 1700/10000, Avg Reward: -509265.7700, Epsilon: 0.844\n",
      "158\n",
      "Episode 1720/10000, Avg Reward: -684495.6700, Epsilon: 0.842\n",
      "158\n",
      "Episode 1740/10000, Avg Reward: -253058.7400, Epsilon: 0.840\n",
      "158\n",
      "Episode 1760/10000, Avg Reward: -489016.5000, Epsilon: 0.839\n",
      "158\n",
      "Episode 1780/10000, Avg Reward: -460841.4600, Epsilon: 0.837\n",
      "158\n",
      "Episode 1800/10000, Avg Reward: -1169239.9500, Epsilon: 0.835\n",
      "158\n",
      "Episode 1820/10000, Avg Reward: -1008068.8300, Epsilon: 0.834\n",
      "158\n",
      "Episode 1840/10000, Avg Reward: -1160396.9700, Epsilon: 0.832\n",
      "158\n",
      "Episode 1860/10000, Avg Reward: -306353.1400, Epsilon: 0.830\n",
      "158\n",
      "Episode 1880/10000, Avg Reward: -770074.6300, Epsilon: 0.829\n",
      "158\n",
      "Episode 1900/10000, Avg Reward: -965277.7300, Epsilon: 0.827\n",
      "158\n",
      "Episode 1920/10000, Avg Reward: -801544.7600, Epsilon: 0.825\n",
      "158\n",
      "Episode 1940/10000, Avg Reward: -1119865.5600, Epsilon: 0.824\n",
      "158\n",
      "Episode 1960/10000, Avg Reward: -671159.4100, Epsilon: 0.822\n",
      "158\n",
      "Episode 1980/10000, Avg Reward: -643911.2100, Epsilon: 0.820\n",
      "158\n",
      "Episode 2000/10000, Avg Reward: -731862.3500, Epsilon: 0.819\n",
      "158\n",
      "Episode 2020/10000, Avg Reward: -423919.5700, Epsilon: 0.817\n",
      "158\n",
      "Episode 2040/10000, Avg Reward: -822471.8900, Epsilon: 0.815\n",
      "158\n",
      "Episode 2060/10000, Avg Reward: -440949.1100, Epsilon: 0.814\n",
      "158\n",
      "Episode 2080/10000, Avg Reward: -1097465.1600, Epsilon: 0.812\n",
      "158\n",
      "Episode 2100/10000, Avg Reward: -890108.4200, Epsilon: 0.811\n",
      "158\n",
      "Episode 2120/10000, Avg Reward: -916686.0400, Epsilon: 0.809\n",
      "158\n",
      "Episode 2140/10000, Avg Reward: -983198.8200, Epsilon: 0.807\n",
      "158\n",
      "Episode 2160/10000, Avg Reward: -1011368.3300, Epsilon: 0.806\n",
      "158\n",
      "Episode 2180/10000, Avg Reward: -973166.2000, Epsilon: 0.804\n",
      "158\n",
      "Episode 2200/10000, Avg Reward: -485730.7400, Epsilon: 0.803\n",
      "158\n",
      "Episode 2220/10000, Avg Reward: -980936.1500, Epsilon: 0.801\n",
      "158\n",
      "Episode 2240/10000, Avg Reward: -1249432.9000, Epsilon: 0.799\n",
      "158\n",
      "Episode 2260/10000, Avg Reward: -634759.4100, Epsilon: 0.798\n",
      "158\n",
      "Episode 2280/10000, Avg Reward: -947692.0400, Epsilon: 0.796\n",
      "158\n",
      "Episode 2300/10000, Avg Reward: -1019895.4500, Epsilon: 0.795\n",
      "158\n",
      "Episode 2320/10000, Avg Reward: -925775.1900, Epsilon: 0.793\n",
      "158\n",
      "Episode 2340/10000, Avg Reward: -884833.4800, Epsilon: 0.791\n",
      "158\n",
      "Episode 2360/10000, Avg Reward: -1702211.5800, Epsilon: 0.790\n",
      "158\n",
      "Episode 2380/10000, Avg Reward: -1168913.3100, Epsilon: 0.788\n",
      "158\n",
      "Episode 2400/10000, Avg Reward: -1706885.8600, Epsilon: 0.787\n",
      "158\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m q_table:\n\u001b[0;32m     92\u001b[0m \tq_table[state] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(action_nums)\n\u001b[1;32m---> 94\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m next_obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mget_action\u001b[1;34m(obs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m# Selects the best action using the trained Q-table.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m epsilon:\n\u001b[1;32m---> 58\u001b[0m \taction \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_nums\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Random action\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m \taction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_table[get_state(obs)])  \u001b[38;5;66;03m# Greedy action\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "q_table = {}\n",
    "epsilon = hyperparameters[\"epsilon_start\"]\n",
    "\n",
    "def get_state(obs):\n",
    "\tif not hasattr(get_state, \"passenger_loc\"):\n",
    "\t\tget_state.passenger_loc = None\n",
    "\tif not hasattr(get_state, \"destination_loc\"):\n",
    "\t\tget_state.destination_loc = None\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col, stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\t\n",
    " # determine which station the taxi is at\n",
    "\ttaxi_station = 0\n",
    "\tfor i in range(4):\n",
    "\t\tif taxi_row == stations[i][0] and taxi_col == stations[i][1]:\n",
    "\t\t\ttaxi_station = i\n",
    "\t\t\tbreak\n",
    "\t\n",
    "\thas_picked_up = False\n",
    "\tif passenger_look == 1 and get_state.destination_loc is None:\n",
    "\t\t# determine the passenger location\n",
    "\t\tfor i in range(4):\n",
    "\t\t\tpassenger_loc_north = int( (taxi_row - 1, taxi_col) == stations[i])\n",
    "\t\t\tpassenger_loc_south = int( (taxi_row + 1, taxi_col) == stations[i])\n",
    "\t\t\tpassenger_loc_east  = int( (taxi_row, taxi_col + 1) == stations[i])\n",
    "\t\t\tpassenger_loc_west  = int( (taxi_row, taxi_col - 1) == stations[i])\n",
    "\t\t\tpassenger_loc_middle  = int( (taxi_row, taxi_col) == stations[i])\n",
    "\t\t\tif passenger_loc_north or passenger_loc_south or passenger_loc_east or passenger_loc_west or passenger_loc_middle:\n",
    "\t\t\t\tget_state.passenger_loc = (stations[i][0], stations[i][1])\n",
    "\t\t\t\tbreak\n",
    "\t\tif passenger_loc is None or passenger_loc_middle:\n",
    "\t\t\thas_picked_up = True\n",
    "\tif has_picked_up:\n",
    "\t\tget_state.passenger_loc = None\n",
    "  \n",
    "\tif destination_look == 1 and get_state.destination_loc is None:\n",
    "\t\t# determine the destination location\n",
    "\t\tfor i in range(4):\n",
    "\t\t\tdestination_loc_north = int( (taxi_row - 1, taxi_col) == stations[i])\n",
    "\t\t\tdestination_loc_south = int( (taxi_row + 1, taxi_col) == stations[i])\n",
    "\t\t\tdestination_loc_east  = int( (taxi_row, taxi_col + 1) == stations[i])\n",
    "\t\t\tdestination_loc_west  = int( (taxi_row, taxi_col - 1) == stations[i])\n",
    "\t\t\tdestination_loc_middle  = int( (taxi_row, taxi_col) == stations[i])\n",
    "\t\t\t# print (taxi_row, taxi_col, stations[i])\n",
    "\t\t\tif destination_loc_north or destination_loc_south or destination_loc_east or destination_loc_west or destination_loc_middle:\n",
    "\t\t\t\tget_state.destination_loc = (stations[i][0], stations[i][1])\n",
    "\t\t\t\tbreak\n",
    "\t\t# print (destination_loc)\n",
    "\t\n",
    "\t\n",
    "\treturn (int(taxi_row), int(taxi_col), obstacle_north, obstacle_south, obstacle_east, obstacle_west, get_state.passenger_loc, get_state.destination_loc, has_picked_up)\n",
    "  \n",
    "def get_action(obs):\n",
    "\t\"\"\"\n",
    "\t# Selects the best action using the trained Q-table.\n",
    "\t\"\"\"\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = np.random.choice(action_nums)  # Random action\n",
    "\telse:\n",
    "\t\taction = np.argmax(q_table[get_state(obs)])  # Greedy action\n",
    "\treturn action\n",
    "\n",
    "env = SimpleTaxiEnv(**env_config)\n",
    "action_nums = 6\n",
    "rewards_per_episode = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "step_count = 0\n",
    "stations = [(0, 0), (0, 4), (4, 0), (4,4)]\n",
    "\n",
    "taxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\n",
    "if render:\n",
    "\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\taction=None, step=step_count, fuel=env.current_fuel)\n",
    "\ttime.sleep(0.5)\n",
    " \n",
    "\n",
    "for episode in range(hyperparameters[\"episodes\"]):\n",
    "\tget_state.passenger_loc, get_state.destination_loc = None, None\n",
    "\tobs, _ = env.reset()\n",
    "\tdone = False\n",
    "\ttotal_reward = 0\n",
    "\tstep_count = 0\n",
    "\tpassenger_loc = None\n",
    "\tdestination = None\n",
    "\tstate = get_state(obs)\n",
    "\twhile not done:\t\n",
    "\t\tif state not in q_table:\n",
    "\t\t\tq_table[state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\taction = get_action(obs)\n",
    "\n",
    "\t\tnext_obs, reward, done, _ = env.step(action)\n",
    "\t\tif done:\n",
    "\t\t\tif reward < 40:\n",
    "\t\t\t\tdone = False\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\t\t# print (reward)\n",
    "\t\t\t\t# print ('done', reward, action, state, get_state(next_obs), next_obs[-2:])\n",
    "\t\t# print (next_obs)\n",
    "   \n",
    "\t\tnext_state = get_state(next_obs)\n",
    "\t\t# shaped_reward = reward-1\n",
    "\t\t# if next_state[-1]:\n",
    "\t\t# \tshaped_reward += 100\n",
    "\t\t# elif done and reward > 40:\n",
    "\t\t# \tshaped_reward += 100\n",
    "   \n",
    "\t\ttotal_reward += reward\n",
    "\t\tif next_obs not in q_table:\n",
    "\t\t\t# print (next_obs)\n",
    "\t\t\tq_table[next_state] = np.zeros(action_nums)\n",
    "\t\tq_table[state][action] += hyperparameters[\"alpha\"] * (reward + hyperparameters[\"gamma\"] * np.max(q_table[next_state]) - q_table[state][action])\n",
    "\t\t\n",
    "\t\tstep_count += 1\n",
    "\t\tobs = next_obs\n",
    "\t\tstate = next_state\n",
    "\t\ttotal_reward += reward\n",
    "\n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look,destination_look = obs\n",
    "\t\n",
    "\t\tif render:\n",
    "\t\t\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\t\t\taction=action, step=step_count, fuel=env.current_fuel)\n",
    "\t# print (step_count)\n",
    "\trewards_per_episode.append(total_reward)\n",
    "\tepsilon = max(hyperparameters[\"epsilon_end\"], epsilon * hyperparameters[\"decay_rate\"])\n",
    "\tif (episode + 1) % 20 == 0:\n",
    "\t\tavg_reward = np.mean(rewards_per_episode[-20:])\n",
    "\t\tprint(f'Episode {episode + 1}/{hyperparameters[\"episodes\"]}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.3f}')\n",
    "\t\t# print ([np.argmax(i) for i in q_table.values()])\n",
    "\t\tprint (len(q_table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
