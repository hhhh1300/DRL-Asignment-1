{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "\n",
    "env_config = {\n",
    "    \"fuel_limit\": 5000\n",
    "}\n",
    "render = False\n",
    "hyperparameters = {\n",
    "    \"alpha\": 0.1,\n",
    "\t\"gamma\": 0.99,\n",
    "\t\"epsilon_start\": 1.0, \n",
    "\t\"epsilon_end\": 0.01,\n",
    "\t\"decay_rate\": 0.9995,\n",
    "\t\"episodes\": 12000,\n",
    " \t\"max_steps\": 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20/12000, Avg Reward: -10900.7900, Epsilon: 0.990\n",
      "Episode 40/12000, Avg Reward: -15583.8400, Epsilon: 0.980\n",
      "Episode 60/12000, Avg Reward: -14111.2950, Epsilon: 0.970\n",
      "Episode 80/12000, Avg Reward: -12794.8050, Epsilon: 0.961\n",
      "Episode 100/12000, Avg Reward: -14296.1250, Epsilon: 0.951\n",
      "Episode 120/12000, Avg Reward: -15253.5250, Epsilon: 0.942\n",
      "Episode 140/12000, Avg Reward: -15174.7000, Epsilon: 0.932\n",
      "Episode 160/12000, Avg Reward: -14377.2900, Epsilon: 0.923\n",
      "Episode 180/12000, Avg Reward: -13555.6100, Epsilon: 0.914\n",
      "Episode 200/12000, Avg Reward: -12080.3700, Epsilon: 0.905\n",
      "Episode 220/12000, Avg Reward: -13623.4650, Epsilon: 0.896\n",
      "Episode 240/12000, Avg Reward: -14576.5850, Epsilon: 0.887\n",
      "Episode 260/12000, Avg Reward: -13714.9600, Epsilon: 0.878\n",
      "Episode 280/12000, Avg Reward: -13663.7600, Epsilon: 0.869\n",
      "Episode 300/12000, Avg Reward: -14112.5350, Epsilon: 0.861\n",
      "Episode 320/12000, Avg Reward: -15213.6750, Epsilon: 0.852\n",
      "Episode 340/12000, Avg Reward: -14718.6900, Epsilon: 0.844\n",
      "Episode 360/12000, Avg Reward: -12568.5750, Epsilon: 0.835\n",
      "Episode 380/12000, Avg Reward: -12692.7200, Epsilon: 0.827\n",
      "Episode 400/12000, Avg Reward: -13917.9350, Epsilon: 0.819\n",
      "Episode 420/12000, Avg Reward: -14327.8700, Epsilon: 0.811\n",
      "Episode 440/12000, Avg Reward: -12626.6250, Epsilon: 0.802\n",
      "Episode 460/12000, Avg Reward: -14312.8650, Epsilon: 0.794\n",
      "Episode 480/12000, Avg Reward: -12658.7650, Epsilon: 0.787\n",
      "Episode 500/12000, Avg Reward: -12989.5600, Epsilon: 0.779\n",
      "Episode 520/12000, Avg Reward: -13075.8800, Epsilon: 0.771\n",
      "Episode 540/12000, Avg Reward: -11567.8750, Epsilon: 0.763\n",
      "Episode 560/12000, Avg Reward: -12082.5350, Epsilon: 0.756\n",
      "Episode 580/12000, Avg Reward: -12833.6600, Epsilon: 0.748\n",
      "Episode 600/12000, Avg Reward: -13636.4000, Epsilon: 0.741\n",
      "Episode 620/12000, Avg Reward: -13131.8450, Epsilon: 0.733\n",
      "Episode 640/12000, Avg Reward: -13962.4650, Epsilon: 0.726\n",
      "Episode 660/12000, Avg Reward: -13195.8550, Epsilon: 0.719\n",
      "Episode 680/12000, Avg Reward: -12588.9500, Epsilon: 0.712\n",
      "Episode 700/12000, Avg Reward: -13132.9650, Epsilon: 0.705\n",
      "Episode 720/12000, Avg Reward: -13015.9150, Epsilon: 0.698\n",
      "Episode 740/12000, Avg Reward: -11515.5800, Epsilon: 0.691\n",
      "Episode 760/12000, Avg Reward: -11960.0750, Epsilon: 0.684\n",
      "Episode 780/12000, Avg Reward: -12349.2700, Epsilon: 0.677\n",
      "Episode 800/12000, Avg Reward: -11447.3400, Epsilon: 0.670\n",
      "Episode 820/12000, Avg Reward: -12106.4200, Epsilon: 0.664\n",
      "Episode 840/12000, Avg Reward: -11526.4650, Epsilon: 0.657\n",
      "Episode 860/12000, Avg Reward: -12033.2750, Epsilon: 0.650\n",
      "Episode 880/12000, Avg Reward: -12047.9250, Epsilon: 0.644\n",
      "Episode 900/12000, Avg Reward: -9012.5300, Epsilon: 0.638\n",
      "Episode 920/12000, Avg Reward: -11678.6850, Epsilon: 0.631\n",
      "Episode 940/12000, Avg Reward: -10180.5800, Epsilon: 0.625\n",
      "Episode 960/12000, Avg Reward: -12034.3650, Epsilon: 0.619\n",
      "Episode 980/12000, Avg Reward: -10883.3000, Epsilon: 0.613\n",
      "Episode 1000/12000, Avg Reward: -11132.3700, Epsilon: 0.606\n",
      "Episode 1020/12000, Avg Reward: -10261.7250, Epsilon: 0.600\n",
      "Episode 1040/12000, Avg Reward: -10798.8300, Epsilon: 0.594\n",
      "Episode 1060/12000, Avg Reward: -11267.1800, Epsilon: 0.589\n",
      "Episode 1080/12000, Avg Reward: -10469.2500, Epsilon: 0.583\n",
      "Episode 1100/12000, Avg Reward: -11019.9500, Epsilon: 0.577\n",
      "Episode 1120/12000, Avg Reward: -11186.3600, Epsilon: 0.571\n",
      "Episode 1140/12000, Avg Reward: -10116.1950, Epsilon: 0.565\n",
      "Episode 1160/12000, Avg Reward: -10625.2350, Epsilon: 0.560\n",
      "Episode 1180/12000, Avg Reward: -11116.5000, Epsilon: 0.554\n",
      "Episode 1200/12000, Avg Reward: -10136.1850, Epsilon: 0.549\n",
      "Episode 1220/12000, Avg Reward: -10693.0000, Epsilon: 0.543\n",
      "Episode 1240/12000, Avg Reward: -10053.0700, Epsilon: 0.538\n",
      "Episode 1260/12000, Avg Reward: -9944.9050, Epsilon: 0.533\n",
      "Episode 1280/12000, Avg Reward: -10265.8050, Epsilon: 0.527\n",
      "Episode 1300/12000, Avg Reward: -8577.7000, Epsilon: 0.522\n",
      "Episode 1320/12000, Avg Reward: -9348.2050, Epsilon: 0.517\n",
      "Episode 1340/12000, Avg Reward: -9586.0150, Epsilon: 0.512\n",
      "Episode 1360/12000, Avg Reward: -9760.3450, Epsilon: 0.507\n",
      "Episode 1380/12000, Avg Reward: -9540.2900, Epsilon: 0.501\n",
      "Episode 1400/12000, Avg Reward: -8832.0650, Epsilon: 0.496\n",
      "Episode 1420/12000, Avg Reward: -9211.3550, Epsilon: 0.492\n",
      "Episode 1440/12000, Avg Reward: -8591.2800, Epsilon: 0.487\n",
      "Episode 1460/12000, Avg Reward: -9117.5000, Epsilon: 0.482\n",
      "Episode 1480/12000, Avg Reward: -9167.8700, Epsilon: 0.477\n",
      "Episode 1500/12000, Avg Reward: -9299.0100, Epsilon: 0.472\n",
      "Episode 1520/12000, Avg Reward: -8861.6450, Epsilon: 0.468\n",
      "Episode 1540/12000, Avg Reward: -8449.2300, Epsilon: 0.463\n",
      "Episode 1560/12000, Avg Reward: -8767.5850, Epsilon: 0.458\n",
      "Episode 1580/12000, Avg Reward: -8432.0900, Epsilon: 0.454\n",
      "Episode 1600/12000, Avg Reward: -8631.7400, Epsilon: 0.449\n",
      "Episode 1620/12000, Avg Reward: -8319.2300, Epsilon: 0.445\n",
      "Episode 1640/12000, Avg Reward: -7961.5700, Epsilon: 0.440\n",
      "Episode 1660/12000, Avg Reward: -7729.8450, Epsilon: 0.436\n",
      "Episode 1680/12000, Avg Reward: -8560.0600, Epsilon: 0.432\n",
      "Episode 1700/12000, Avg Reward: -8666.4000, Epsilon: 0.427\n",
      "Episode 1720/12000, Avg Reward: -8172.1600, Epsilon: 0.423\n",
      "Episode 1740/12000, Avg Reward: -7932.2550, Epsilon: 0.419\n",
      "Episode 1760/12000, Avg Reward: -7931.2050, Epsilon: 0.415\n",
      "Episode 1780/12000, Avg Reward: -8184.0550, Epsilon: 0.411\n",
      "Episode 1800/12000, Avg Reward: -7713.7300, Epsilon: 0.406\n",
      "Episode 1820/12000, Avg Reward: -8143.5000, Epsilon: 0.402\n",
      "Episode 1840/12000, Avg Reward: -7937.7500, Epsilon: 0.398\n",
      "Episode 1860/12000, Avg Reward: -7837.2500, Epsilon: 0.394\n",
      "Episode 1880/12000, Avg Reward: -7385.4750, Epsilon: 0.391\n",
      "Episode 1900/12000, Avg Reward: -7419.9600, Epsilon: 0.387\n",
      "Episode 1920/12000, Avg Reward: -7744.0800, Epsilon: 0.383\n",
      "Episode 1940/12000, Avg Reward: -7448.5000, Epsilon: 0.379\n",
      "Episode 1960/12000, Avg Reward: -7069.5850, Epsilon: 0.375\n",
      "Episode 1980/12000, Avg Reward: -7425.7500, Epsilon: 0.371\n",
      "Episode 2000/12000, Avg Reward: -7542.2400, Epsilon: 0.368\n",
      "Episode 2020/12000, Avg Reward: -7290.7500, Epsilon: 0.364\n",
      "Episode 2040/12000, Avg Reward: -7319.8350, Epsilon: 0.361\n",
      "Episode 2060/12000, Avg Reward: -7206.5000, Epsilon: 0.357\n",
      "Episode 2080/12000, Avg Reward: -7041.1750, Epsilon: 0.353\n",
      "Episode 2100/12000, Avg Reward: -6844.9400, Epsilon: 0.350\n",
      "Episode 2120/12000, Avg Reward: -6563.2150, Epsilon: 0.346\n",
      "Episode 2140/12000, Avg Reward: -6827.7500, Epsilon: 0.343\n",
      "Episode 2160/12000, Avg Reward: -6570.3550, Epsilon: 0.340\n",
      "Episode 2180/12000, Avg Reward: -6847.7500, Epsilon: 0.336\n",
      "Episode 2200/12000, Avg Reward: -6308.7800, Epsilon: 0.333\n",
      "Episode 2220/12000, Avg Reward: -6292.2950, Epsilon: 0.329\n",
      "Episode 2240/12000, Avg Reward: -6476.8600, Epsilon: 0.326\n",
      "Episode 2260/12000, Avg Reward: -6458.9800, Epsilon: 0.323\n",
      "Episode 2280/12000, Avg Reward: -6462.5000, Epsilon: 0.320\n",
      "Episode 2300/12000, Avg Reward: -6236.8650, Epsilon: 0.317\n",
      "Episode 2320/12000, Avg Reward: -6176.8600, Epsilon: 0.313\n",
      "Episode 2340/12000, Avg Reward: -6307.8750, Epsilon: 0.310\n",
      "Episode 2360/12000, Avg Reward: -5982.4050, Epsilon: 0.307\n",
      "Episode 2380/12000, Avg Reward: -6026.8650, Epsilon: 0.304\n",
      "Episode 2400/12000, Avg Reward: -6193.2500, Epsilon: 0.301\n",
      "Episode 2420/12000, Avg Reward: -5733.7600, Epsilon: 0.298\n",
      "Episode 2440/12000, Avg Reward: -6013.5000, Epsilon: 0.295\n",
      "Episode 2460/12000, Avg Reward: -5862.4750, Epsilon: 0.292\n",
      "Episode 2480/12000, Avg Reward: -6029.5000, Epsilon: 0.289\n",
      "Episode 2500/12000, Avg Reward: -5809.5000, Epsilon: 0.286\n",
      "Episode 2520/12000, Avg Reward: -5833.2100, Epsilon: 0.284\n",
      "Episode 2540/12000, Avg Reward: -5622.9650, Epsilon: 0.281\n",
      "Episode 2560/12000, Avg Reward: -5581.0000, Epsilon: 0.278\n",
      "Episode 2580/12000, Avg Reward: -5418.0000, Epsilon: 0.275\n",
      "Episode 2600/12000, Avg Reward: -5380.4750, Epsilon: 0.272\n",
      "Episode 2620/12000, Avg Reward: -5320.5000, Epsilon: 0.270\n",
      "Episode 2640/12000, Avg Reward: -5513.7500, Epsilon: 0.267\n",
      "Episode 2660/12000, Avg Reward: -5457.7500, Epsilon: 0.264\n",
      "Episode 2680/12000, Avg Reward: -5389.7500, Epsilon: 0.262\n",
      "Episode 2700/12000, Avg Reward: -5347.5000, Epsilon: 0.259\n",
      "Episode 2720/12000, Avg Reward: -5340.0000, Epsilon: 0.257\n",
      "Episode 2740/12000, Avg Reward: -4880.4450, Epsilon: 0.254\n",
      "Episode 2760/12000, Avg Reward: -5139.5000, Epsilon: 0.251\n",
      "Episode 2780/12000, Avg Reward: -5189.2500, Epsilon: 0.249\n",
      "Episode 2800/12000, Avg Reward: -5212.2500, Epsilon: 0.247\n",
      "Episode 2820/12000, Avg Reward: -4725.5050, Epsilon: 0.244\n",
      "Episode 2840/12000, Avg Reward: -5177.2500, Epsilon: 0.242\n",
      "Episode 2860/12000, Avg Reward: -4907.4000, Epsilon: 0.239\n",
      "Episode 2880/12000, Avg Reward: -4959.2500, Epsilon: 0.237\n",
      "Episode 2900/12000, Avg Reward: -4658.1300, Epsilon: 0.234\n",
      "Episode 2920/12000, Avg Reward: -4795.7500, Epsilon: 0.232\n",
      "Episode 2940/12000, Avg Reward: -4791.0000, Epsilon: 0.230\n",
      "Episode 2960/12000, Avg Reward: -4842.2500, Epsilon: 0.228\n",
      "Episode 2980/12000, Avg Reward: -4719.2500, Epsilon: 0.225\n",
      "Episode 3000/12000, Avg Reward: -4598.7500, Epsilon: 0.223\n",
      "Episode 3020/12000, Avg Reward: -4453.8100, Epsilon: 0.221\n",
      "Episode 3040/12000, Avg Reward: -4604.0000, Epsilon: 0.219\n",
      "Episode 3060/12000, Avg Reward: -4310.0650, Epsilon: 0.216\n",
      "Episode 3080/12000, Avg Reward: -4495.7500, Epsilon: 0.214\n",
      "Episode 3100/12000, Avg Reward: -4563.0000, Epsilon: 0.212\n",
      "Episode 3120/12000, Avg Reward: -4404.5000, Epsilon: 0.210\n",
      "Episode 3140/12000, Avg Reward: -4234.4100, Epsilon: 0.208\n",
      "Episode 3160/12000, Avg Reward: -4194.1200, Epsilon: 0.206\n",
      "Episode 3180/12000, Avg Reward: -4283.5000, Epsilon: 0.204\n",
      "Episode 3200/12000, Avg Reward: -4253.2500, Epsilon: 0.202\n",
      "Episode 3220/12000, Avg Reward: -4211.5000, Epsilon: 0.200\n",
      "Episode 3240/12000, Avg Reward: -4210.0000, Epsilon: 0.198\n",
      "Episode 3260/12000, Avg Reward: -4145.5000, Epsilon: 0.196\n",
      "Episode 3280/12000, Avg Reward: -4180.7500, Epsilon: 0.194\n",
      "Episode 3300/12000, Avg Reward: -4172.2500, Epsilon: 0.192\n",
      "Episode 3320/12000, Avg Reward: -4022.7500, Epsilon: 0.190\n",
      "Episode 3340/12000, Avg Reward: -4049.7500, Epsilon: 0.188\n",
      "Episode 3360/12000, Avg Reward: -4013.7500, Epsilon: 0.186\n",
      "Episode 3380/12000, Avg Reward: -3777.4700, Epsilon: 0.184\n",
      "Episode 3400/12000, Avg Reward: -3903.5000, Epsilon: 0.183\n",
      "Episode 3420/12000, Avg Reward: -4010.5000, Epsilon: 0.181\n",
      "Episode 3440/12000, Avg Reward: -3757.5000, Epsilon: 0.179\n",
      "Episode 3460/12000, Avg Reward: -3820.7500, Epsilon: 0.177\n",
      "Episode 3480/12000, Avg Reward: -3797.2500, Epsilon: 0.175\n",
      "Episode 3500/12000, Avg Reward: -3770.5000, Epsilon: 0.174\n",
      "Episode 3520/12000, Avg Reward: -3707.0000, Epsilon: 0.172\n",
      "Episode 3540/12000, Avg Reward: -3626.0000, Epsilon: 0.170\n",
      "Episode 3560/12000, Avg Reward: -3645.0000, Epsilon: 0.169\n",
      "Episode 3580/12000, Avg Reward: -3567.3100, Epsilon: 0.167\n",
      "Episode 3600/12000, Avg Reward: -3571.0000, Epsilon: 0.165\n",
      "Episode 3620/12000, Avg Reward: -3512.0000, Epsilon: 0.164\n",
      "Episode 3640/12000, Avg Reward: -3533.7500, Epsilon: 0.162\n",
      "Episode 3660/12000, Avg Reward: -3524.2500, Epsilon: 0.160\n",
      "Episode 3680/12000, Avg Reward: -3345.5700, Epsilon: 0.159\n",
      "Episode 3700/12000, Avg Reward: -3406.4900, Epsilon: 0.157\n",
      "Episode 3720/12000, Avg Reward: -3425.5000, Epsilon: 0.156\n",
      "Episode 3740/12000, Avg Reward: -3582.7500, Epsilon: 0.154\n",
      "Episode 3760/12000, Avg Reward: -3303.0000, Epsilon: 0.153\n",
      "Episode 3780/12000, Avg Reward: -3299.5000, Epsilon: 0.151\n",
      "Episode 3800/12000, Avg Reward: -3388.5000, Epsilon: 0.149\n",
      "Episode 3820/12000, Avg Reward: -3252.5000, Epsilon: 0.148\n",
      "Episode 3840/12000, Avg Reward: -3260.2500, Epsilon: 0.147\n",
      "Episode 3860/12000, Avg Reward: -3188.5000, Epsilon: 0.145\n",
      "Episode 3880/12000, Avg Reward: -3140.2500, Epsilon: 0.144\n",
      "Episode 3900/12000, Avg Reward: -3151.5000, Epsilon: 0.142\n",
      "Episode 3920/12000, Avg Reward: -3197.0000, Epsilon: 0.141\n",
      "Episode 3940/12000, Avg Reward: -3121.7500, Epsilon: 0.139\n",
      "Episode 3960/12000, Avg Reward: -3206.0800, Epsilon: 0.138\n",
      "Episode 3980/12000, Avg Reward: -3041.7500, Epsilon: 0.137\n",
      "Episode 4000/12000, Avg Reward: -3048.7500, Epsilon: 0.135\n",
      "Episode 4020/12000, Avg Reward: -3015.5000, Epsilon: 0.134\n",
      "Episode 4040/12000, Avg Reward: -2967.0000, Epsilon: 0.133\n",
      "Episode 4060/12000, Avg Reward: -2962.2500, Epsilon: 0.131\n",
      "Episode 4080/12000, Avg Reward: -2875.2500, Epsilon: 0.130\n",
      "Episode 4100/12000, Avg Reward: -2927.7500, Epsilon: 0.129\n",
      "Episode 4120/12000, Avg Reward: -2898.2500, Epsilon: 0.127\n",
      "Episode 4140/12000, Avg Reward: -2746.6900, Epsilon: 0.126\n",
      "Episode 4160/12000, Avg Reward: -2895.5000, Epsilon: 0.125\n",
      "Episode 4180/12000, Avg Reward: -2796.4750, Epsilon: 0.124\n",
      "Episode 4200/12000, Avg Reward: -2805.7500, Epsilon: 0.122\n",
      "Episode 4220/12000, Avg Reward: -2795.7500, Epsilon: 0.121\n",
      "Episode 4240/12000, Avg Reward: -2845.0000, Epsilon: 0.120\n",
      "Episode 4260/12000, Avg Reward: -2735.2500, Epsilon: 0.119\n",
      "Episode 4280/12000, Avg Reward: -2679.0000, Epsilon: 0.118\n",
      "Episode 4300/12000, Avg Reward: -2727.0000, Epsilon: 0.116\n",
      "Episode 4320/12000, Avg Reward: -2720.2500, Epsilon: 0.115\n",
      "Episode 4340/12000, Avg Reward: -2618.5000, Epsilon: 0.114\n",
      "Episode 4360/12000, Avg Reward: -2634.7500, Epsilon: 0.113\n",
      "Episode 4380/12000, Avg Reward: -2595.2500, Epsilon: 0.112\n",
      "Episode 4400/12000, Avg Reward: -2604.0000, Epsilon: 0.111\n",
      "Episode 4420/12000, Avg Reward: -2559.2500, Epsilon: 0.110\n",
      "Episode 4440/12000, Avg Reward: -2528.7500, Epsilon: 0.109\n",
      "Episode 4460/12000, Avg Reward: -2513.7500, Epsilon: 0.107\n",
      "Episode 4480/12000, Avg Reward: -2485.5000, Epsilon: 0.106\n",
      "Episode 4500/12000, Avg Reward: -2601.0000, Epsilon: 0.105\n",
      "Episode 4520/12000, Avg Reward: -2467.8250, Epsilon: 0.104\n",
      "Episode 4540/12000, Avg Reward: -2404.0000, Epsilon: 0.103\n",
      "Episode 4560/12000, Avg Reward: -2458.5000, Epsilon: 0.102\n",
      "Episode 4580/12000, Avg Reward: -2346.7500, Epsilon: 0.101\n",
      "Episode 4600/12000, Avg Reward: -2379.0000, Epsilon: 0.100\n",
      "Episode 4620/12000, Avg Reward: -2434.2500, Epsilon: 0.099\n",
      "Episode 4640/12000, Avg Reward: -2367.5000, Epsilon: 0.098\n",
      "Episode 4660/12000, Avg Reward: -2329.0000, Epsilon: 0.097\n",
      "Episode 4680/12000, Avg Reward: -2291.2500, Epsilon: 0.096\n",
      "Episode 4700/12000, Avg Reward: -2332.2500, Epsilon: 0.095\n",
      "Episode 4720/12000, Avg Reward: -2292.2500, Epsilon: 0.094\n",
      "Episode 4740/12000, Avg Reward: -2269.7500, Epsilon: 0.093\n",
      "Episode 4760/12000, Avg Reward: -2239.2500, Epsilon: 0.092\n",
      "Episode 4780/12000, Avg Reward: -2168.6900, Epsilon: 0.092\n",
      "Episode 4800/12000, Avg Reward: -2253.7500, Epsilon: 0.091\n",
      "Episode 4820/12000, Avg Reward: -2223.0000, Epsilon: 0.090\n",
      "Episode 4840/12000, Avg Reward: -2208.5000, Epsilon: 0.089\n",
      "Episode 4860/12000, Avg Reward: -2055.1850, Epsilon: 0.088\n",
      "Episode 4880/12000, Avg Reward: -2139.0000, Epsilon: 0.087\n",
      "Episode 4900/12000, Avg Reward: -2149.5000, Epsilon: 0.086\n",
      "Episode 4920/12000, Avg Reward: -2067.0000, Epsilon: 0.085\n",
      "Episode 4940/12000, Avg Reward: -2135.7500, Epsilon: 0.085\n",
      "Episode 4960/12000, Avg Reward: -2034.0000, Epsilon: 0.084\n",
      "Episode 4980/12000, Avg Reward: -2088.0000, Epsilon: 0.083\n",
      "Episode 5000/12000, Avg Reward: -2089.0000, Epsilon: 0.082\n",
      "Episode 5020/12000, Avg Reward: -1994.2500, Epsilon: 0.081\n",
      "Episode 5040/12000, Avg Reward: -1985.0000, Epsilon: 0.080\n",
      "Episode 5060/12000, Avg Reward: -2017.2500, Epsilon: 0.080\n",
      "Episode 5080/12000, Avg Reward: -2020.2500, Epsilon: 0.079\n",
      "Episode 5100/12000, Avg Reward: -1950.0000, Epsilon: 0.078\n",
      "Episode 5120/12000, Avg Reward: -1978.2500, Epsilon: 0.077\n",
      "Episode 5140/12000, Avg Reward: -1985.5000, Epsilon: 0.076\n",
      "Episode 5160/12000, Avg Reward: -1877.5000, Epsilon: 0.076\n",
      "Episode 5180/12000, Avg Reward: -1937.5000, Epsilon: 0.075\n",
      "Episode 5200/12000, Avg Reward: -1928.7500, Epsilon: 0.074\n",
      "Episode 5220/12000, Avg Reward: -1878.5000, Epsilon: 0.073\n",
      "Episode 5240/12000, Avg Reward: -1845.0000, Epsilon: 0.073\n",
      "Episode 5260/12000, Avg Reward: -1747.7800, Epsilon: 0.072\n",
      "Episode 5280/12000, Avg Reward: -1851.7500, Epsilon: 0.071\n",
      "Episode 5300/12000, Avg Reward: -1847.0000, Epsilon: 0.071\n",
      "Episode 5320/12000, Avg Reward: -1902.7500, Epsilon: 0.070\n",
      "Episode 5340/12000, Avg Reward: -1817.7500, Epsilon: 0.069\n",
      "Episode 5360/12000, Avg Reward: -1771.5000, Epsilon: 0.069\n",
      "Episode 5380/12000, Avg Reward: -1823.7500, Epsilon: 0.068\n",
      "Episode 5400/12000, Avg Reward: -1775.0000, Epsilon: 0.067\n",
      "Episode 5420/12000, Avg Reward: -1780.7500, Epsilon: 0.066\n",
      "Episode 5440/12000, Avg Reward: -1743.5000, Epsilon: 0.066\n",
      "Episode 5460/12000, Avg Reward: -1722.7500, Epsilon: 0.065\n",
      "Episode 5480/12000, Avg Reward: -1751.0000, Epsilon: 0.065\n",
      "Episode 5500/12000, Avg Reward: -1710.2500, Epsilon: 0.064\n",
      "Episode 5520/12000, Avg Reward: -1637.0000, Epsilon: 0.063\n",
      "Episode 5540/12000, Avg Reward: -1693.7500, Epsilon: 0.063\n",
      "Episode 5560/12000, Avg Reward: -1675.5000, Epsilon: 0.062\n",
      "Episode 5580/12000, Avg Reward: -1661.7500, Epsilon: 0.061\n",
      "Episode 5600/12000, Avg Reward: -1607.5000, Epsilon: 0.061\n",
      "Episode 5620/12000, Avg Reward: -1650.5000, Epsilon: 0.060\n",
      "Episode 5640/12000, Avg Reward: -1630.0000, Epsilon: 0.060\n",
      "Episode 5660/12000, Avg Reward: -1633.5000, Epsilon: 0.059\n",
      "Episode 5680/12000, Avg Reward: -1616.2500, Epsilon: 0.058\n",
      "Episode 5700/12000, Avg Reward: -1574.0000, Epsilon: 0.058\n",
      "Episode 5720/12000, Avg Reward: -1603.2500, Epsilon: 0.057\n",
      "Episode 5740/12000, Avg Reward: -1594.5000, Epsilon: 0.057\n",
      "Episode 5760/12000, Avg Reward: -1556.5000, Epsilon: 0.056\n",
      "Episode 5780/12000, Avg Reward: -1566.2500, Epsilon: 0.056\n",
      "Episode 5800/12000, Avg Reward: -1511.5000, Epsilon: 0.055\n",
      "Episode 5820/12000, Avg Reward: -1545.7500, Epsilon: 0.054\n",
      "Episode 5840/12000, Avg Reward: -1570.7500, Epsilon: 0.054\n",
      "Episode 5860/12000, Avg Reward: -1508.2500, Epsilon: 0.053\n",
      "Episode 5880/12000, Avg Reward: -1534.2500, Epsilon: 0.053\n",
      "Episode 5900/12000, Avg Reward: -1482.5000, Epsilon: 0.052\n",
      "Episode 5920/12000, Avg Reward: -1459.7500, Epsilon: 0.052\n",
      "Episode 5940/12000, Avg Reward: -1438.2500, Epsilon: 0.051\n",
      "Episode 5960/12000, Avg Reward: -1465.2500, Epsilon: 0.051\n",
      "Episode 5980/12000, Avg Reward: -1473.7500, Epsilon: 0.050\n",
      "Episode 6000/12000, Avg Reward: -1434.0000, Epsilon: 0.050\n",
      "Episode 6020/12000, Avg Reward: -1411.5000, Epsilon: 0.049\n",
      "Episode 6040/12000, Avg Reward: -1418.7500, Epsilon: 0.049\n",
      "Episode 6060/12000, Avg Reward: -1456.2500, Epsilon: 0.048\n",
      "Episode 6080/12000, Avg Reward: -1416.2500, Epsilon: 0.048\n",
      "Episode 6100/12000, Avg Reward: -1419.0000, Epsilon: 0.047\n",
      "Episode 6120/12000, Avg Reward: -1412.2500, Epsilon: 0.047\n",
      "Episode 6140/12000, Avg Reward: -1372.5000, Epsilon: 0.046\n",
      "Episode 6160/12000, Avg Reward: -1360.0000, Epsilon: 0.046\n",
      "Episode 6180/12000, Avg Reward: -1298.5000, Epsilon: 0.045\n",
      "Episode 6200/12000, Avg Reward: -1368.0000, Epsilon: 0.045\n",
      "Episode 6220/12000, Avg Reward: -1356.5000, Epsilon: 0.045\n",
      "Episode 6240/12000, Avg Reward: -1371.7500, Epsilon: 0.044\n",
      "Episode 6260/12000, Avg Reward: -1334.7500, Epsilon: 0.044\n",
      "Episode 6280/12000, Avg Reward: -1342.7500, Epsilon: 0.043\n",
      "Episode 6300/12000, Avg Reward: -1307.7500, Epsilon: 0.043\n",
      "Episode 6320/12000, Avg Reward: -1316.2500, Epsilon: 0.042\n",
      "Episode 6340/12000, Avg Reward: -1300.7500, Epsilon: 0.042\n",
      "Episode 6360/12000, Avg Reward: -1318.0000, Epsilon: 0.042\n",
      "Episode 6380/12000, Avg Reward: -1243.0000, Epsilon: 0.041\n",
      "Episode 6400/12000, Avg Reward: -1253.0000, Epsilon: 0.041\n",
      "Episode 6420/12000, Avg Reward: -1261.0000, Epsilon: 0.040\n",
      "Episode 6440/12000, Avg Reward: -1245.2500, Epsilon: 0.040\n",
      "Episode 6460/12000, Avg Reward: -1266.7500, Epsilon: 0.040\n",
      "Episode 6480/12000, Avg Reward: -1234.7500, Epsilon: 0.039\n",
      "Episode 6500/12000, Avg Reward: -1210.5000, Epsilon: 0.039\n",
      "Episode 6520/12000, Avg Reward: -1238.2500, Epsilon: 0.038\n",
      "Episode 6540/12000, Avg Reward: -1219.2500, Epsilon: 0.038\n",
      "Episode 6560/12000, Avg Reward: -1214.5000, Epsilon: 0.038\n",
      "Episode 6580/12000, Avg Reward: -1230.7500, Epsilon: 0.037\n",
      "Episode 6600/12000, Avg Reward: -1233.0000, Epsilon: 0.037\n",
      "Episode 6620/12000, Avg Reward: -1174.5000, Epsilon: 0.036\n",
      "Episode 6640/12000, Avg Reward: -1196.0000, Epsilon: 0.036\n",
      "Episode 6660/12000, Avg Reward: -1178.5000, Epsilon: 0.036\n",
      "Episode 6680/12000, Avg Reward: -1201.7500, Epsilon: 0.035\n",
      "Episode 6700/12000, Avg Reward: -1177.7500, Epsilon: 0.035\n",
      "Episode 6720/12000, Avg Reward: -1170.5000, Epsilon: 0.035\n",
      "Episode 6740/12000, Avg Reward: -1153.7500, Epsilon: 0.034\n",
      "Episode 6760/12000, Avg Reward: -1148.5000, Epsilon: 0.034\n",
      "Episode 6780/12000, Avg Reward: -1151.5000, Epsilon: 0.034\n",
      "Episode 6800/12000, Avg Reward: -1153.7500, Epsilon: 0.033\n",
      "Episode 6820/12000, Avg Reward: -1131.2500, Epsilon: 0.033\n",
      "Episode 6840/12000, Avg Reward: -1126.2500, Epsilon: 0.033\n",
      "Episode 6860/12000, Avg Reward: -1138.2500, Epsilon: 0.032\n",
      "Episode 6880/12000, Avg Reward: -1113.5000, Epsilon: 0.032\n",
      "Episode 6900/12000, Avg Reward: -1109.0000, Epsilon: 0.032\n",
      "Episode 6920/12000, Avg Reward: -1121.0000, Epsilon: 0.031\n",
      "Episode 6940/12000, Avg Reward: -1071.2500, Epsilon: 0.031\n",
      "Episode 6960/12000, Avg Reward: -1099.5000, Epsilon: 0.031\n",
      "Episode 6980/12000, Avg Reward: -1095.2500, Epsilon: 0.030\n",
      "Episode 7000/12000, Avg Reward: -1094.2500, Epsilon: 0.030\n",
      "Episode 7020/12000, Avg Reward: -1092.7500, Epsilon: 0.030\n",
      "Episode 7040/12000, Avg Reward: -1044.7500, Epsilon: 0.030\n",
      "Episode 7060/12000, Avg Reward: -1080.7500, Epsilon: 0.029\n",
      "Episode 7080/12000, Avg Reward: -1056.2500, Epsilon: 0.029\n",
      "Episode 7100/12000, Avg Reward: -1051.5000, Epsilon: 0.029\n",
      "Episode 7120/12000, Avg Reward: -1066.2500, Epsilon: 0.028\n",
      "Episode 7140/12000, Avg Reward: -1000.5000, Epsilon: 0.028\n",
      "Episode 7160/12000, Avg Reward: -1035.0000, Epsilon: 0.028\n",
      "Episode 7180/12000, Avg Reward: -1022.7500, Epsilon: 0.028\n",
      "Episode 7200/12000, Avg Reward: -1049.0000, Epsilon: 0.027\n",
      "Episode 7220/12000, Avg Reward: -1036.5000, Epsilon: 0.027\n",
      "Episode 7240/12000, Avg Reward: -1007.7500, Epsilon: 0.027\n",
      "Episode 7260/12000, Avg Reward: -992.7500, Epsilon: 0.026\n",
      "Episode 7280/12000, Avg Reward: -1004.7500, Epsilon: 0.026\n",
      "Episode 7300/12000, Avg Reward: -978.5000, Epsilon: 0.026\n",
      "Episode 7320/12000, Avg Reward: -992.2500, Epsilon: 0.026\n",
      "Episode 7340/12000, Avg Reward: -976.2500, Epsilon: 0.025\n",
      "Episode 7360/12000, Avg Reward: -982.5000, Epsilon: 0.025\n",
      "Episode 7380/12000, Avg Reward: -998.7500, Epsilon: 0.025\n",
      "Episode 7400/12000, Avg Reward: -969.0000, Epsilon: 0.025\n",
      "Episode 7420/12000, Avg Reward: -964.2500, Epsilon: 0.024\n",
      "Episode 7440/12000, Avg Reward: -970.7500, Epsilon: 0.024\n",
      "Episode 7460/12000, Avg Reward: -968.0000, Epsilon: 0.024\n",
      "Episode 7480/12000, Avg Reward: -950.7500, Epsilon: 0.024\n",
      "Episode 7500/12000, Avg Reward: -961.7500, Epsilon: 0.023\n",
      "Episode 7520/12000, Avg Reward: -959.7500, Epsilon: 0.023\n",
      "Episode 7540/12000, Avg Reward: -945.2500, Epsilon: 0.023\n",
      "Episode 7560/12000, Avg Reward: -946.2500, Epsilon: 0.023\n",
      "Episode 7580/12000, Avg Reward: -956.2500, Epsilon: 0.023\n",
      "Episode 7600/12000, Avg Reward: -899.7500, Epsilon: 0.022\n",
      "Episode 7620/12000, Avg Reward: -923.0000, Epsilon: 0.022\n",
      "Episode 7640/12000, Avg Reward: -922.0000, Epsilon: 0.022\n",
      "Episode 7660/12000, Avg Reward: -929.7500, Epsilon: 0.022\n",
      "Episode 7680/12000, Avg Reward: -927.0000, Epsilon: 0.021\n",
      "Episode 7700/12000, Avg Reward: -891.2500, Epsilon: 0.021\n",
      "Episode 7720/12000, Avg Reward: -904.5000, Epsilon: 0.021\n",
      "Episode 7740/12000, Avg Reward: -881.0000, Epsilon: 0.021\n",
      "Episode 7760/12000, Avg Reward: -915.2500, Epsilon: 0.021\n",
      "Episode 7780/12000, Avg Reward: -886.0000, Epsilon: 0.020\n",
      "Episode 7800/12000, Avg Reward: -883.5000, Epsilon: 0.020\n",
      "Episode 7820/12000, Avg Reward: -870.0000, Epsilon: 0.020\n",
      "Episode 7840/12000, Avg Reward: -882.7500, Epsilon: 0.020\n",
      "Episode 7860/12000, Avg Reward: -871.0000, Epsilon: 0.020\n",
      "Episode 7880/12000, Avg Reward: -898.2500, Epsilon: 0.019\n",
      "Episode 7900/12000, Avg Reward: -870.2500, Epsilon: 0.019\n",
      "Episode 7920/12000, Avg Reward: -879.0000, Epsilon: 0.019\n",
      "Episode 7940/12000, Avg Reward: -859.0000, Epsilon: 0.019\n",
      "Episode 7960/12000, Avg Reward: -860.2500, Epsilon: 0.019\n",
      "Episode 7980/12000, Avg Reward: -847.0000, Epsilon: 0.018\n",
      "Episode 8000/12000, Avg Reward: -863.2500, Epsilon: 0.018\n",
      "Episode 8020/12000, Avg Reward: -849.7500, Epsilon: 0.018\n",
      "Episode 8040/12000, Avg Reward: -822.2500, Epsilon: 0.018\n",
      "Episode 8060/12000, Avg Reward: -820.2500, Epsilon: 0.018\n",
      "Episode 8080/12000, Avg Reward: -842.5000, Epsilon: 0.018\n",
      "Episode 8100/12000, Avg Reward: -834.0000, Epsilon: 0.017\n",
      "Episode 8120/12000, Avg Reward: -839.5000, Epsilon: 0.017\n",
      "Episode 8140/12000, Avg Reward: -842.2500, Epsilon: 0.017\n",
      "Episode 8160/12000, Avg Reward: -836.0000, Epsilon: 0.017\n",
      "Episode 8180/12000, Avg Reward: -819.0000, Epsilon: 0.017\n",
      "Episode 8200/12000, Avg Reward: -790.2500, Epsilon: 0.017\n",
      "Episode 8220/12000, Avg Reward: -813.7500, Epsilon: 0.016\n",
      "Episode 8240/12000, Avg Reward: -829.2500, Epsilon: 0.016\n",
      "Episode 8260/12000, Avg Reward: -818.2500, Epsilon: 0.016\n",
      "Episode 8280/12000, Avg Reward: -832.7500, Epsilon: 0.016\n",
      "Episode 8300/12000, Avg Reward: -796.0000, Epsilon: 0.016\n",
      "Episode 8320/12000, Avg Reward: -811.5000, Epsilon: 0.016\n",
      "Episode 8340/12000, Avg Reward: -794.7500, Epsilon: 0.015\n",
      "Episode 8360/12000, Avg Reward: -796.5000, Epsilon: 0.015\n",
      "Episode 8380/12000, Avg Reward: -783.0000, Epsilon: 0.015\n",
      "Episode 8400/12000, Avg Reward: -790.2500, Epsilon: 0.015\n",
      "Episode 8420/12000, Avg Reward: -806.5000, Epsilon: 0.015\n",
      "Episode 8440/12000, Avg Reward: -794.0000, Epsilon: 0.015\n",
      "Episode 8460/12000, Avg Reward: -781.0000, Epsilon: 0.015\n",
      "Episode 8480/12000, Avg Reward: -805.5000, Epsilon: 0.014\n",
      "Episode 8500/12000, Avg Reward: -795.5000, Epsilon: 0.014\n",
      "Episode 8520/12000, Avg Reward: -774.0000, Epsilon: 0.014\n",
      "Episode 8540/12000, Avg Reward: -787.2500, Epsilon: 0.014\n",
      "Episode 8560/12000, Avg Reward: -768.0000, Epsilon: 0.014\n",
      "Episode 8580/12000, Avg Reward: -750.7500, Epsilon: 0.014\n",
      "Episode 8600/12000, Avg Reward: -746.5000, Epsilon: 0.014\n",
      "Episode 8620/12000, Avg Reward: -754.2500, Epsilon: 0.013\n",
      "Episode 8640/12000, Avg Reward: -757.0000, Epsilon: 0.013\n",
      "Episode 8660/12000, Avg Reward: -773.7500, Epsilon: 0.013\n",
      "Episode 8680/12000, Avg Reward: -758.5000, Epsilon: 0.013\n",
      "Episode 8700/12000, Avg Reward: -737.5000, Epsilon: 0.013\n",
      "Episode 8720/12000, Avg Reward: -744.0000, Epsilon: 0.013\n",
      "Episode 8740/12000, Avg Reward: -750.2500, Epsilon: 0.013\n",
      "Episode 8760/12000, Avg Reward: -720.5000, Epsilon: 0.013\n",
      "Episode 8780/12000, Avg Reward: -727.7500, Epsilon: 0.012\n",
      "Episode 8800/12000, Avg Reward: -752.0000, Epsilon: 0.012\n",
      "Episode 8820/12000, Avg Reward: -737.7500, Epsilon: 0.012\n",
      "Episode 8840/12000, Avg Reward: -724.7500, Epsilon: 0.012\n",
      "Episode 8860/12000, Avg Reward: -736.7500, Epsilon: 0.012\n",
      "Episode 8880/12000, Avg Reward: -732.5000, Epsilon: 0.012\n",
      "Episode 8900/12000, Avg Reward: -721.2500, Epsilon: 0.012\n",
      "Episode 8920/12000, Avg Reward: -721.7500, Epsilon: 0.012\n",
      "Episode 8940/12000, Avg Reward: -742.5000, Epsilon: 0.011\n",
      "Episode 8960/12000, Avg Reward: -728.0000, Epsilon: 0.011\n",
      "Episode 8980/12000, Avg Reward: -725.0000, Epsilon: 0.011\n",
      "Episode 9000/12000, Avg Reward: -715.7500, Epsilon: 0.011\n",
      "Episode 9020/12000, Avg Reward: -723.0000, Epsilon: 0.011\n",
      "Episode 9040/12000, Avg Reward: -709.5000, Epsilon: 0.011\n",
      "Episode 9060/12000, Avg Reward: -711.5000, Epsilon: 0.011\n",
      "Episode 9080/12000, Avg Reward: -715.0000, Epsilon: 0.011\n",
      "Episode 9100/12000, Avg Reward: -717.2500, Epsilon: 0.011\n",
      "Episode 9120/12000, Avg Reward: -686.7500, Epsilon: 0.010\n",
      "Episode 9140/12000, Avg Reward: -805.5000, Epsilon: 0.010\n",
      "Episode 9160/12000, Avg Reward: -711.0000, Epsilon: 0.010\n",
      "Episode 9180/12000, Avg Reward: -689.7500, Epsilon: 0.010\n",
      "Episode 9200/12000, Avg Reward: -701.5000, Epsilon: 0.010\n",
      "Episode 9220/12000, Avg Reward: -700.0000, Epsilon: 0.010\n",
      "Episode 9240/12000, Avg Reward: -686.0000, Epsilon: 0.010\n",
      "Episode 9260/12000, Avg Reward: -701.2500, Epsilon: 0.010\n",
      "Episode 9280/12000, Avg Reward: -677.5000, Epsilon: 0.010\n",
      "Episode 9300/12000, Avg Reward: -696.0000, Epsilon: 0.010\n",
      "Episode 9320/12000, Avg Reward: -695.0000, Epsilon: 0.010\n",
      "Episode 9340/12000, Avg Reward: -691.5000, Epsilon: 0.010\n",
      "Episode 9360/12000, Avg Reward: -691.5000, Epsilon: 0.010\n",
      "Episode 9380/12000, Avg Reward: -690.5000, Epsilon: 0.010\n",
      "Episode 9400/12000, Avg Reward: -717.0000, Epsilon: 0.010\n",
      "Episode 9420/12000, Avg Reward: -714.7500, Epsilon: 0.010\n",
      "Episode 9440/12000, Avg Reward: -706.5000, Epsilon: 0.010\n",
      "Episode 9460/12000, Avg Reward: -681.5000, Epsilon: 0.010\n",
      "Episode 9480/12000, Avg Reward: -693.5000, Epsilon: 0.010\n",
      "Episode 9500/12000, Avg Reward: -702.7500, Epsilon: 0.010\n",
      "Episode 9520/12000, Avg Reward: -705.5000, Epsilon: 0.010\n",
      "Episode 9540/12000, Avg Reward: -683.7500, Epsilon: 0.010\n",
      "Episode 9560/12000, Avg Reward: -708.2500, Epsilon: 0.010\n",
      "Episode 9580/12000, Avg Reward: -693.2500, Epsilon: 0.010\n",
      "Episode 9600/12000, Avg Reward: -699.2500, Epsilon: 0.010\n",
      "Episode 9620/12000, Avg Reward: -693.0000, Epsilon: 0.010\n",
      "Episode 9640/12000, Avg Reward: -703.0000, Epsilon: 0.010\n",
      "Episode 9660/12000, Avg Reward: -704.5000, Epsilon: 0.010\n",
      "Episode 9680/12000, Avg Reward: -690.2500, Epsilon: 0.010\n",
      "Episode 9700/12000, Avg Reward: -709.0000, Epsilon: 0.010\n",
      "Episode 9720/12000, Avg Reward: -702.0000, Epsilon: 0.010\n",
      "Episode 9740/12000, Avg Reward: -697.5000, Epsilon: 0.010\n",
      "Episode 9760/12000, Avg Reward: -692.2500, Epsilon: 0.010\n",
      "Episode 9780/12000, Avg Reward: -702.7500, Epsilon: 0.010\n",
      "Episode 9800/12000, Avg Reward: -689.2500, Epsilon: 0.010\n",
      "Episode 9820/12000, Avg Reward: -718.5000, Epsilon: 0.010\n",
      "Episode 9840/12000, Avg Reward: -698.2500, Epsilon: 0.010\n",
      "Episode 9860/12000, Avg Reward: -679.2500, Epsilon: 0.010\n",
      "Episode 9880/12000, Avg Reward: -697.5000, Epsilon: 0.010\n",
      "Episode 9900/12000, Avg Reward: -704.0000, Epsilon: 0.010\n",
      "Episode 9920/12000, Avg Reward: -686.7500, Epsilon: 0.010\n",
      "Episode 9940/12000, Avg Reward: -709.5000, Epsilon: 0.010\n",
      "Episode 9960/12000, Avg Reward: -711.2500, Epsilon: 0.010\n",
      "Episode 9980/12000, Avg Reward: -696.2500, Epsilon: 0.010\n",
      "Episode 10000/12000, Avg Reward: -696.2500, Epsilon: 0.010\n",
      "Episode 10020/12000, Avg Reward: -691.5000, Epsilon: 0.010\n",
      "Episode 10040/12000, Avg Reward: -694.7500, Epsilon: 0.010\n",
      "Episode 10060/12000, Avg Reward: -698.5000, Epsilon: 0.010\n",
      "Episode 10080/12000, Avg Reward: -701.2500, Epsilon: 0.010\n",
      "Episode 10100/12000, Avg Reward: -695.0000, Epsilon: 0.010\n",
      "Episode 10120/12000, Avg Reward: -703.7500, Epsilon: 0.010\n",
      "Episode 10140/12000, Avg Reward: -700.5000, Epsilon: 0.010\n",
      "Episode 10160/12000, Avg Reward: -710.0000, Epsilon: 0.010\n",
      "Episode 10180/12000, Avg Reward: -696.5000, Epsilon: 0.010\n",
      "Episode 10200/12000, Avg Reward: -708.7500, Epsilon: 0.010\n",
      "Episode 10220/12000, Avg Reward: -714.0000, Epsilon: 0.010\n",
      "Episode 10240/12000, Avg Reward: -701.7500, Epsilon: 0.010\n",
      "Episode 10260/12000, Avg Reward: -695.0000, Epsilon: 0.010\n",
      "Episode 10280/12000, Avg Reward: -687.5000, Epsilon: 0.010\n",
      "Episode 10300/12000, Avg Reward: -717.0000, Epsilon: 0.010\n",
      "Episode 10320/12000, Avg Reward: -696.7500, Epsilon: 0.010\n",
      "Episode 10340/12000, Avg Reward: -709.2500, Epsilon: 0.010\n",
      "Episode 10360/12000, Avg Reward: -694.7500, Epsilon: 0.010\n",
      "Episode 10380/12000, Avg Reward: -694.5000, Epsilon: 0.010\n",
      "Episode 10400/12000, Avg Reward: -702.5000, Epsilon: 0.010\n",
      "Episode 10420/12000, Avg Reward: -693.5000, Epsilon: 0.010\n",
      "Episode 10440/12000, Avg Reward: -689.2500, Epsilon: 0.010\n",
      "Episode 10460/12000, Avg Reward: -700.0000, Epsilon: 0.010\n",
      "Episode 10480/12000, Avg Reward: -675.0000, Epsilon: 0.010\n",
      "Episode 10500/12000, Avg Reward: -684.5000, Epsilon: 0.010\n",
      "Episode 10520/12000, Avg Reward: -702.5000, Epsilon: 0.010\n",
      "Episode 10540/12000, Avg Reward: -703.7500, Epsilon: 0.010\n",
      "Episode 10560/12000, Avg Reward: -693.5000, Epsilon: 0.010\n",
      "Episode 10580/12000, Avg Reward: -704.0000, Epsilon: 0.010\n",
      "Episode 10600/12000, Avg Reward: -710.5000, Epsilon: 0.010\n",
      "Episode 10620/12000, Avg Reward: -709.0000, Epsilon: 0.010\n",
      "Episode 10640/12000, Avg Reward: -692.0000, Epsilon: 0.010\n",
      "Episode 10660/12000, Avg Reward: -698.7500, Epsilon: 0.010\n",
      "Episode 10680/12000, Avg Reward: -690.5000, Epsilon: 0.010\n",
      "Episode 10700/12000, Avg Reward: -691.7500, Epsilon: 0.010\n",
      "Episode 10720/12000, Avg Reward: -705.0000, Epsilon: 0.010\n",
      "Episode 10740/12000, Avg Reward: -693.0000, Epsilon: 0.010\n",
      "Episode 10760/12000, Avg Reward: -709.5000, Epsilon: 0.010\n",
      "Episode 10780/12000, Avg Reward: -702.7500, Epsilon: 0.010\n",
      "Episode 10800/12000, Avg Reward: -679.5000, Epsilon: 0.010\n",
      "Episode 10820/12000, Avg Reward: -694.7500, Epsilon: 0.010\n",
      "Episode 10840/12000, Avg Reward: -695.2500, Epsilon: 0.010\n",
      "Episode 10860/12000, Avg Reward: -713.7500, Epsilon: 0.010\n",
      "Episode 10880/12000, Avg Reward: -701.2500, Epsilon: 0.010\n",
      "Episode 10900/12000, Avg Reward: -698.7500, Epsilon: 0.010\n",
      "Episode 10920/12000, Avg Reward: -713.0000, Epsilon: 0.010\n",
      "Episode 10940/12000, Avg Reward: -687.5000, Epsilon: 0.010\n",
      "Episode 10960/12000, Avg Reward: -707.7500, Epsilon: 0.010\n",
      "Episode 10980/12000, Avg Reward: -695.5000, Epsilon: 0.010\n",
      "Episode 11000/12000, Avg Reward: -687.2500, Epsilon: 0.010\n",
      "Episode 11020/12000, Avg Reward: -689.0000, Epsilon: 0.010\n",
      "Episode 11040/12000, Avg Reward: -694.5000, Epsilon: 0.010\n",
      "Episode 11060/12000, Avg Reward: -703.7500, Epsilon: 0.010\n",
      "Episode 11080/12000, Avg Reward: -682.2500, Epsilon: 0.010\n",
      "Episode 11100/12000, Avg Reward: -702.5000, Epsilon: 0.010\n",
      "Episode 11120/12000, Avg Reward: -710.0000, Epsilon: 0.010\n",
      "Episode 11140/12000, Avg Reward: -690.0000, Epsilon: 0.010\n",
      "Episode 11160/12000, Avg Reward: -682.0000, Epsilon: 0.010\n",
      "Episode 11180/12000, Avg Reward: -702.2500, Epsilon: 0.010\n",
      "Episode 11200/12000, Avg Reward: -687.7500, Epsilon: 0.010\n",
      "Episode 11220/12000, Avg Reward: -696.0000, Epsilon: 0.010\n",
      "Episode 11240/12000, Avg Reward: -696.5000, Epsilon: 0.010\n",
      "Episode 11260/12000, Avg Reward: -707.0000, Epsilon: 0.010\n",
      "Episode 11280/12000, Avg Reward: -698.7500, Epsilon: 0.010\n",
      "Episode 11300/12000, Avg Reward: -682.0000, Epsilon: 0.010\n",
      "Episode 11320/12000, Avg Reward: -710.5000, Epsilon: 0.010\n",
      "Episode 11340/12000, Avg Reward: -700.0000, Epsilon: 0.010\n",
      "Episode 11360/12000, Avg Reward: -701.5000, Epsilon: 0.010\n",
      "Episode 11380/12000, Avg Reward: -689.0000, Epsilon: 0.010\n",
      "Episode 11400/12000, Avg Reward: -693.5000, Epsilon: 0.010\n",
      "Episode 11420/12000, Avg Reward: -705.5000, Epsilon: 0.010\n",
      "Episode 11440/12000, Avg Reward: -704.5000, Epsilon: 0.010\n",
      "Episode 11460/12000, Avg Reward: -711.2500, Epsilon: 0.010\n",
      "Episode 11480/12000, Avg Reward: -698.7500, Epsilon: 0.010\n",
      "Episode 11500/12000, Avg Reward: -685.0000, Epsilon: 0.010\n",
      "Episode 11520/12000, Avg Reward: -694.0000, Epsilon: 0.010\n",
      "Episode 11540/12000, Avg Reward: -703.2500, Epsilon: 0.010\n",
      "Episode 11560/12000, Avg Reward: -700.2500, Epsilon: 0.010\n",
      "Episode 11580/12000, Avg Reward: -697.5000, Epsilon: 0.010\n",
      "Episode 11600/12000, Avg Reward: -700.0000, Epsilon: 0.010\n",
      "Episode 11620/12000, Avg Reward: -712.0000, Epsilon: 0.010\n",
      "Episode 11640/12000, Avg Reward: -690.2500, Epsilon: 0.010\n",
      "Episode 11660/12000, Avg Reward: -714.0000, Epsilon: 0.010\n",
      "Episode 11680/12000, Avg Reward: -686.7500, Epsilon: 0.010\n",
      "Episode 11700/12000, Avg Reward: -696.2500, Epsilon: 0.010\n",
      "Episode 11720/12000, Avg Reward: -698.7500, Epsilon: 0.010\n",
      "Episode 11740/12000, Avg Reward: -699.5000, Epsilon: 0.010\n",
      "Episode 11760/12000, Avg Reward: -700.7500, Epsilon: 0.010\n",
      "Episode 11780/12000, Avg Reward: -698.2500, Epsilon: 0.010\n",
      "Episode 11800/12000, Avg Reward: -700.2500, Epsilon: 0.010\n",
      "Episode 11820/12000, Avg Reward: -691.7500, Epsilon: 0.010\n",
      "Episode 11840/12000, Avg Reward: -687.5000, Epsilon: 0.010\n",
      "Episode 11860/12000, Avg Reward: -709.2500, Epsilon: 0.010\n",
      "Episode 11880/12000, Avg Reward: -691.5000, Epsilon: 0.010\n",
      "Episode 11900/12000, Avg Reward: -683.5000, Epsilon: 0.010\n",
      "Episode 11920/12000, Avg Reward: -705.5000, Epsilon: 0.010\n",
      "Episode 11940/12000, Avg Reward: -694.2500, Epsilon: 0.010\n",
      "Episode 11960/12000, Avg Reward: -723.5000, Epsilon: 0.010\n",
      "Episode 11980/12000, Avg Reward: -710.2500, Epsilon: 0.010\n",
      "Episode 12000/12000, Avg Reward: -700.7500, Epsilon: 0.010\n",
      "Q-table 已儲存至 q_table.pkl\n"
     ]
    }
   ],
   "source": [
    "q_table = {}\n",
    "epsilon = hyperparameters[\"epsilon_start\"]\n",
    "\n",
    "def get_state(obs, target_loc=None, has_picked_up=False):\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col, stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\t\n",
    "\n",
    "\tassert target_loc is not None\n",
    "\tx_dir = target_loc[0] - taxi_row\n",
    "\ty_dir = target_loc[1] - taxi_col\n",
    "\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\treturn (x_dir, y_dir, obstacle_north, obstacle_south, obstacle_east, obstacle_west, has_picked_up)\n",
    "  \n",
    "def get_action(obs):\n",
    "\t\"\"\"\n",
    "\t# Selects the best action using the trained Q-table.\n",
    "\t\"\"\"\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = np.random.choice(action_nums)  # Random action\n",
    "\telse:\n",
    "\t\taction = np.argmax(q_table[get_state(obs)])  # Greedy action\n",
    "\treturn action\n",
    "\n",
    "def is_in_station(obs):\n",
    "\t\"\"\"\n",
    "\t# Checks if the taxi is in a station.\n",
    "\t\"\"\"\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col,stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\n",
    "\treturn (taxi_row, taxi_col) in stations\n",
    "\n",
    "env = SimpleTaxiEnv(**env_config)\n",
    "action_nums = 6\n",
    "rewards_per_episode = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "step_count = 0\n",
    "stations = [(0, 0), (0, 4), (4, 0), (4,4)]\n",
    "\n",
    "taxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\n",
    "if render:\n",
    "\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\taction=None, step=step_count, fuel=env.current_fuel)\n",
    "\ttime.sleep(0.5)\n",
    " \n",
    "\n",
    "for episode in range(hyperparameters[\"episodes\"]):\n",
    "\tget_state.passenger_loc, get_state.destination_loc = None, None\n",
    "\tobs, _ = env.reset()\n",
    "\tdone = False\n",
    "\ttotal_reward = 0\n",
    "\tstep_count = 0\n",
    " \n",
    "\tdestination = None\n",
    "\tvisited = []\n",
    "\thas_picked_up = False\n",
    " \n",
    "\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [(obs[2], obs[3]), (obs[4], obs[5]), (obs[6], obs[7]), (obs[8], obs[9])]\n",
    "\ttarget_loc = stations[0]\n",
    "\tstate = get_state(obs, target_loc, has_picked_up)\n",
    "\t\n",
    "\twhile not done:\t\n",
    "\t\tif state not in q_table:\n",
    "\t\t\tq_table[state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\t\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\t\taction_probs = np.ones(action_nums) / action_nums\n",
    "\t\t\t# if obstacle_south:\n",
    "\t\t\t# \taction_probs[0] = 0\n",
    "\t\t\t# if obstacle_north:\n",
    "\t\t\t# \taction_probs[1] = 0\n",
    "\t\t\t# if obstacle_east:\n",
    "\t\t\t# \taction_probs[2] = 0\n",
    "\t\t\t# if obstacle_west:\n",
    "\t\t\t# \taction_probs[3] = 0\t\n",
    "\t\t\t# # if not passenger_look or has_picked_up or not is_in_station(obs):\n",
    "\t\t\t# action_probs[4] = 0\n",
    "\t\t\t# # if not destination_look or not has_picked_up or not is_in_station(obs):\n",
    "\t\t\t# action_probs[5] = 0\n",
    "\t\t\taction_probs = action_probs / np.sum(action_probs)\n",
    "\t\t\taction = np.random.choice(action_nums, p=action_probs)  # Random action\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(q_table[state])  # Greedy action\n",
    "\t\t\n",
    "\t\tif not has_picked_up and passenger_look and is_in_station(obs) and action == 4:\n",
    "\t\t\thas_picked_up = True\n",
    "\t\tif has_picked_up and destination_look and is_in_station(obs) and action == 5:\n",
    "\t\t\tdone = True\n",
    "   \n",
    "\t\tshaped_reward = 0\n",
    "\t\tx_dir = target_loc[0] - taxi_row\n",
    "\t\ty_dir = target_loc[1] - taxi_col\n",
    "\t\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\t\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\t\t# if action == 0 :  # Move Down\n",
    "        #     next_row += 1\n",
    "        # elif action == 1:  # Move Up\n",
    "        #     next_row -= 1\n",
    "        # elif action == 2:  # Move Right\n",
    "        #     next_col += 1\n",
    "        # elif action == 3:  # Move Left\n",
    "        #     next_col -= 1\n",
    "\t\t# if y_dir == 1 and action == 2 and not obstacle_east:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if y_dir == -1 and action == 3 and not obstacle_west:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if x_dir == 1 and action == 0 and not obstacle_south:\n",
    "\t\t# \t# print (target_loc, (taxi_row, taxi_col), (x_dir, y_dir), action)\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if x_dir == -1 and action == 1 and not obstacle_north:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\tif action != 4 and passenger_look and not has_picked_up and is_in_station(obs): \n",
    "\t\t\tshaped_reward = -50\n",
    "\t\tif action != 5 and destination_look and has_picked_up and is_in_station(obs):\n",
    "\t\t\tshaped_reward = -100\n",
    "   \n",
    "\t\tif obstacle_south and action == 0:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"south\")\n",
    "\t\tif obstacle_north and action == 1:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"north\")\n",
    "\t\tif obstacle_east and action == 2:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"east\")\n",
    "\t\tif obstacle_west and action == 3:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"west\")\n",
    "\t\tif (not passenger_look or has_picked_up or not is_in_station(obs)) and action == 4:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"not passenger\")\n",
    "\t\tif (not destination_look or not has_picked_up or not is_in_station(obs)) and action == 5:\n",
    "\t\t\tshaped_reward = -100\n",
    "\t\t\t# print (\"not destination\")\n",
    "   \n",
    "\t\tnext_obs, reward, done, _ = env.step(action)\n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = next_obs\n",
    "\t\t# print (taxi_row, taxi_col)\n",
    "  \n",
    "\t\tif is_in_station(next_obs) and (taxi_row, taxi_col) not in visited:\n",
    "\t\t\tvisited.append((taxi_row, taxi_col))\n",
    "\t\t\tif destination_look:\n",
    "\t\t\t\tdestination = (taxi_row, taxi_col)\n",
    "\t\t\tif has_picked_up and destination is not None:\n",
    "\t\t\t\ttarget_loc = destination\n",
    "\t\t\telse:\n",
    "\t\t\t\t# choose a station that has not been visited yet\n",
    "\t\t\t\t# print (visited, (taxi_row, taxi_col))\n",
    "\t\t\t\tfor station in stations:\n",
    "\t\t\t\t\tif station not in visited:\n",
    "\t\t\t\t\t\ttarget_loc = station\n",
    "\t\t\t\t\t\tbreak\t\n",
    "\t\t\t\t# print (target_loc)\n",
    "\n",
    "\t\t# if done:\n",
    "\t\t# \tif not has_picked_up:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not picked up\", next_obs, action, has_picked_up, destination)\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and not destination_look:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not destination\")\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and destination_look:\n",
    "\t\t# \t\t# print (\"done\")\n",
    "\t\t# \t\tshaped_reward = 2000\n",
    "\t\t\n",
    "\t\ttotal_reward += reward\n",
    "\t\treward = 0\n",
    "\t\treward += shaped_reward\n",
    "\t\tnext_state = get_state(next_obs, target_loc, has_picked_up)\n",
    "\t\tif next_state not in q_table:\n",
    "\t\t\tq_table[next_state] = np.zeros(action_nums)\n",
    "\t\tq_table[state][action] += hyperparameters[\"alpha\"] * (reward + hyperparameters[\"gamma\"] * np.max(q_table[next_state]) - q_table[state][action])\n",
    "\t\t\n",
    "\t\tstep_count += 1\n",
    "\t\tobs = next_obs\n",
    "\t\tstate = next_state\n",
    "   \n",
    "\t\n",
    "\t\tif render:\n",
    "\t\t\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\t\t\taction=action, step=step_count, fuel=env.current_fuel)\n",
    "\t# print (step_count)\n",
    "\trewards_per_episode.append(total_reward)\n",
    "\tepsilon = max(hyperparameters[\"epsilon_end\"], epsilon * hyperparameters[\"decay_rate\"])\n",
    "\tif (episode + 1) % 20 == 0:\n",
    "\t\tavg_reward = np.mean(rewards_per_episode[-20:])\n",
    "\t\tprint(f'Episode {episode + 1}/{hyperparameters[\"episodes\"]}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.3f}')\n",
    "\t\t# print ([np.argmax(i) for i in q_table.values()])\n",
    "\t\t# print ((q_table))\n",
    "  \n",
    "filename = \"q_table.pkl\"\n",
    "\n",
    "# 儲存 Q-table\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "print(f\"Q-table 已儲存至 {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
