{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv, run_agent\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "\n",
    "env_config = {\n",
    "    \"fuel_limit\": 5000\n",
    "}\n",
    "render = False\n",
    "hyperparameters = {\n",
    "    \"alpha\": 0.01,\n",
    "\t\"gamma\": 0.99,\n",
    "\t\"epsilon_start\": 1.0, \n",
    "\t\"epsilon_end\": 0.01,\n",
    "\t\"decay_rate\": 0.9999,\n",
    "\t\"episodes\": 25000,\n",
    " \t\"max_steps\": 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/25000, Avg Reward: 15.5830, Epsilon: 0.990, Achieved: 1.00, Avg Steps: 304.17\n",
      "149\n",
      "Episode 200/25000, Avg Reward: 16.9140, Epsilon: 0.980, Achieved: 1.00, Avg Steps: 269.86\n",
      "149\n",
      "Episode 300/25000, Avg Reward: 20.3000, Epsilon: 0.970, Achieved: 1.00, Avg Steps: 260.50\n",
      "149\n",
      "Episode 400/25000, Avg Reward: 21.0660, Epsilon: 0.961, Achieved: 1.00, Avg Steps: 245.34\n",
      "149\n",
      "Episode 500/25000, Avg Reward: 23.4950, Epsilon: 0.951, Achieved: 1.00, Avg Steps: 250.05\n",
      "149\n",
      "Episode 600/25000, Avg Reward: 22.4900, Epsilon: 0.942, Achieved: 1.00, Avg Steps: 257.10\n",
      "149\n",
      "Episode 700/25000, Avg Reward: 26.3370, Epsilon: 0.932, Achieved: 1.00, Avg Steps: 228.13\n",
      "149\n",
      "Episode 800/25000, Avg Reward: 27.7560, Epsilon: 0.923, Achieved: 1.00, Avg Steps: 216.44\n",
      "149\n",
      "Episode 900/25000, Avg Reward: 30.3050, Epsilon: 0.914, Achieved: 1.00, Avg Steps: 195.95\n",
      "149\n",
      "Episode 1000/25000, Avg Reward: 31.0720, Epsilon: 0.905, Achieved: 1.00, Avg Steps: 185.28\n",
      "149\n",
      "Episode 1100/25000, Avg Reward: 29.2400, Epsilon: 0.896, Achieved: 1.00, Avg Steps: 205.60\n",
      "149\n",
      "Episode 1200/25000, Avg Reward: 30.8360, Epsilon: 0.887, Achieved: 1.00, Avg Steps: 190.64\n",
      "149\n",
      "Episode 1300/25000, Avg Reward: 31.5160, Epsilon: 0.878, Achieved: 1.00, Avg Steps: 183.84\n",
      "149\n",
      "Episode 1400/25000, Avg Reward: 29.3420, Epsilon: 0.869, Achieved: 1.00, Avg Steps: 205.58\n",
      "149\n",
      "Episode 1500/25000, Avg Reward: 32.1590, Epsilon: 0.861, Achieved: 1.00, Avg Steps: 177.41\n",
      "149\n",
      "Episode 1600/25000, Avg Reward: 31.5310, Epsilon: 0.852, Achieved: 1.00, Avg Steps: 183.69\n",
      "149\n",
      "Episode 1700/25000, Avg Reward: 30.4390, Epsilon: 0.844, Achieved: 1.00, Avg Steps: 194.61\n",
      "149\n",
      "Episode 1800/25000, Avg Reward: 32.2210, Epsilon: 0.835, Achieved: 1.00, Avg Steps: 176.79\n",
      "149\n",
      "Episode 1900/25000, Avg Reward: 31.7900, Epsilon: 0.827, Achieved: 1.00, Avg Steps: 181.10\n",
      "149\n",
      "Episode 2000/25000, Avg Reward: 33.3170, Epsilon: 0.819, Achieved: 1.00, Avg Steps: 165.83\n",
      "149\n",
      "Episode 2100/25000, Avg Reward: 32.9550, Epsilon: 0.811, Achieved: 1.00, Avg Steps: 169.45\n",
      "149\n",
      "Episode 2200/25000, Avg Reward: 35.0260, Epsilon: 0.803, Achieved: 1.00, Avg Steps: 148.74\n",
      "149\n",
      "Episode 2300/25000, Avg Reward: 30.0820, Epsilon: 0.795, Achieved: 1.00, Avg Steps: 198.18\n",
      "149\n",
      "Episode 2400/25000, Avg Reward: 36.1380, Epsilon: 0.787, Achieved: 1.00, Avg Steps: 137.62\n",
      "149\n",
      "Episode 2500/25000, Avg Reward: 34.3790, Epsilon: 0.779, Achieved: 1.00, Avg Steps: 155.21\n",
      "149\n",
      "Episode 2600/25000, Avg Reward: 36.6440, Epsilon: 0.771, Achieved: 1.00, Avg Steps: 132.56\n",
      "149\n",
      "Episode 2700/25000, Avg Reward: 30.6810, Epsilon: 0.763, Achieved: 1.00, Avg Steps: 192.19\n",
      "149\n",
      "Episode 2800/25000, Avg Reward: 34.4610, Epsilon: 0.756, Achieved: 1.00, Avg Steps: 154.39\n",
      "149\n",
      "Episode 2900/25000, Avg Reward: 35.4250, Epsilon: 0.748, Achieved: 1.00, Avg Steps: 144.75\n",
      "149\n",
      "Episode 3000/25000, Avg Reward: 35.0860, Epsilon: 0.741, Achieved: 1.00, Avg Steps: 148.14\n",
      "149\n",
      "Episode 3100/25000, Avg Reward: 37.4390, Epsilon: 0.733, Achieved: 1.00, Avg Steps: 124.61\n",
      "149\n",
      "Episode 3200/25000, Avg Reward: 33.6770, Epsilon: 0.726, Achieved: 1.00, Avg Steps: 162.23\n",
      "149\n",
      "Episode 3300/25000, Avg Reward: 28.4680, Epsilon: 0.719, Achieved: 1.00, Avg Steps: 214.32\n",
      "149\n",
      "Episode 3400/25000, Avg Reward: 32.4120, Epsilon: 0.712, Achieved: 1.00, Avg Steps: 174.88\n",
      "149\n",
      "Episode 3500/25000, Avg Reward: 30.1330, Epsilon: 0.705, Achieved: 1.00, Avg Steps: 197.67\n",
      "149\n",
      "Episode 3600/25000, Avg Reward: 37.1440, Epsilon: 0.698, Achieved: 1.00, Avg Steps: 127.56\n",
      "149\n",
      "Episode 3700/25000, Avg Reward: 34.5900, Epsilon: 0.691, Achieved: 1.00, Avg Steps: 153.10\n",
      "149\n",
      "Episode 3800/25000, Avg Reward: 34.5070, Epsilon: 0.684, Achieved: 1.00, Avg Steps: 153.93\n",
      "149\n",
      "Episode 3900/25000, Avg Reward: 31.2100, Epsilon: 0.677, Achieved: 1.00, Avg Steps: 186.90\n",
      "149\n",
      "Episode 4000/25000, Avg Reward: 27.6450, Epsilon: 0.670, Achieved: 1.00, Avg Steps: 222.55\n",
      "149\n",
      "Episode 4100/25000, Avg Reward: 31.0280, Epsilon: 0.664, Achieved: 1.00, Avg Steps: 188.72\n",
      "149\n",
      "Episode 4200/25000, Avg Reward: 35.9340, Epsilon: 0.657, Achieved: 1.00, Avg Steps: 139.66\n",
      "149\n",
      "Episode 4300/25000, Avg Reward: 37.2090, Epsilon: 0.650, Achieved: 1.00, Avg Steps: 126.91\n",
      "149\n",
      "Episode 4400/25000, Avg Reward: 31.4780, Epsilon: 0.644, Achieved: 1.00, Avg Steps: 184.22\n",
      "149\n",
      "Episode 4500/25000, Avg Reward: 35.3740, Epsilon: 0.638, Achieved: 1.00, Avg Steps: 145.26\n",
      "149\n",
      "Episode 4600/25000, Avg Reward: 29.0980, Epsilon: 0.631, Achieved: 1.00, Avg Steps: 208.02\n",
      "149\n",
      "Episode 4700/25000, Avg Reward: 36.9530, Epsilon: 0.625, Achieved: 1.00, Avg Steps: 129.47\n",
      "149\n",
      "Episode 4800/25000, Avg Reward: 36.4660, Epsilon: 0.619, Achieved: 1.00, Avg Steps: 134.34\n",
      "149\n",
      "Episode 4900/25000, Avg Reward: 34.4760, Epsilon: 0.613, Achieved: 1.00, Avg Steps: 154.24\n",
      "149\n",
      "Episode 5000/25000, Avg Reward: 31.4630, Epsilon: 0.607, Achieved: 1.00, Avg Steps: 184.37\n",
      "149\n",
      "Episode 5100/25000, Avg Reward: 39.1440, Epsilon: 0.600, Achieved: 1.00, Avg Steps: 107.56\n",
      "149\n",
      "Episode 5200/25000, Avg Reward: 38.1670, Epsilon: 0.595, Achieved: 1.00, Avg Steps: 117.33\n",
      "149\n",
      "Episode 5300/25000, Avg Reward: 28.3200, Epsilon: 0.589, Achieved: 0.99, Avg Steps: 209.80\n",
      "149\n",
      "Episode 5400/25000, Avg Reward: 34.0550, Epsilon: 0.583, Achieved: 1.00, Avg Steps: 158.45\n",
      "149\n",
      "Episode 5500/25000, Avg Reward: 32.2910, Epsilon: 0.577, Achieved: 1.00, Avg Steps: 176.09\n",
      "149\n",
      "Episode 5600/25000, Avg Reward: 36.3310, Epsilon: 0.571, Achieved: 1.00, Avg Steps: 135.69\n",
      "149\n",
      "Episode 5700/25000, Avg Reward: 34.4330, Epsilon: 0.566, Achieved: 1.00, Avg Steps: 154.67\n",
      "149\n",
      "Episode 5800/25000, Avg Reward: 30.8650, Epsilon: 0.560, Achieved: 1.00, Avg Steps: 190.35\n",
      "149\n",
      "Episode 5900/25000, Avg Reward: 30.0570, Epsilon: 0.554, Achieved: 1.00, Avg Steps: 198.43\n",
      "149\n",
      "Episode 6000/25000, Avg Reward: 33.5970, Epsilon: 0.549, Achieved: 1.00, Avg Steps: 163.03\n",
      "149\n",
      "Episode 6100/25000, Avg Reward: 32.3600, Epsilon: 0.543, Achieved: 1.00, Avg Steps: 175.40\n",
      "149\n",
      "Episode 6200/25000, Avg Reward: 34.1390, Epsilon: 0.538, Achieved: 1.00, Avg Steps: 157.61\n",
      "149\n",
      "Episode 6300/25000, Avg Reward: 32.6250, Epsilon: 0.533, Achieved: 1.00, Avg Steps: 172.75\n",
      "149\n",
      "Episode 6400/25000, Avg Reward: 34.4250, Epsilon: 0.527, Achieved: 1.00, Avg Steps: 154.75\n",
      "149\n",
      "Episode 6500/25000, Avg Reward: 29.5970, Epsilon: 0.522, Achieved: 1.00, Avg Steps: 203.03\n",
      "149\n",
      "Episode 6600/25000, Avg Reward: 33.0150, Epsilon: 0.517, Achieved: 1.00, Avg Steps: 168.85\n",
      "149\n",
      "Episode 6700/25000, Avg Reward: 32.4950, Epsilon: 0.512, Achieved: 1.00, Avg Steps: 174.05\n",
      "149\n",
      "Episode 6800/25000, Avg Reward: 33.3100, Epsilon: 0.507, Achieved: 1.00, Avg Steps: 165.90\n",
      "149\n",
      "Episode 6900/25000, Avg Reward: 30.1040, Epsilon: 0.502, Achieved: 1.00, Avg Steps: 197.96\n",
      "149\n",
      "Episode 7000/25000, Avg Reward: 31.3900, Epsilon: 0.497, Achieved: 1.00, Avg Steps: 185.10\n",
      "149\n",
      "Episode 7100/25000, Avg Reward: 29.3810, Epsilon: 0.492, Achieved: 1.00, Avg Steps: 205.19\n",
      "149\n",
      "Episode 7200/25000, Avg Reward: 35.0770, Epsilon: 0.487, Achieved: 1.00, Avg Steps: 148.23\n",
      "149\n",
      "Episode 7300/25000, Avg Reward: 29.2800, Epsilon: 0.482, Achieved: 1.00, Avg Steps: 206.20\n",
      "149\n",
      "Episode 7400/25000, Avg Reward: 31.7840, Epsilon: 0.477, Achieved: 1.00, Avg Steps: 181.16\n",
      "149\n",
      "Episode 7500/25000, Avg Reward: 27.3710, Epsilon: 0.472, Achieved: 1.00, Avg Steps: 225.29\n",
      "149\n",
      "Episode 7600/25000, Avg Reward: 38.0980, Epsilon: 0.468, Achieved: 1.00, Avg Steps: 118.02\n",
      "149\n",
      "Episode 7700/25000, Avg Reward: 33.0410, Epsilon: 0.463, Achieved: 1.00, Avg Steps: 168.59\n",
      "149\n",
      "Episode 7800/25000, Avg Reward: 25.2560, Epsilon: 0.458, Achieved: 0.99, Avg Steps: 240.44\n",
      "149\n",
      "Episode 7900/25000, Avg Reward: 37.6200, Epsilon: 0.454, Achieved: 1.00, Avg Steps: 122.80\n",
      "149\n",
      "Episode 8000/25000, Avg Reward: 30.2440, Epsilon: 0.449, Achieved: 1.00, Avg Steps: 196.56\n",
      "149\n",
      "Episode 8100/25000, Avg Reward: 29.8950, Epsilon: 0.445, Achieved: 1.00, Avg Steps: 200.05\n",
      "149\n",
      "Episode 8200/25000, Avg Reward: 35.4850, Epsilon: 0.440, Achieved: 1.00, Avg Steps: 144.15\n",
      "149\n",
      "Episode 8300/25000, Avg Reward: 38.2920, Epsilon: 0.436, Achieved: 1.00, Avg Steps: 116.08\n",
      "149\n",
      "Episode 8400/25000, Avg Reward: 39.7410, Epsilon: 0.432, Achieved: 1.00, Avg Steps: 101.59\n",
      "149\n",
      "Episode 8500/25000, Avg Reward: 37.8130, Epsilon: 0.427, Achieved: 1.00, Avg Steps: 120.87\n",
      "149\n",
      "Episode 8600/25000, Avg Reward: 28.2180, Epsilon: 0.423, Achieved: 1.00, Avg Steps: 216.82\n",
      "149\n",
      "Episode 8700/25000, Avg Reward: 36.4750, Epsilon: 0.419, Achieved: 1.00, Avg Steps: 134.25\n",
      "149\n",
      "Episode 8800/25000, Avg Reward: 38.2890, Epsilon: 0.415, Achieved: 1.00, Avg Steps: 116.11\n",
      "149\n",
      "Episode 8900/25000, Avg Reward: 28.0670, Epsilon: 0.411, Achieved: 1.00, Avg Steps: 218.33\n",
      "149\n",
      "Episode 9000/25000, Avg Reward: 27.7970, Epsilon: 0.407, Achieved: 0.99, Avg Steps: 215.03\n",
      "149\n",
      "Episode 9100/25000, Avg Reward: 36.7550, Epsilon: 0.403, Achieved: 1.00, Avg Steps: 131.45\n",
      "149\n",
      "Episode 9200/25000, Avg Reward: 36.7400, Epsilon: 0.399, Achieved: 1.00, Avg Steps: 131.60\n",
      "149\n",
      "Episode 9300/25000, Avg Reward: 33.4550, Epsilon: 0.395, Achieved: 0.99, Avg Steps: 158.45\n",
      "149\n",
      "Episode 9400/25000, Avg Reward: 42.6290, Epsilon: 0.391, Achieved: 1.00, Avg Steps: 72.71\n",
      "149\n",
      "Episode 9500/25000, Avg Reward: 32.9080, Epsilon: 0.387, Achieved: 1.00, Avg Steps: 169.92\n",
      "149\n",
      "Episode 9600/25000, Avg Reward: 34.6180, Epsilon: 0.383, Achieved: 1.00, Avg Steps: 152.82\n",
      "149\n",
      "Episode 9700/25000, Avg Reward: 34.2740, Epsilon: 0.379, Achieved: 0.99, Avg Steps: 150.26\n",
      "149\n",
      "Episode 9800/25000, Avg Reward: 38.7530, Epsilon: 0.375, Achieved: 1.00, Avg Steps: 111.47\n",
      "149\n",
      "Episode 9900/25000, Avg Reward: 35.2470, Epsilon: 0.372, Achieved: 1.00, Avg Steps: 146.53\n",
      "149\n",
      "Episode 10000/25000, Avg Reward: 39.0650, Epsilon: 0.368, Achieved: 1.00, Avg Steps: 108.35\n",
      "149\n",
      "Episode 10100/25000, Avg Reward: 39.1260, Epsilon: 0.364, Achieved: 1.00, Avg Steps: 107.74\n",
      "149\n",
      "Episode 10200/25000, Avg Reward: 21.7150, Epsilon: 0.361, Achieved: 0.98, Avg Steps: 269.85\n",
      "149\n",
      "Episode 10300/25000, Avg Reward: 25.3630, Epsilon: 0.357, Achieved: 1.00, Avg Steps: 245.37\n",
      "149\n",
      "Episode 10400/25000, Avg Reward: 21.1510, Epsilon: 0.353, Achieved: 1.00, Avg Steps: 287.49\n",
      "149\n",
      "Episode 10500/25000, Avg Reward: 29.3890, Epsilon: 0.350, Achieved: 0.99, Avg Steps: 199.11\n",
      "149\n",
      "Episode 10600/25000, Avg Reward: 30.8530, Epsilon: 0.346, Achieved: 1.00, Avg Steps: 190.47\n",
      "149\n",
      "Episode 10700/25000, Avg Reward: 26.5580, Epsilon: 0.343, Achieved: 1.00, Avg Steps: 233.42\n",
      "149\n",
      "Episode 10800/25000, Avg Reward: 33.5580, Epsilon: 0.340, Achieved: 1.00, Avg Steps: 163.42\n",
      "149\n",
      "Episode 10900/25000, Avg Reward: 23.2850, Epsilon: 0.336, Achieved: 0.99, Avg Steps: 260.15\n",
      "149\n",
      "Episode 11000/25000, Avg Reward: 35.6760, Epsilon: 0.333, Achieved: 1.00, Avg Steps: 142.24\n",
      "149\n",
      "Episode 11100/25000, Avg Reward: 36.1350, Epsilon: 0.330, Achieved: 1.00, Avg Steps: 137.65\n",
      "149\n",
      "Episode 11200/25000, Avg Reward: 38.1600, Epsilon: 0.326, Achieved: 1.00, Avg Steps: 117.40\n",
      "149\n",
      "Episode 11300/25000, Avg Reward: 31.0970, Epsilon: 0.323, Achieved: 1.00, Avg Steps: 188.03\n",
      "149\n",
      "Episode 11400/25000, Avg Reward: 38.4360, Epsilon: 0.320, Achieved: 1.00, Avg Steps: 114.64\n",
      "149\n",
      "Episode 11500/25000, Avg Reward: 40.1020, Epsilon: 0.317, Achieved: 1.00, Avg Steps: 97.98\n",
      "149\n",
      "Episode 11600/25000, Avg Reward: 35.9860, Epsilon: 0.313, Achieved: 1.00, Avg Steps: 139.14\n",
      "149\n",
      "Episode 11700/25000, Avg Reward: 38.7270, Epsilon: 0.310, Achieved: 1.00, Avg Steps: 111.73\n",
      "149\n",
      "Episode 11800/25000, Avg Reward: 42.0840, Epsilon: 0.307, Achieved: 1.00, Avg Steps: 78.16\n",
      "149\n",
      "Episode 11900/25000, Avg Reward: 40.2480, Epsilon: 0.304, Achieved: 1.00, Avg Steps: 96.52\n",
      "149\n",
      "Episode 12000/25000, Avg Reward: 42.6730, Epsilon: 0.301, Achieved: 1.00, Avg Steps: 72.27\n",
      "149\n",
      "Episode 12100/25000, Avg Reward: 34.9260, Epsilon: 0.298, Achieved: 0.99, Avg Steps: 143.74\n",
      "149\n",
      "Episode 12200/25000, Avg Reward: 34.4250, Epsilon: 0.295, Achieved: 0.99, Avg Steps: 148.75\n",
      "149\n",
      "Episode 12300/25000, Avg Reward: 41.9460, Epsilon: 0.292, Achieved: 1.00, Avg Steps: 79.54\n",
      "149\n",
      "Episode 12400/25000, Avg Reward: 33.0610, Epsilon: 0.289, Achieved: 1.00, Avg Steps: 168.39\n",
      "149\n",
      "Episode 12500/25000, Avg Reward: 37.6300, Epsilon: 0.286, Achieved: 1.00, Avg Steps: 122.70\n",
      "149\n",
      "Episode 12600/25000, Avg Reward: 32.4330, Epsilon: 0.284, Achieved: 1.00, Avg Steps: 174.67\n",
      "149\n",
      "Episode 12700/25000, Avg Reward: 35.5670, Epsilon: 0.281, Achieved: 1.00, Avg Steps: 143.33\n",
      "149\n",
      "Episode 12800/25000, Avg Reward: 36.4060, Epsilon: 0.278, Achieved: 1.00, Avg Steps: 134.94\n",
      "149\n",
      "Episode 12900/25000, Avg Reward: 31.5140, Epsilon: 0.275, Achieved: 0.99, Avg Steps: 177.86\n",
      "149\n",
      "Episode 13000/25000, Avg Reward: 37.4020, Epsilon: 0.273, Achieved: 1.00, Avg Steps: 124.98\n",
      "149\n",
      "Episode 13100/25000, Avg Reward: 35.9900, Epsilon: 0.270, Achieved: 1.00, Avg Steps: 139.10\n",
      "149\n",
      "Episode 13200/25000, Avg Reward: 38.9270, Epsilon: 0.267, Achieved: 1.00, Avg Steps: 109.73\n",
      "149\n",
      "Episode 13300/25000, Avg Reward: 21.7430, Epsilon: 0.264, Achieved: 0.99, Avg Steps: 275.57\n",
      "149\n",
      "Episode 13400/25000, Avg Reward: 34.6250, Epsilon: 0.262, Achieved: 1.00, Avg Steps: 152.75\n",
      "149\n",
      "Episode 13500/25000, Avg Reward: 34.4700, Epsilon: 0.259, Achieved: 1.00, Avg Steps: 154.30\n",
      "149\n",
      "Episode 13600/25000, Avg Reward: 40.2110, Epsilon: 0.257, Achieved: 1.00, Avg Steps: 96.89\n",
      "149\n",
      "Episode 13700/25000, Avg Reward: 34.8310, Epsilon: 0.254, Achieved: 1.00, Avg Steps: 150.69\n",
      "149\n",
      "Episode 13800/25000, Avg Reward: 41.4550, Epsilon: 0.252, Achieved: 1.00, Avg Steps: 84.45\n",
      "149\n",
      "Episode 13900/25000, Avg Reward: 24.1240, Epsilon: 0.249, Achieved: 0.99, Avg Steps: 251.76\n",
      "149\n",
      "Episode 14000/25000, Avg Reward: 35.1830, Epsilon: 0.247, Achieved: 1.00, Avg Steps: 147.17\n",
      "149\n",
      "Episode 14100/25000, Avg Reward: 40.7210, Epsilon: 0.244, Achieved: 1.00, Avg Steps: 91.79\n",
      "149\n",
      "Episode 14200/25000, Avg Reward: 38.2080, Epsilon: 0.242, Achieved: 1.00, Avg Steps: 116.92\n",
      "149\n",
      "Episode 14300/25000, Avg Reward: 37.1240, Epsilon: 0.239, Achieved: 1.00, Avg Steps: 127.76\n",
      "149\n",
      "Episode 14400/25000, Avg Reward: 35.7140, Epsilon: 0.237, Achieved: 1.00, Avg Steps: 141.86\n",
      "149\n",
      "Episode 14500/25000, Avg Reward: 33.3130, Epsilon: 0.235, Achieved: 0.99, Avg Steps: 159.87\n",
      "149\n",
      "Episode 14600/25000, Avg Reward: 33.6170, Epsilon: 0.232, Achieved: 0.99, Avg Steps: 156.83\n",
      "149\n",
      "Episode 14700/25000, Avg Reward: 27.4990, Epsilon: 0.230, Achieved: 1.00, Avg Steps: 224.01\n",
      "149\n",
      "Episode 14800/25000, Avg Reward: 32.6410, Epsilon: 0.228, Achieved: 1.00, Avg Steps: 172.59\n",
      "149\n",
      "Episode 14900/25000, Avg Reward: 35.4370, Epsilon: 0.225, Achieved: 0.99, Avg Steps: 138.63\n",
      "149\n",
      "Episode 15000/25000, Avg Reward: 29.7360, Epsilon: 0.223, Achieved: 1.00, Avg Steps: 201.64\n",
      "149\n",
      "Episode 15100/25000, Avg Reward: 30.3790, Epsilon: 0.221, Achieved: 0.99, Avg Steps: 189.21\n",
      "149\n",
      "Episode 15200/25000, Avg Reward: 34.0670, Epsilon: 0.219, Achieved: 1.00, Avg Steps: 158.33\n",
      "149\n",
      "Episode 15300/25000, Avg Reward: 34.4930, Epsilon: 0.217, Achieved: 1.00, Avg Steps: 154.07\n",
      "149\n",
      "Episode 15400/25000, Avg Reward: 33.9050, Epsilon: 0.214, Achieved: 0.99, Avg Steps: 153.95\n",
      "149\n",
      "Episode 15500/25000, Avg Reward: 40.5740, Epsilon: 0.212, Achieved: 1.00, Avg Steps: 93.26\n",
      "149\n",
      "Episode 15600/25000, Avg Reward: 34.7760, Epsilon: 0.210, Achieved: 0.99, Avg Steps: 145.24\n",
      "149\n",
      "Episode 15700/25000, Avg Reward: 34.2740, Epsilon: 0.208, Achieved: 1.00, Avg Steps: 156.26\n",
      "149\n",
      "Episode 15800/25000, Avg Reward: 35.1860, Epsilon: 0.206, Achieved: 1.00, Avg Steps: 147.14\n",
      "149\n",
      "Episode 15900/25000, Avg Reward: 30.8500, Epsilon: 0.204, Achieved: 0.98, Avg Steps: 178.50\n",
      "149\n",
      "Episode 16000/25000, Avg Reward: 37.3700, Epsilon: 0.202, Achieved: 1.00, Avg Steps: 125.30\n",
      "149\n",
      "Episode 16100/25000, Avg Reward: 28.0030, Epsilon: 0.200, Achieved: 0.99, Avg Steps: 212.97\n",
      "149\n",
      "Episode 16200/25000, Avg Reward: 31.8590, Epsilon: 0.198, Achieved: 1.00, Avg Steps: 180.41\n",
      "149\n",
      "Episode 16300/25000, Avg Reward: 32.0350, Epsilon: 0.196, Achieved: 1.00, Avg Steps: 178.65\n",
      "149\n",
      "Episode 16400/25000, Avg Reward: 20.2080, Epsilon: 0.194, Achieved: 0.98, Avg Steps: 284.92\n",
      "149\n",
      "Episode 16500/25000, Avg Reward: 32.9610, Epsilon: 0.192, Achieved: 1.00, Avg Steps: 169.39\n",
      "149\n",
      "Episode 16600/25000, Avg Reward: 36.8470, Epsilon: 0.190, Achieved: 1.00, Avg Steps: 130.53\n",
      "149\n",
      "Episode 16700/25000, Avg Reward: 38.4350, Epsilon: 0.188, Achieved: 0.99, Avg Steps: 108.65\n",
      "149\n",
      "Episode 16800/25000, Avg Reward: 44.7770, Epsilon: 0.186, Achieved: 1.00, Avg Steps: 51.23\n",
      "149\n",
      "Episode 16900/25000, Avg Reward: 30.5770, Epsilon: 0.185, Achieved: 0.99, Avg Steps: 187.23\n",
      "149\n",
      "Episode 17000/25000, Avg Reward: 28.8210, Epsilon: 0.183, Achieved: 0.99, Avg Steps: 204.79\n",
      "149\n",
      "Episode 17100/25000, Avg Reward: 38.0040, Epsilon: 0.181, Achieved: 0.99, Avg Steps: 112.96\n",
      "149\n",
      "Episode 17200/25000, Avg Reward: 28.2450, Epsilon: 0.179, Achieved: 0.99, Avg Steps: 210.55\n",
      "149\n",
      "Episode 17300/25000, Avg Reward: 43.3040, Epsilon: 0.177, Achieved: 1.00, Avg Steps: 65.96\n",
      "149\n",
      "Episode 17400/25000, Avg Reward: 28.6450, Epsilon: 0.176, Achieved: 1.00, Avg Steps: 212.55\n",
      "149\n",
      "Episode 17500/25000, Avg Reward: 31.8550, Epsilon: 0.174, Achieved: 0.99, Avg Steps: 174.45\n",
      "149\n",
      "Episode 17600/25000, Avg Reward: 30.8590, Epsilon: 0.172, Achieved: 0.99, Avg Steps: 184.41\n",
      "149\n",
      "Episode 17700/25000, Avg Reward: 39.9750, Epsilon: 0.170, Achieved: 1.00, Avg Steps: 99.25\n",
      "149\n",
      "Episode 17800/25000, Avg Reward: 42.3680, Epsilon: 0.169, Achieved: 1.00, Avg Steps: 75.32\n",
      "149\n",
      "Episode 17900/25000, Avg Reward: 42.6480, Epsilon: 0.167, Achieved: 1.00, Avg Steps: 72.52\n",
      "149\n",
      "Episode 18000/25000, Avg Reward: 37.4640, Epsilon: 0.165, Achieved: 1.00, Avg Steps: 124.36\n",
      "149\n",
      "Episode 18100/25000, Avg Reward: 32.0250, Epsilon: 0.164, Achieved: 0.99, Avg Steps: 172.75\n",
      "149\n",
      "Episode 18200/25000, Avg Reward: 38.7070, Epsilon: 0.162, Achieved: 1.00, Avg Steps: 111.93\n",
      "149\n",
      "Episode 18300/25000, Avg Reward: 37.3480, Epsilon: 0.160, Achieved: 1.00, Avg Steps: 125.52\n",
      "149\n",
      "Episode 18400/25000, Avg Reward: 29.0060, Epsilon: 0.159, Achieved: 1.00, Avg Steps: 208.94\n",
      "149\n",
      "Episode 18500/25000, Avg Reward: 32.9110, Epsilon: 0.157, Achieved: 1.00, Avg Steps: 169.89\n",
      "149\n",
      "Episode 18600/25000, Avg Reward: 25.8970, Epsilon: 0.156, Achieved: 0.99, Avg Steps: 234.03\n",
      "149\n",
      "Episode 18700/25000, Avg Reward: 31.8480, Epsilon: 0.154, Achieved: 1.00, Avg Steps: 180.52\n",
      "149\n",
      "Episode 18800/25000, Avg Reward: 40.4070, Epsilon: 0.153, Achieved: 1.00, Avg Steps: 94.93\n",
      "149\n",
      "Episode 18900/25000, Avg Reward: 43.8920, Epsilon: 0.151, Achieved: 1.00, Avg Steps: 60.08\n",
      "149\n",
      "Episode 19000/25000, Avg Reward: 41.1610, Epsilon: 0.150, Achieved: 1.00, Avg Steps: 87.39\n",
      "149\n",
      "Episode 19100/25000, Avg Reward: 46.2170, Epsilon: 0.148, Achieved: 1.00, Avg Steps: 36.83\n",
      "149\n",
      "Episode 19200/25000, Avg Reward: 35.3550, Epsilon: 0.147, Achieved: 1.00, Avg Steps: 145.45\n",
      "149\n",
      "Episode 19300/25000, Avg Reward: 35.4550, Epsilon: 0.145, Achieved: 1.00, Avg Steps: 144.45\n",
      "149\n",
      "Episode 19400/25000, Avg Reward: 34.4880, Epsilon: 0.144, Achieved: 1.00, Avg Steps: 154.12\n",
      "149\n",
      "Episode 19500/25000, Avg Reward: 41.4570, Epsilon: 0.142, Achieved: 1.00, Avg Steps: 84.43\n",
      "149\n",
      "Episode 19600/25000, Avg Reward: 38.4720, Epsilon: 0.141, Achieved: 1.00, Avg Steps: 114.28\n",
      "149\n",
      "Episode 19700/25000, Avg Reward: 40.0560, Epsilon: 0.139, Achieved: 1.00, Avg Steps: 98.44\n",
      "149\n",
      "Episode 19800/25000, Avg Reward: 36.9140, Epsilon: 0.138, Achieved: 0.99, Avg Steps: 123.86\n",
      "149\n",
      "Episode 19900/25000, Avg Reward: 43.6860, Epsilon: 0.137, Achieved: 1.00, Avg Steps: 62.14\n",
      "149\n",
      "Episode 20000/25000, Avg Reward: 34.5440, Epsilon: 0.135, Achieved: 0.99, Avg Steps: 147.56\n",
      "149\n",
      "Episode 20100/25000, Avg Reward: 29.9850, Epsilon: 0.134, Achieved: 1.00, Avg Steps: 199.15\n",
      "149\n",
      "Episode 20200/25000, Avg Reward: 36.0060, Epsilon: 0.133, Achieved: 1.00, Avg Steps: 138.94\n",
      "149\n",
      "Episode 20300/25000, Avg Reward: 36.7330, Epsilon: 0.131, Achieved: 0.99, Avg Steps: 125.67\n",
      "149\n",
      "Episode 20400/25000, Avg Reward: 42.6740, Epsilon: 0.130, Achieved: 1.00, Avg Steps: 72.26\n",
      "149\n",
      "Episode 20500/25000, Avg Reward: 38.8980, Epsilon: 0.129, Achieved: 0.99, Avg Steps: 104.02\n",
      "149\n",
      "Episode 20600/25000, Avg Reward: 44.5400, Epsilon: 0.127, Achieved: 1.00, Avg Steps: 53.60\n",
      "149\n",
      "Episode 20700/25000, Avg Reward: 30.0700, Epsilon: 0.126, Achieved: 0.99, Avg Steps: 192.30\n",
      "149\n",
      "Episode 20800/25000, Avg Reward: 34.7970, Epsilon: 0.125, Achieved: 1.00, Avg Steps: 151.03\n",
      "149\n",
      "Episode 20900/25000, Avg Reward: 43.2380, Epsilon: 0.124, Achieved: 1.00, Avg Steps: 66.62\n",
      "149\n",
      "Episode 21000/25000, Avg Reward: 36.4640, Epsilon: 0.122, Achieved: 0.99, Avg Steps: 128.36\n",
      "149\n",
      "Episode 21100/25000, Avg Reward: 38.3350, Epsilon: 0.121, Achieved: 1.00, Avg Steps: 115.65\n",
      "149\n",
      "Episode 21200/25000, Avg Reward: 33.5340, Epsilon: 0.120, Achieved: 1.00, Avg Steps: 163.66\n",
      "149\n",
      "Episode 21300/25000, Avg Reward: 42.5210, Epsilon: 0.119, Achieved: 1.00, Avg Steps: 73.79\n",
      "149\n",
      "Episode 21400/25000, Avg Reward: 41.7030, Epsilon: 0.118, Achieved: 1.00, Avg Steps: 81.97\n",
      "149\n",
      "Episode 21500/25000, Avg Reward: 47.4060, Epsilon: 0.116, Achieved: 1.00, Avg Steps: 24.94\n",
      "149\n",
      "Episode 21600/25000, Avg Reward: 32.2210, Epsilon: 0.115, Achieved: 0.99, Avg Steps: 170.79\n",
      "149\n",
      "Episode 21700/25000, Avg Reward: 43.1590, Epsilon: 0.114, Achieved: 1.00, Avg Steps: 67.41\n",
      "149\n",
      "Episode 21800/25000, Avg Reward: 40.1730, Epsilon: 0.113, Achieved: 0.99, Avg Steps: 91.27\n",
      "149\n",
      "Episode 21900/25000, Avg Reward: 39.3640, Epsilon: 0.112, Achieved: 0.99, Avg Steps: 99.36\n",
      "149\n",
      "Episode 22000/25000, Avg Reward: 32.9720, Epsilon: 0.111, Achieved: 0.99, Avg Steps: 163.28\n",
      "149\n",
      "Episode 22100/25000, Avg Reward: 38.7390, Epsilon: 0.110, Achieved: 1.00, Avg Steps: 111.61\n",
      "149\n",
      "Episode 22200/25000, Avg Reward: 34.9060, Epsilon: 0.109, Achieved: 1.00, Avg Steps: 149.94\n",
      "149\n",
      "Episode 22300/25000, Avg Reward: 34.1980, Epsilon: 0.108, Achieved: 0.99, Avg Steps: 151.02\n",
      "149\n",
      "Episode 22400/25000, Avg Reward: 36.4000, Epsilon: 0.106, Achieved: 1.00, Avg Steps: 135.00\n",
      "149\n",
      "Episode 22500/25000, Avg Reward: 23.0000, Epsilon: 0.105, Achieved: 0.99, Avg Steps: 263.00\n",
      "149\n",
      "Episode 22600/25000, Avg Reward: 32.6270, Epsilon: 0.104, Achieved: 0.99, Avg Steps: 166.73\n",
      "149\n",
      "Episode 22700/25000, Avg Reward: 44.3720, Epsilon: 0.103, Achieved: 1.00, Avg Steps: 55.28\n",
      "149\n",
      "Episode 22800/25000, Avg Reward: 37.5480, Epsilon: 0.102, Achieved: 1.00, Avg Steps: 123.52\n",
      "149\n",
      "Episode 22900/25000, Avg Reward: 41.9380, Epsilon: 0.101, Achieved: 1.00, Avg Steps: 79.62\n",
      "149\n",
      "Episode 23000/25000, Avg Reward: 34.0180, Epsilon: 0.100, Achieved: 0.98, Avg Steps: 146.82\n",
      "149\n",
      "Episode 23100/25000, Avg Reward: 34.3710, Epsilon: 0.099, Achieved: 0.98, Avg Steps: 143.29\n",
      "149\n",
      "Episode 23200/25000, Avg Reward: 47.6090, Epsilon: 0.098, Achieved: 1.00, Avg Steps: 22.91\n",
      "149\n",
      "Episode 23300/25000, Avg Reward: 47.4700, Epsilon: 0.097, Achieved: 1.00, Avg Steps: 24.30\n",
      "149\n",
      "Episode 23400/25000, Avg Reward: 47.5290, Epsilon: 0.096, Achieved: 1.00, Avg Steps: 23.71\n",
      "149\n",
      "Episode 23500/25000, Avg Reward: 41.3360, Epsilon: 0.095, Achieved: 0.99, Avg Steps: 79.64\n",
      "149\n",
      "Episode 23600/25000, Avg Reward: 46.1320, Epsilon: 0.094, Achieved: 1.00, Avg Steps: 37.68\n",
      "149\n",
      "Episode 23700/25000, Avg Reward: 24.7990, Epsilon: 0.093, Achieved: 0.97, Avg Steps: 233.01\n",
      "149\n",
      "Episode 23800/25000, Avg Reward: 40.4860, Epsilon: 0.093, Achieved: 1.00, Avg Steps: 94.14\n",
      "149\n",
      "Episode 23900/25000, Avg Reward: 37.6660, Epsilon: 0.092, Achieved: 0.99, Avg Steps: 116.34\n",
      "149\n",
      "Episode 24000/25000, Avg Reward: 47.1820, Epsilon: 0.091, Achieved: 1.00, Avg Steps: 27.18\n",
      "149\n",
      "Episode 24100/25000, Avg Reward: 28.3580, Epsilon: 0.090, Achieved: 0.99, Avg Steps: 209.42\n",
      "149\n",
      "Episode 24200/25000, Avg Reward: 41.1220, Epsilon: 0.089, Achieved: 0.99, Avg Steps: 81.78\n",
      "149\n",
      "Episode 24300/25000, Avg Reward: 43.9340, Epsilon: 0.088, Achieved: 1.00, Avg Steps: 59.66\n",
      "149\n",
      "Episode 24400/25000, Avg Reward: 34.3580, Epsilon: 0.087, Achieved: 0.99, Avg Steps: 149.42\n",
      "149\n",
      "Episode 24500/25000, Avg Reward: 37.2130, Epsilon: 0.086, Achieved: 0.99, Avg Steps: 120.87\n",
      "149\n",
      "Episode 24600/25000, Avg Reward: 39.5350, Epsilon: 0.085, Achieved: 1.00, Avg Steps: 103.65\n",
      "149\n",
      "Episode 24700/25000, Avg Reward: 43.4650, Epsilon: 0.085, Achieved: 1.00, Avg Steps: 64.35\n",
      "149\n",
      "Episode 24800/25000, Avg Reward: 42.7660, Epsilon: 0.084, Achieved: 1.00, Avg Steps: 71.34\n",
      "149\n",
      "Episode 24900/25000, Avg Reward: 45.0990, Epsilon: 0.083, Achieved: 1.00, Avg Steps: 48.01\n",
      "149\n",
      "Episode 25000/25000, Avg Reward: 47.6210, Epsilon: 0.082, Achieved: 1.00, Avg Steps: 22.79\n",
      "149\n",
      "Q-table 已儲存至 q_table_1.pkl\n"
     ]
    }
   ],
   "source": [
    "q_table = {}\n",
    "epsilon = hyperparameters[\"epsilon_start\"]\n",
    "\n",
    "def random_action(passenger_look, destination_look, has_picked_up, obs):\n",
    "\taction_probs = np.ones(action_nums) / action_nums\n",
    "\tif obstacle_south:\n",
    "\t\taction_probs[0] = 0\n",
    "\tif obstacle_north:\n",
    "\t\taction_probs[1] = 0\n",
    "\tif obstacle_east:\n",
    "\t\taction_probs[2] = 0\n",
    "\tif obstacle_west:\n",
    "\t\taction_probs[3] = 0\t\n",
    "\tif not passenger_look or has_picked_up or not is_in_station(obs):\n",
    "\t\taction_probs[4] = 0\n",
    "\tif not destination_look or not has_picked_up or not is_in_station(obs):\n",
    "\t\taction_probs[5] = 0\n",
    "\tif np.sum(action_probs) == 0:\n",
    "\t\tprint (obs)\n",
    "\t\tprint (action_probs)\n",
    "\t\taction_probs = np.ones(action_nums) / action_nums\n",
    "\taction_probs = action_probs / np.sum(action_probs)\n",
    "\taction = np.random.choice(action_nums, p=action_probs)  # Random action\n",
    "\treturn action\n",
    "\n",
    "def get_state(obs, target_loc=None, has_picked_up=False):\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col, stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\t\n",
    "\n",
    "\tassert target_loc is not None\n",
    "\tx_dir = target_loc[0] - taxi_row\n",
    "\ty_dir = target_loc[1] - taxi_col\n",
    "\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\treturn (x_dir, y_dir, obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look, has_picked_up)\n",
    "\t# return (obstacle_north, obstacle_south, obstacle_east, obstacle_west)\n",
    "  \n",
    "def get_action(obs):\n",
    "\t\"\"\"\n",
    "\t# Selects the best action using the trained Q-table.\n",
    "\t\"\"\"\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = np.random.choice(action_nums)  # Random action\n",
    "\telse:\n",
    "\t\taction = np.argmax(q_table[get_state(obs)])  # Greedy action\n",
    "\treturn action\n",
    "\n",
    "def is_in_station(obs):\n",
    "\t\"\"\"\n",
    "\t# Checks if the taxi is in a station.\n",
    "\t\"\"\"\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col,stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\n",
    "\treturn (taxi_row, taxi_col) in stations\n",
    "\n",
    "action_nums = 6\n",
    "rewards_per_episode = []\n",
    "env = SimpleTaxiEnv(**env_config)\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "step_count = 0\n",
    "stations = [(0, 0), (0, 4), (4, 0), (4,4)]\n",
    "\n",
    "taxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\n",
    "if render:\n",
    "\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\taction=None, step=step_count, fuel=env.current_fuel)\n",
    "\ttime.sleep(0.5)\n",
    " \n",
    "\n",
    "achieved = []\n",
    "step_counts = []\n",
    "for episode in range(hyperparameters[\"episodes\"]):\n",
    "\tget_state.passenger_loc, get_state.destination_loc = None, None\n",
    "\tenv = SimpleTaxiEnv(grid_size=np.random.randint(5, 10), **env_config)\n",
    "\tobs, _ = env.reset()\n",
    "\tdone = False\n",
    "\ttotal_reward = 0\n",
    "\tstep_count = 0\n",
    " \n",
    "\tdestination = None\n",
    "\tvisited = []\n",
    "\thas_picked_up = False\n",
    "\tviolations = []\n",
    " \n",
    "\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [(obs[2], obs[3]), (obs[4], obs[5]), (obs[6], obs[7]), (obs[8], obs[9])]\n",
    "\ttarget_loc = random.choice(stations)\n",
    "\tstate = get_state(obs, target_loc, has_picked_up)\n",
    "\t\n",
    "\twhile not done:\t\n",
    "\t\tif state not in q_table:\n",
    "\t\t\tq_table[state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\t\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\t\taction = random_action(passenger_look, destination_look, has_picked_up, obs)\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(q_table[state])  # Greedy action\n",
    "\t\t\n",
    "\t\tshaped_reward = 0\n",
    "   \n",
    "\t\tx_dir = target_loc[0] - taxi_row\n",
    "\t\ty_dir = target_loc[1] - taxi_col\n",
    "\t\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\t\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\t\t# if action == 0 :  # Move Down\n",
    "        #     next_row += 1\n",
    "        # elif action == 1:  # Move Up\n",
    "        #     next_row -= 1\n",
    "        # elif action == 2:  # Move Right\n",
    "        #     next_col += 1\n",
    "        # elif action == 3:  # Move Left\n",
    "        #     next_col -= 1\n",
    "\t\t# if y_dir == 1 and action == 2 and not obstacle_east:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if y_dir == -1 and action == 3 and not obstacle_west:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if x_dir == 1 and action == 0 and not obstacle_south:\n",
    "\t\t# \t# print (target_loc, (taxi_row, taxi_col), (x_dir, y_dir), action)\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if x_dir == -1 and action == 1 and not obstacle_north:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\tif action == 4 and passenger_look and not has_picked_up and is_in_station(obs): \n",
    "\t\t\tshaped_reward += 100\n",
    "\t\t\thas_picked_up = True\n",
    "\t\tif action == 5 and destination_look and has_picked_up and is_in_station(obs):\n",
    "\t\t\tshaped_reward += 100\n",
    "\t\t\tdone = True\n",
    "   \n",
    "\t\tif obstacle_south and action == 0:\n",
    "\t\t\tviolations.append(0)\n",
    "\t\t\tshaped_reward += -20\n",
    "\t\t\t# print (\"south\")\n",
    "\t\t\t# print ((taxi_row, taxi_col), (x_dir, y_dir), obstacle_south)\n",
    "\t\tif obstacle_north and action == 1:\n",
    "\t\t\tviolations.append(1)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"north\")\n",
    "\t\tif obstacle_east and action == 2:\n",
    "\t\t\tviolations.append(2)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"east\")\n",
    "\t\tif obstacle_west and action == 3:\n",
    "\t\t\tviolations.append(3)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"west\")\n",
    "\t\tif (not passenger_look or has_picked_up or not is_in_station(obs)) and action == 4:\n",
    "\t\t\tviolations.append(4)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"not passenger\")\n",
    "\t\tif (not destination_look or not has_picked_up or not is_in_station(obs)) and action == 5:\n",
    "\t\t\tviolations.append(5)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"not destination\")\n",
    "\n",
    "\t\tprev_dist = abs(taxi_row - target_loc[0]) + abs(taxi_col - target_loc[1])\n",
    "  \n",
    "\t\tnext_obs, reward, done, _ = env.step(action)\n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = next_obs\n",
    "  \n",
    "\t\t# next_dist = abs(taxi_row - target_loc[0]) + abs(taxi_col - target_loc[1])\n",
    "\t\t# if next_dist < prev_dist:\n",
    "\t\t# \tshaped_reward += 10\n",
    "  \n",
    "\t\tif is_in_station(next_obs):\n",
    "\t\t\tvisited.append((taxi_row, taxi_col))\n",
    "\t\t\tif destination_look:\n",
    "\t\t\t\tdestination = (taxi_row, taxi_col)\n",
    "\t\t\tif has_picked_up and destination is not None:\n",
    "\t\t\t\ttarget_loc = destination\n",
    "\t\t\telse:\n",
    "\t\t\t\t# choose a station that has not been visited yet\n",
    "\t\t\t\t# print (visited, (taxi_row, taxi_col))\n",
    "\t\t\t\tfor station in stations:\n",
    "\t\t\t\t\tif station not in visited:\n",
    "\t\t\t\t\t\ttarget_loc = station\n",
    "\t\t\t\t\t\tbreak\t\n",
    "\t\t\t\t# print (target_loc)\n",
    "\t\t# print (destination, (taxi_row, taxi_col), action, shaped_reward)\n",
    "\t\tif done:\n",
    "\t\t\tif step_count == 4999:\n",
    "\t\t\t\tachieved.append(0)\n",
    "\t\t\telif step_count < 4999:\n",
    "\t\t\t\tachieved.append(1)\n",
    "\t\t\t\tshaped_reward += 200\n",
    "\t\t\tstep_counts.append(step_count)\n",
    "\t\t# if done:\n",
    "\t\t# \tif not has_picked_up:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not picked up\", next_obs, action, has_picked_up, destination)\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and not destination_look:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not destination\")\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and destination_look:\n",
    "\t\t# \t\t# print (\"done\")\n",
    "\t\t# \t\tshaped_reward = 100\n",
    "\t\t# \t\tstep_counts.append(step_count)\n",
    "   \n",
    "\t\ttotal_reward += reward\n",
    "\t\treward += shaped_reward\n",
    "\t\tnext_state = get_state(next_obs, target_loc, has_picked_up)\n",
    "  \n",
    "\t\tif next_state not in q_table:\n",
    "\t\t\tq_table[next_state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\tq_table[state][action] += hyperparameters[\"alpha\"] * (reward + hyperparameters[\"gamma\"] * np.max(q_table[next_state]) - q_table[state][action])\n",
    "\t\t\n",
    "\t\tstep_count += 1\n",
    "\t\tobs = next_obs\n",
    "\t\tstate = next_state\n",
    "\t\t\n",
    "\t\tif render:\n",
    "\t\t\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\t\t\taction=action, step=step_count, fuel=env.current_fuel)\n",
    "\t# print (np.sum(np.array(violations) == 0), np.sum(np.array(violations) == 1), np.sum(np.array(violations) == 2), np.sum(np.array(violations) == 3), np.sum(np.array(violations) == 4), np.sum(np.array(violations) == 5))\n",
    "\trewards_per_episode.append(total_reward)\n",
    "\tepsilon = max(hyperparameters[\"epsilon_end\"], epsilon * hyperparameters[\"decay_rate\"])\n",
    "\tif (episode + 1) % 100 == 0:\n",
    "\t\tavg_reward = np.mean(rewards_per_episode[-100:])\n",
    "\t\tprint(f'Episode {episode + 1}/{hyperparameters[\"episodes\"]}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.3f}, Achieved: {np.mean(achieved[-100:]):.2f}, Avg Steps: {np.mean(step_counts[-100:]):.2f}')\n",
    "\t\t# print ([np.argmax(i) for i in q_table.values()])\n",
    "\t\tprint (len(q_table))\n",
    "  \n",
    "filename = \"q_table_1.pkl\"\n",
    "\n",
    "# 儲存 Q-table\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "print(f\"Q-table 已儲存至 {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table 已儲存至 q_table_1.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = \"q_table_1.pkl\"\n",
    "\n",
    "# 儲存 Q-table\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "print(f\"Q-table 已儲存至 {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n",
      "[0, 0]\n",
      "obs= (1, 0, 0, 0, 0, 4, 4, 0, 4, 4, 0, 0, 0, 1, 0, 0)\n",
      "obs= (0, 0, 0, 0, 0, 4, 4, 0, 4, 4, 1, 0, 0, 1, 0, 0)\n",
      "obs= (0, 1, 0, 0, 0, 4, 4, 0, 4, 4, 1, 0, 0, 0, 0, 0)\n",
      "obs= (0, 2, 0, 0, 0, 4, 4, 0, 4, 4, 1, 0, 0, 0, 0, 0)\n",
      "obs= (0, 3, 0, 0, 0, 4, 4, 0, 4, 4, 1, 0, 0, 0, 1, 0)\n",
      "obs= (0, 4, 0, 0, 0, 4, 4, 0, 4, 4, 1, 0, 1, 0, 1, 0)\n",
      "obs= (0, 4, 0, 0, 0, 4, 4, 0, 4, 4, 1, 0, 1, 0, 1, 0)\n",
      "obs= (0, 3, 0, 0, 0, 4, 4, 0, 4, 4, 1, 0, 0, 0, 1, 0)\n",
      "obs= (1, 3, 0, 0, 0, 4, 4, 0, 4, 4, 0, 0, 0, 0, 1, 0)\n",
      "obs= (1, 2, 0, 0, 0, 4, 4, 0, 4, 4, 0, 0, 0, 0, 1, 0)\n",
      "obs= (1, 1, 0, 0, 0, 4, 4, 0, 4, 4, 0, 0, 0, 0, 1, 0)\n",
      "obs= (1, 0, 0, 0, 0, 4, 4, 0, 4, 4, 0, 0, 0, 1, 1, 0)\n",
      "obs= (2, 0, 0, 0, 0, 4, 4, 0, 4, 4, 0, 0, 0, 1, 1, 0)\n",
      "obs= (3, 0, 0, 0, 0, 4, 4, 0, 4, 4, 0, 0, 0, 1, 1, 1)\n",
      "obs= (4, 0, 0, 0, 0, 4, 4, 0, 4, 4, 0, 1, 0, 1, 1, 1)\n",
      "obs= (4, 0, 0, 0, 0, 4, 4, 0, 4, 4, 0, 1, 0, 1, 1, 1)\n",
      "Agent Finished in 16 steps, Score: 48.4\n",
      "Final Score: 48.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_config = {\n",
    "        # \"grid_size\": np.random.randint(5, 6),\n",
    "        \"fuel_limit\": 5000\n",
    "    }\n",
    "    \n",
    "agent_score = run_agent(\"student_agent.py\", env_config, render=False)\n",
    "print(f\"Final Score: {agent_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
