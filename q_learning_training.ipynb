{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv, run_agent\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "\n",
    "env_config = {\n",
    "    \"fuel_limit\": 5000\n",
    "}\n",
    "render = False\n",
    "hyperparameters = {\n",
    "    \"alpha\": 0.01,\n",
    "\t\"gamma\": 0.99,\n",
    "\t\"epsilon_start\": 1.0, \n",
    "\t\"epsilon_end\": 0.01,\n",
    "\t\"decay_rate\": 0.9999,\n",
    "\t\"episodes\": 25000,\n",
    " \t\"max_steps\": 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/25000, Avg Reward: -81.0370, Epsilon: 0.990, Achieved: 0.94, Avg Steps: 1031.37\n",
      "190\n",
      "Episode 200/25000, Avg Reward: -32.6020, Epsilon: 0.980, Achieved: 0.99, Avg Steps: 737.02\n",
      "194\n",
      "Episode 300/25000, Avg Reward: -37.9350, Epsilon: 0.970, Achieved: 0.99, Avg Steps: 812.35\n",
      "194\n",
      "Episode 400/25000, Avg Reward: -4.8930, Epsilon: 0.961, Achieved: 1.00, Avg Steps: 521.43\n",
      "194\n",
      "Episode 500/25000, Avg Reward: -7.1010, Epsilon: 0.951, Achieved: 1.00, Avg Steps: 560.01\n",
      "194\n",
      "Episode 600/25000, Avg Reward: -8.7810, Epsilon: 0.942, Achieved: 1.00, Avg Steps: 574.31\n",
      "194\n",
      "Episode 700/25000, Avg Reward: -9.9470, Epsilon: 0.932, Achieved: 1.00, Avg Steps: 583.97\n",
      "194\n",
      "Episode 800/25000, Avg Reward: 4.3540, Epsilon: 0.923, Achieved: 1.00, Avg Steps: 449.46\n",
      "194\n",
      "Episode 900/25000, Avg Reward: 0.7840, Epsilon: 0.914, Achieved: 1.00, Avg Steps: 488.66\n",
      "194\n",
      "Episode 1000/25000, Avg Reward: 7.1850, Epsilon: 0.905, Achieved: 1.00, Avg Steps: 427.15\n",
      "194\n",
      "Episode 1100/25000, Avg Reward: 0.5290, Epsilon: 0.896, Achieved: 1.00, Avg Steps: 493.71\n",
      "194\n",
      "Episode 1200/25000, Avg Reward: -1.1880, Epsilon: 0.887, Achieved: 0.99, Avg Steps: 504.88\n",
      "194\n",
      "Episode 1300/25000, Avg Reward: 5.9990, Epsilon: 0.878, Achieved: 1.00, Avg Steps: 439.01\n",
      "194\n",
      "Episode 1400/25000, Avg Reward: 2.8780, Epsilon: 0.869, Achieved: 1.00, Avg Steps: 470.22\n",
      "194\n",
      "Episode 1500/25000, Avg Reward: -2.4920, Epsilon: 0.861, Achieved: 0.99, Avg Steps: 517.92\n",
      "194\n",
      "Episode 1600/25000, Avg Reward: -1.8360, Epsilon: 0.852, Achieved: 1.00, Avg Steps: 517.36\n",
      "194\n",
      "Episode 1700/25000, Avg Reward: 10.0950, Epsilon: 0.844, Achieved: 1.00, Avg Steps: 398.05\n",
      "194\n",
      "Episode 1800/25000, Avg Reward: 14.0800, Epsilon: 0.835, Achieved: 1.00, Avg Steps: 358.20\n",
      "194\n",
      "Episode 1900/25000, Avg Reward: 10.9510, Epsilon: 0.827, Achieved: 1.00, Avg Steps: 389.49\n",
      "194\n",
      "Episode 2000/25000, Avg Reward: 8.8920, Epsilon: 0.819, Achieved: 0.99, Avg Steps: 404.08\n",
      "194\n",
      "Episode 2100/25000, Avg Reward: 13.8290, Epsilon: 0.811, Achieved: 1.00, Avg Steps: 360.71\n",
      "194\n",
      "Episode 2200/25000, Avg Reward: 0.2330, Epsilon: 0.803, Achieved: 0.99, Avg Steps: 490.67\n",
      "194\n",
      "Episode 2300/25000, Avg Reward: 18.3050, Epsilon: 0.795, Achieved: 1.00, Avg Steps: 315.95\n",
      "194\n",
      "Episode 2400/25000, Avg Reward: 24.6000, Epsilon: 0.787, Achieved: 1.00, Avg Steps: 253.00\n",
      "194\n",
      "Episode 2500/25000, Avg Reward: 23.1770, Epsilon: 0.779, Achieved: 1.00, Avg Steps: 267.23\n",
      "194\n",
      "Episode 2600/25000, Avg Reward: -2.1050, Epsilon: 0.771, Achieved: 0.98, Avg Steps: 508.05\n",
      "194\n",
      "Episode 2700/25000, Avg Reward: 17.7550, Epsilon: 0.763, Achieved: 1.00, Avg Steps: 321.45\n",
      "194\n",
      "Episode 2800/25000, Avg Reward: 10.2680, Epsilon: 0.756, Achieved: 0.99, Avg Steps: 390.32\n",
      "194\n",
      "Episode 2900/25000, Avg Reward: 12.9080, Epsilon: 0.748, Achieved: 1.00, Avg Steps: 369.92\n",
      "194\n",
      "Episode 3000/25000, Avg Reward: 23.1730, Epsilon: 0.741, Achieved: 1.00, Avg Steps: 267.27\n",
      "194\n",
      "Episode 3100/25000, Avg Reward: -1.6290, Epsilon: 0.733, Achieved: 0.99, Avg Steps: 509.29\n",
      "194\n",
      "Episode 3200/25000, Avg Reward: 28.4870, Epsilon: 0.726, Achieved: 1.00, Avg Steps: 214.13\n",
      "194\n",
      "Episode 3300/25000, Avg Reward: 22.7390, Epsilon: 0.719, Achieved: 1.00, Avg Steps: 271.61\n",
      "194\n",
      "Episode 3400/25000, Avg Reward: 5.7380, Epsilon: 0.712, Achieved: 0.98, Avg Steps: 429.62\n",
      "194\n",
      "Episode 3500/25000, Avg Reward: 12.8330, Epsilon: 0.705, Achieved: 0.98, Avg Steps: 358.67\n",
      "194\n",
      "Episode 3600/25000, Avg Reward: 9.9160, Epsilon: 0.698, Achieved: 0.97, Avg Steps: 381.84\n",
      "194\n",
      "Episode 3700/25000, Avg Reward: 0.0360, Epsilon: 0.691, Achieved: 0.95, Avg Steps: 468.64\n",
      "194\n",
      "Episode 3800/25000, Avg Reward: -5.4820, Epsilon: 0.684, Achieved: 0.96, Avg Steps: 529.82\n",
      "194\n",
      "Episode 3900/25000, Avg Reward: 15.1830, Epsilon: 0.677, Achieved: 0.99, Avg Steps: 341.17\n",
      "194\n",
      "Episode 4000/25000, Avg Reward: 28.0290, Epsilon: 0.670, Achieved: 1.00, Avg Steps: 218.71\n",
      "194\n",
      "Episode 4100/25000, Avg Reward: 10.4390, Epsilon: 0.664, Achieved: 0.99, Avg Steps: 388.61\n",
      "194\n",
      "Episode 4200/25000, Avg Reward: 4.8960, Epsilon: 0.657, Achieved: 0.99, Avg Steps: 444.04\n",
      "194\n",
      "Episode 4300/25000, Avg Reward: -0.9700, Epsilon: 0.650, Achieved: 0.98, Avg Steps: 496.70\n",
      "194\n",
      "Episode 4400/25000, Avg Reward: 19.0070, Epsilon: 0.644, Achieved: 1.00, Avg Steps: 308.93\n",
      "194\n",
      "Episode 4500/25000, Avg Reward: 17.7960, Epsilon: 0.638, Achieved: 1.00, Avg Steps: 321.04\n",
      "194\n",
      "Episode 4600/25000, Avg Reward: 13.6100, Epsilon: 0.631, Achieved: 0.98, Avg Steps: 350.90\n",
      "194\n",
      "Episode 4700/25000, Avg Reward: 11.1660, Epsilon: 0.625, Achieved: 0.98, Avg Steps: 375.34\n",
      "194\n",
      "Episode 4800/25000, Avg Reward: 3.0210, Epsilon: 0.619, Achieved: 0.97, Avg Steps: 450.79\n",
      "194\n",
      "Episode 4900/25000, Avg Reward: 10.3050, Epsilon: 0.613, Achieved: 0.99, Avg Steps: 389.95\n",
      "194\n",
      "Episode 5000/25000, Avg Reward: 13.0660, Epsilon: 0.607, Achieved: 0.98, Avg Steps: 356.34\n",
      "194\n",
      "Episode 5100/25000, Avg Reward: 13.2670, Epsilon: 0.600, Achieved: 0.99, Avg Steps: 360.33\n",
      "194\n",
      "Episode 5200/25000, Avg Reward: 20.7040, Epsilon: 0.595, Achieved: 1.00, Avg Steps: 291.96\n",
      "194\n",
      "Episode 5300/25000, Avg Reward: 21.7720, Epsilon: 0.589, Achieved: 1.00, Avg Steps: 281.28\n",
      "194\n",
      "Episode 5400/25000, Avg Reward: 8.3770, Epsilon: 0.583, Achieved: 0.98, Avg Steps: 403.23\n",
      "194\n",
      "Episode 5500/25000, Avg Reward: 3.8320, Epsilon: 0.577, Achieved: 0.97, Avg Steps: 442.68\n",
      "194\n",
      "Episode 5600/25000, Avg Reward: 5.8150, Epsilon: 0.571, Achieved: 0.95, Avg Steps: 410.85\n",
      "194\n",
      "Episode 5700/25000, Avg Reward: 12.2120, Epsilon: 0.566, Achieved: 0.99, Avg Steps: 370.88\n",
      "194\n",
      "Episode 5800/25000, Avg Reward: 17.7230, Epsilon: 0.560, Achieved: 0.98, Avg Steps: 309.77\n",
      "194\n",
      "Episode 5900/25000, Avg Reward: 12.3400, Epsilon: 0.554, Achieved: 0.98, Avg Steps: 363.60\n",
      "194\n",
      "Episode 6000/25000, Avg Reward: 24.6210, Epsilon: 0.549, Achieved: 0.99, Avg Steps: 246.79\n",
      "194\n",
      "Episode 6100/25000, Avg Reward: 14.4210, Epsilon: 0.543, Achieved: 0.98, Avg Steps: 342.79\n",
      "194\n",
      "Episode 6200/25000, Avg Reward: 22.1400, Epsilon: 0.538, Achieved: 0.99, Avg Steps: 271.60\n",
      "194\n",
      "Episode 6300/25000, Avg Reward: 11.2270, Epsilon: 0.533, Achieved: 0.97, Avg Steps: 368.73\n",
      "194\n",
      "Episode 6400/25000, Avg Reward: 20.6660, Epsilon: 0.527, Achieved: 1.00, Avg Steps: 292.34\n",
      "194\n",
      "Episode 6500/25000, Avg Reward: 15.3080, Epsilon: 0.522, Achieved: 0.98, Avg Steps: 333.92\n",
      "194\n",
      "Episode 6600/25000, Avg Reward: 18.5030, Epsilon: 0.517, Achieved: 0.98, Avg Steps: 301.97\n",
      "194\n",
      "Episode 6700/25000, Avg Reward: 35.1730, Epsilon: 0.512, Achieved: 1.00, Avg Steps: 147.27\n",
      "194\n",
      "Episode 6800/25000, Avg Reward: 7.5730, Epsilon: 0.507, Achieved: 0.98, Avg Steps: 411.27\n",
      "194\n",
      "Episode 6900/25000, Avg Reward: 9.6290, Epsilon: 0.502, Achieved: 0.98, Avg Steps: 390.71\n",
      "194\n",
      "Episode 7000/25000, Avg Reward: 11.9460, Epsilon: 0.497, Achieved: 0.98, Avg Steps: 367.54\n",
      "194\n",
      "Episode 7100/25000, Avg Reward: 13.6540, Epsilon: 0.492, Achieved: 0.98, Avg Steps: 350.46\n",
      "194\n",
      "Episode 7200/25000, Avg Reward: 26.2580, Epsilon: 0.487, Achieved: 0.99, Avg Steps: 230.42\n",
      "194\n",
      "Episode 7300/25000, Avg Reward: -7.8010, Epsilon: 0.482, Achieved: 0.95, Avg Steps: 547.01\n",
      "194\n",
      "Episode 7400/25000, Avg Reward: 6.0500, Epsilon: 0.477, Achieved: 0.98, Avg Steps: 426.50\n",
      "194\n",
      "Episode 7500/25000, Avg Reward: 26.4710, Epsilon: 0.472, Achieved: 0.98, Avg Steps: 222.29\n",
      "194\n",
      "Episode 7600/25000, Avg Reward: 12.6500, Epsilon: 0.468, Achieved: 0.96, Avg Steps: 348.50\n",
      "194\n",
      "Episode 7700/25000, Avg Reward: 6.2760, Epsilon: 0.463, Achieved: 0.98, Avg Steps: 424.24\n",
      "194\n",
      "Episode 7800/25000, Avg Reward: 18.5080, Epsilon: 0.458, Achieved: 0.99, Avg Steps: 307.92\n",
      "194\n",
      "Episode 7900/25000, Avg Reward: 18.5370, Epsilon: 0.454, Achieved: 0.99, Avg Steps: 307.63\n",
      "194\n",
      "Episode 8000/25000, Avg Reward: 20.6010, Epsilon: 0.449, Achieved: 0.99, Avg Steps: 286.99\n",
      "194\n",
      "Episode 8100/25000, Avg Reward: 8.6410, Epsilon: 0.445, Achieved: 0.99, Avg Steps: 406.59\n",
      "194\n",
      "Episode 8200/25000, Avg Reward: 13.7020, Epsilon: 0.440, Achieved: 0.98, Avg Steps: 349.98\n",
      "194\n",
      "Episode 8300/25000, Avg Reward: 15.4370, Epsilon: 0.436, Achieved: 0.98, Avg Steps: 332.63\n",
      "194\n",
      "Episode 8400/25000, Avg Reward: 3.1420, Epsilon: 0.432, Achieved: 0.97, Avg Steps: 449.58\n",
      "194\n",
      "Episode 8500/25000, Avg Reward: 7.7250, Epsilon: 0.427, Achieved: 0.98, Avg Steps: 409.75\n",
      "194\n",
      "Episode 8600/25000, Avg Reward: 15.8820, Epsilon: 0.423, Achieved: 0.99, Avg Steps: 334.18\n",
      "194\n",
      "Episode 8700/25000, Avg Reward: -9.0250, Epsilon: 0.419, Achieved: 0.95, Avg Steps: 559.25\n",
      "194\n",
      "Episode 8800/25000, Avg Reward: 7.3030, Epsilon: 0.415, Achieved: 0.98, Avg Steps: 413.97\n",
      "194\n",
      "Episode 8900/25000, Avg Reward: 28.1030, Epsilon: 0.411, Achieved: 1.00, Avg Steps: 217.97\n",
      "194\n",
      "Episode 9000/25000, Avg Reward: 23.8060, Epsilon: 0.407, Achieved: 0.99, Avg Steps: 254.94\n",
      "194\n",
      "Episode 9100/25000, Avg Reward: 3.7840, Epsilon: 0.403, Achieved: 0.96, Avg Steps: 437.16\n",
      "194\n",
      "Episode 9200/25000, Avg Reward: 11.5070, Epsilon: 0.399, Achieved: 0.99, Avg Steps: 377.93\n",
      "194\n",
      "Episode 9300/25000, Avg Reward: 1.6570, Epsilon: 0.395, Achieved: 0.98, Avg Steps: 470.43\n",
      "194\n",
      "Episode 9400/25000, Avg Reward: 24.7400, Epsilon: 0.391, Achieved: 0.99, Avg Steps: 245.60\n",
      "194\n",
      "Episode 9500/25000, Avg Reward: 15.0990, Epsilon: 0.387, Achieved: 0.98, Avg Steps: 336.01\n",
      "194\n",
      "Episode 9600/25000, Avg Reward: 15.2390, Epsilon: 0.383, Achieved: 0.99, Avg Steps: 340.61\n",
      "194\n",
      "Episode 9700/25000, Avg Reward: 29.5100, Epsilon: 0.379, Achieved: 0.99, Avg Steps: 197.90\n",
      "194\n",
      "Episode 9800/25000, Avg Reward: 12.9230, Epsilon: 0.375, Achieved: 0.96, Avg Steps: 345.77\n",
      "194\n",
      "Episode 9900/25000, Avg Reward: 22.5080, Epsilon: 0.372, Achieved: 0.98, Avg Steps: 261.92\n",
      "194\n",
      "Episode 10000/25000, Avg Reward: 11.4040, Epsilon: 0.368, Achieved: 0.98, Avg Steps: 372.96\n",
      "194\n",
      "Episode 10100/25000, Avg Reward: 34.1530, Epsilon: 0.364, Achieved: 1.00, Avg Steps: 157.47\n",
      "194\n",
      "Episode 10200/25000, Avg Reward: 6.0080, Epsilon: 0.361, Achieved: 0.97, Avg Steps: 420.92\n",
      "194\n",
      "Episode 10300/25000, Avg Reward: 10.6730, Epsilon: 0.357, Achieved: 0.97, Avg Steps: 374.27\n",
      "194\n",
      "Episode 10400/25000, Avg Reward: 13.8300, Epsilon: 0.353, Achieved: 0.98, Avg Steps: 348.70\n",
      "194\n",
      "Episode 10500/25000, Avg Reward: 6.1040, Epsilon: 0.350, Achieved: 0.97, Avg Steps: 419.96\n",
      "194\n",
      "Episode 10600/25000, Avg Reward: 18.5790, Epsilon: 0.346, Achieved: 0.98, Avg Steps: 301.21\n",
      "194\n",
      "Episode 10700/25000, Avg Reward: 27.3510, Epsilon: 0.343, Achieved: 0.99, Avg Steps: 219.49\n",
      "194\n",
      "Episode 10800/25000, Avg Reward: 5.9630, Epsilon: 0.340, Achieved: 0.99, Avg Steps: 433.37\n",
      "194\n",
      "Episode 10900/25000, Avg Reward: 12.3790, Epsilon: 0.336, Achieved: 0.99, Avg Steps: 369.21\n",
      "194\n",
      "Episode 11000/25000, Avg Reward: 23.1080, Epsilon: 0.333, Achieved: 1.00, Avg Steps: 267.92\n",
      "194\n",
      "Episode 11100/25000, Avg Reward: 11.1280, Epsilon: 0.330, Achieved: 0.98, Avg Steps: 375.72\n",
      "194\n",
      "Episode 11200/25000, Avg Reward: 21.9770, Epsilon: 0.326, Achieved: 0.97, Avg Steps: 261.23\n",
      "194\n",
      "Episode 11300/25000, Avg Reward: 4.0300, Epsilon: 0.323, Achieved: 0.95, Avg Steps: 428.70\n",
      "194\n",
      "Episode 11400/25000, Avg Reward: 28.6600, Epsilon: 0.320, Achieved: 0.99, Avg Steps: 206.40\n",
      "194\n",
      "Episode 11500/25000, Avg Reward: 20.5710, Epsilon: 0.317, Achieved: 0.98, Avg Steps: 281.29\n",
      "194\n",
      "Episode 11600/25000, Avg Reward: 24.0130, Epsilon: 0.313, Achieved: 0.97, Avg Steps: 240.87\n",
      "194\n",
      "Episode 11700/25000, Avg Reward: 31.9960, Epsilon: 0.310, Achieved: 0.98, Avg Steps: 167.04\n",
      "194\n",
      "Episode 11800/25000, Avg Reward: 20.2420, Epsilon: 0.307, Achieved: 0.99, Avg Steps: 290.58\n",
      "194\n",
      "Episode 11900/25000, Avg Reward: 34.4660, Epsilon: 0.304, Achieved: 1.00, Avg Steps: 154.34\n",
      "194\n",
      "Episode 12000/25000, Avg Reward: -6.5390, Epsilon: 0.301, Achieved: 0.97, Avg Steps: 546.39\n",
      "194\n",
      "Episode 12100/25000, Avg Reward: 19.4670, Epsilon: 0.298, Achieved: 0.99, Avg Steps: 298.33\n",
      "194\n",
      "Episode 12200/25000, Avg Reward: 2.9560, Epsilon: 0.295, Achieved: 0.96, Avg Steps: 445.44\n",
      "194\n",
      "Episode 12300/25000, Avg Reward: 8.9670, Epsilon: 0.292, Achieved: 0.96, Avg Steps: 385.33\n",
      "194\n",
      "Episode 12400/25000, Avg Reward: 7.1090, Epsilon: 0.289, Achieved: 0.96, Avg Steps: 403.91\n",
      "194\n",
      "Episode 12500/25000, Avg Reward: 40.2090, Epsilon: 0.286, Achieved: 1.00, Avg Steps: 96.91\n",
      "194\n",
      "Episode 12600/25000, Avg Reward: 21.7290, Epsilon: 0.284, Achieved: 0.98, Avg Steps: 269.71\n",
      "194\n",
      "Episode 12700/25000, Avg Reward: 19.8810, Epsilon: 0.281, Achieved: 0.98, Avg Steps: 288.19\n",
      "194\n",
      "Episode 12800/25000, Avg Reward: 25.7180, Epsilon: 0.278, Achieved: 1.00, Avg Steps: 241.82\n",
      "194\n",
      "Episode 12900/25000, Avg Reward: 31.6890, Epsilon: 0.275, Achieved: 0.98, Avg Steps: 170.11\n",
      "194\n",
      "Episode 13000/25000, Avg Reward: 20.8380, Epsilon: 0.273, Achieved: 0.97, Avg Steps: 272.62\n",
      "194\n",
      "Episode 13100/25000, Avg Reward: 26.4830, Epsilon: 0.270, Achieved: 0.98, Avg Steps: 222.17\n",
      "194\n",
      "Episode 13200/25000, Avg Reward: 16.3950, Epsilon: 0.267, Achieved: 0.98, Avg Steps: 323.05\n",
      "194\n",
      "Episode 13300/25000, Avg Reward: -0.9090, Epsilon: 0.264, Achieved: 0.96, Avg Steps: 484.09\n",
      "194\n",
      "Episode 13400/25000, Avg Reward: 7.2690, Epsilon: 0.262, Achieved: 0.96, Avg Steps: 402.31\n",
      "194\n",
      "Episode 13500/25000, Avg Reward: 13.1920, Epsilon: 0.259, Achieved: 0.98, Avg Steps: 355.08\n",
      "194\n",
      "Episode 13600/25000, Avg Reward: 17.3870, Epsilon: 0.257, Achieved: 0.97, Avg Steps: 307.13\n",
      "194\n",
      "Episode 13700/25000, Avg Reward: 12.6290, Epsilon: 0.254, Achieved: 0.98, Avg Steps: 360.71\n",
      "194\n",
      "Episode 13800/25000, Avg Reward: 20.6320, Epsilon: 0.252, Achieved: 0.98, Avg Steps: 280.68\n",
      "194\n",
      "Episode 13900/25000, Avg Reward: 24.5610, Epsilon: 0.249, Achieved: 0.98, Avg Steps: 241.39\n",
      "194\n",
      "Episode 14000/25000, Avg Reward: 14.3160, Epsilon: 0.247, Achieved: 0.97, Avg Steps: 337.84\n",
      "194\n",
      "Episode 14100/25000, Avg Reward: 0.5060, Epsilon: 0.244, Achieved: 0.96, Avg Steps: 469.94\n",
      "194\n",
      "Episode 14200/25000, Avg Reward: 22.1030, Epsilon: 0.242, Achieved: 0.99, Avg Steps: 271.97\n",
      "194\n",
      "Episode 14300/25000, Avg Reward: 39.3300, Epsilon: 0.239, Achieved: 1.00, Avg Steps: 105.70\n",
      "194\n",
      "Episode 14400/25000, Avg Reward: 24.8850, Epsilon: 0.237, Achieved: 0.98, Avg Steps: 238.15\n",
      "194\n",
      "Episode 14500/25000, Avg Reward: 12.9120, Epsilon: 0.235, Achieved: 0.96, Avg Steps: 345.88\n",
      "194\n",
      "Episode 14600/25000, Avg Reward: 28.6110, Epsilon: 0.232, Achieved: 1.00, Avg Steps: 212.89\n",
      "194\n",
      "Episode 14700/25000, Avg Reward: 44.6570, Epsilon: 0.230, Achieved: 1.00, Avg Steps: 52.43\n",
      "194\n",
      "Episode 14800/25000, Avg Reward: 36.2720, Epsilon: 0.228, Achieved: 1.00, Avg Steps: 136.28\n",
      "194\n",
      "Episode 14900/25000, Avg Reward: 8.2530, Epsilon: 0.225, Achieved: 0.96, Avg Steps: 392.47\n",
      "194\n",
      "Episode 15000/25000, Avg Reward: 9.5140, Epsilon: 0.223, Achieved: 0.97, Avg Steps: 385.86\n",
      "194\n",
      "Episode 15100/25000, Avg Reward: 34.8050, Epsilon: 0.221, Achieved: 1.00, Avg Steps: 150.95\n",
      "194\n",
      "Episode 15200/25000, Avg Reward: 33.8590, Epsilon: 0.219, Achieved: 1.00, Avg Steps: 160.41\n",
      "194\n",
      "Episode 15300/25000, Avg Reward: 26.1680, Epsilon: 0.217, Achieved: 0.99, Avg Steps: 231.32\n",
      "194\n",
      "Episode 15400/25000, Avg Reward: 42.3830, Epsilon: 0.214, Achieved: 1.00, Avg Steps: 75.17\n",
      "194\n",
      "Episode 15500/25000, Avg Reward: 15.7480, Epsilon: 0.212, Achieved: 0.98, Avg Steps: 329.52\n",
      "194\n",
      "Episode 15600/25000, Avg Reward: 22.9360, Epsilon: 0.210, Achieved: 0.97, Avg Steps: 251.64\n",
      "194\n",
      "Episode 15700/25000, Avg Reward: 33.3540, Epsilon: 0.208, Achieved: 0.99, Avg Steps: 159.46\n",
      "194\n",
      "Episode 15800/25000, Avg Reward: 14.2340, Epsilon: 0.206, Achieved: 0.96, Avg Steps: 332.66\n",
      "194\n",
      "Episode 15900/25000, Avg Reward: 10.4940, Epsilon: 0.204, Achieved: 0.97, Avg Steps: 376.06\n",
      "194\n",
      "Episode 16000/25000, Avg Reward: 35.1870, Epsilon: 0.202, Achieved: 0.99, Avg Steps: 141.13\n",
      "194\n",
      "Episode 16100/25000, Avg Reward: 5.4170, Epsilon: 0.200, Achieved: 0.95, Avg Steps: 414.83\n",
      "194\n",
      "Episode 16200/25000, Avg Reward: 10.5570, Epsilon: 0.198, Achieved: 0.97, Avg Steps: 375.43\n",
      "194\n",
      "Episode 16300/25000, Avg Reward: 31.0370, Epsilon: 0.196, Achieved: 0.99, Avg Steps: 182.63\n",
      "194\n",
      "Episode 16400/25000, Avg Reward: 3.4200, Epsilon: 0.194, Achieved: 0.94, Avg Steps: 428.80\n",
      "194\n",
      "Episode 16500/25000, Avg Reward: -1.0960, Epsilon: 0.192, Achieved: 0.95, Avg Steps: 479.96\n",
      "194\n",
      "Episode 16600/25000, Avg Reward: 20.0310, Epsilon: 0.190, Achieved: 0.98, Avg Steps: 286.69\n",
      "194\n",
      "Episode 16700/25000, Avg Reward: 11.9170, Epsilon: 0.188, Achieved: 0.97, Avg Steps: 361.83\n",
      "194\n",
      "Episode 16800/25000, Avg Reward: 27.9070, Epsilon: 0.186, Achieved: 0.99, Avg Steps: 213.93\n",
      "194\n",
      "Episode 16900/25000, Avg Reward: 26.3400, Epsilon: 0.185, Achieved: 0.99, Avg Steps: 229.60\n",
      "194\n",
      "Episode 17000/25000, Avg Reward: 24.2840, Epsilon: 0.183, Achieved: 0.98, Avg Steps: 244.16\n",
      "194\n",
      "Episode 17100/25000, Avg Reward: 6.1590, Epsilon: 0.181, Achieved: 0.97, Avg Steps: 419.41\n",
      "194\n",
      "Episode 17200/25000, Avg Reward: 26.8200, Epsilon: 0.179, Achieved: 0.99, Avg Steps: 224.80\n",
      "194\n",
      "Episode 17300/25000, Avg Reward: 7.7840, Epsilon: 0.177, Achieved: 0.94, Avg Steps: 385.16\n",
      "194\n",
      "Episode 17400/25000, Avg Reward: 22.0580, Epsilon: 0.176, Achieved: 0.98, Avg Steps: 266.42\n",
      "194\n",
      "Episode 17500/25000, Avg Reward: 15.7890, Epsilon: 0.174, Achieved: 0.97, Avg Steps: 323.11\n",
      "194\n",
      "Episode 17600/25000, Avg Reward: 24.9470, Epsilon: 0.172, Achieved: 0.98, Avg Steps: 237.53\n",
      "194\n",
      "Episode 17700/25000, Avg Reward: 17.7630, Epsilon: 0.170, Achieved: 0.97, Avg Steps: 303.37\n",
      "194\n",
      "Episode 17800/25000, Avg Reward: 35.2720, Epsilon: 0.169, Achieved: 1.00, Avg Steps: 146.28\n",
      "194\n",
      "Episode 17900/25000, Avg Reward: -13.3140, Epsilon: 0.167, Achieved: 0.92, Avg Steps: 584.14\n",
      "194\n",
      "Episode 18000/25000, Avg Reward: 40.0340, Epsilon: 0.165, Achieved: 1.00, Avg Steps: 98.66\n",
      "194\n",
      "Episode 18100/25000, Avg Reward: 24.1610, Epsilon: 0.164, Achieved: 0.98, Avg Steps: 245.39\n",
      "194\n",
      "Episode 18200/25000, Avg Reward: 32.6700, Epsilon: 0.162, Achieved: 0.99, Avg Steps: 166.30\n",
      "194\n",
      "Episode 18300/25000, Avg Reward: 37.4060, Epsilon: 0.160, Achieved: 0.99, Avg Steps: 118.94\n",
      "194\n",
      "Episode 18400/25000, Avg Reward: 16.0790, Epsilon: 0.159, Achieved: 0.98, Avg Steps: 326.21\n",
      "194\n",
      "Episode 18500/25000, Avg Reward: 44.4700, Epsilon: 0.157, Achieved: 1.00, Avg Steps: 54.30\n",
      "194\n",
      "Episode 18600/25000, Avg Reward: 23.1940, Epsilon: 0.156, Achieved: 0.97, Avg Steps: 249.06\n",
      "194\n",
      "Episode 18700/25000, Avg Reward: -11.3010, Epsilon: 0.154, Achieved: 0.92, Avg Steps: 564.01\n",
      "194\n",
      "Episode 18800/25000, Avg Reward: 16.2690, Epsilon: 0.153, Achieved: 0.98, Avg Steps: 324.31\n",
      "194\n",
      "Episode 18900/25000, Avg Reward: 31.6590, Epsilon: 0.151, Achieved: 0.99, Avg Steps: 176.41\n",
      "194\n",
      "Episode 19000/25000, Avg Reward: 22.3360, Epsilon: 0.150, Achieved: 0.99, Avg Steps: 269.64\n",
      "194\n",
      "Episode 19100/25000, Avg Reward: 36.3750, Epsilon: 0.148, Achieved: 0.99, Avg Steps: 129.25\n",
      "194\n",
      "Episode 19200/25000, Avg Reward: 43.7490, Epsilon: 0.147, Achieved: 1.00, Avg Steps: 61.51\n",
      "194\n",
      "Episode 19300/25000, Avg Reward: 30.9230, Epsilon: 0.145, Achieved: 0.99, Avg Steps: 183.77\n",
      "194\n",
      "Episode 19400/25000, Avg Reward: 29.9440, Epsilon: 0.144, Achieved: 0.98, Avg Steps: 187.56\n",
      "194\n",
      "Episode 19500/25000, Avg Reward: 32.2270, Epsilon: 0.142, Achieved: 0.98, Avg Steps: 164.73\n",
      "194\n",
      "Episode 19600/25000, Avg Reward: 44.9970, Epsilon: 0.141, Achieved: 1.00, Avg Steps: 49.03\n",
      "194\n",
      "Episode 19700/25000, Avg Reward: 13.5440, Epsilon: 0.139, Achieved: 0.97, Avg Steps: 345.56\n",
      "194\n",
      "Episode 19800/25000, Avg Reward: 22.2190, Epsilon: 0.138, Achieved: 0.99, Avg Steps: 270.81\n",
      "194\n",
      "Episode 19900/25000, Avg Reward: 29.8670, Epsilon: 0.137, Achieved: 0.98, Avg Steps: 188.33\n",
      "194\n",
      "Episode 20000/25000, Avg Reward: 39.9200, Epsilon: 0.135, Achieved: 1.00, Avg Steps: 99.80\n",
      "194\n",
      "Episode 20100/25000, Avg Reward: 28.5230, Epsilon: 0.134, Achieved: 0.99, Avg Steps: 207.77\n",
      "194\n",
      "Episode 20200/25000, Avg Reward: 26.9870, Epsilon: 0.133, Achieved: 0.98, Avg Steps: 217.13\n",
      "194\n",
      "Episode 20300/25000, Avg Reward: 42.1940, Epsilon: 0.131, Achieved: 1.00, Avg Steps: 77.06\n",
      "194\n",
      "Episode 20400/25000, Avg Reward: 28.0340, Epsilon: 0.130, Achieved: 0.98, Avg Steps: 206.66\n",
      "194\n",
      "Episode 20500/25000, Avg Reward: 13.3070, Epsilon: 0.129, Achieved: 0.97, Avg Steps: 347.93\n",
      "194\n",
      "Episode 20600/25000, Avg Reward: 42.5780, Epsilon: 0.127, Achieved: 1.00, Avg Steps: 73.22\n",
      "194\n",
      "Episode 20700/25000, Avg Reward: 23.2240, Epsilon: 0.126, Achieved: 0.97, Avg Steps: 248.76\n",
      "194\n",
      "Episode 20800/25000, Avg Reward: 26.3450, Epsilon: 0.125, Achieved: 0.98, Avg Steps: 223.55\n",
      "194\n",
      "Episode 20900/25000, Avg Reward: 37.6290, Epsilon: 0.124, Achieved: 0.99, Avg Steps: 116.71\n",
      "194\n",
      "Episode 21000/25000, Avg Reward: 42.8280, Epsilon: 0.122, Achieved: 1.00, Avg Steps: 70.72\n",
      "194\n",
      "Episode 21100/25000, Avg Reward: 42.6840, Epsilon: 0.121, Achieved: 1.00, Avg Steps: 72.16\n",
      "194\n",
      "Episode 21200/25000, Avg Reward: 43.6630, Epsilon: 0.120, Achieved: 1.00, Avg Steps: 62.37\n",
      "194\n",
      "Episode 21300/25000, Avg Reward: 42.5250, Epsilon: 0.119, Achieved: 1.00, Avg Steps: 73.75\n",
      "194\n",
      "Episode 21400/25000, Avg Reward: 43.5340, Epsilon: 0.118, Achieved: 1.00, Avg Steps: 63.66\n",
      "194\n",
      "Episode 21500/25000, Avg Reward: 44.5810, Epsilon: 0.116, Achieved: 1.00, Avg Steps: 53.19\n",
      "194\n",
      "Episode 21600/25000, Avg Reward: 43.6340, Epsilon: 0.115, Achieved: 1.00, Avg Steps: 62.66\n",
      "194\n",
      "Episode 21700/25000, Avg Reward: 43.2500, Epsilon: 0.114, Achieved: 1.00, Avg Steps: 66.50\n",
      "194\n",
      "Episode 21800/25000, Avg Reward: 43.1590, Epsilon: 0.113, Achieved: 1.00, Avg Steps: 67.41\n",
      "194\n",
      "Episode 21900/25000, Avg Reward: 44.1910, Epsilon: 0.112, Achieved: 1.00, Avg Steps: 57.09\n",
      "194\n",
      "Episode 22000/25000, Avg Reward: 42.0630, Epsilon: 0.111, Achieved: 1.00, Avg Steps: 78.37\n",
      "194\n",
      "Episode 22100/25000, Avg Reward: 43.4810, Epsilon: 0.110, Achieved: 1.00, Avg Steps: 64.19\n",
      "194\n",
      "Episode 22200/25000, Avg Reward: 43.4870, Epsilon: 0.109, Achieved: 1.00, Avg Steps: 64.13\n",
      "194\n",
      "Episode 22300/25000, Avg Reward: 8.8940, Epsilon: 0.108, Achieved: 0.94, Avg Steps: 374.06\n",
      "194\n",
      "Episode 22400/25000, Avg Reward: 42.8350, Epsilon: 0.106, Achieved: 1.00, Avg Steps: 70.65\n",
      "194\n",
      "Episode 22500/25000, Avg Reward: 40.0960, Epsilon: 0.105, Achieved: 1.00, Avg Steps: 98.04\n",
      "194\n",
      "Episode 22600/25000, Avg Reward: 45.4500, Epsilon: 0.104, Achieved: 1.00, Avg Steps: 44.50\n",
      "194\n",
      "Episode 22700/25000, Avg Reward: 44.5960, Epsilon: 0.103, Achieved: 1.00, Avg Steps: 53.04\n",
      "194\n",
      "Episode 22800/25000, Avg Reward: 36.3760, Epsilon: 0.102, Achieved: 0.99, Avg Steps: 129.24\n",
      "194\n",
      "Episode 22900/25000, Avg Reward: 36.6100, Epsilon: 0.101, Achieved: 0.99, Avg Steps: 126.90\n",
      "194\n",
      "Episode 23000/25000, Avg Reward: 39.1060, Epsilon: 0.100, Achieved: 1.00, Avg Steps: 107.94\n",
      "194\n",
      "Episode 23100/25000, Avg Reward: 44.2820, Epsilon: 0.099, Achieved: 1.00, Avg Steps: 56.18\n",
      "194\n",
      "Episode 23200/25000, Avg Reward: 43.7140, Epsilon: 0.098, Achieved: 1.00, Avg Steps: 61.86\n",
      "194\n",
      "Episode 23300/25000, Avg Reward: 43.4540, Epsilon: 0.097, Achieved: 1.00, Avg Steps: 64.46\n",
      "194\n",
      "Episode 23400/25000, Avg Reward: 35.3810, Epsilon: 0.096, Achieved: 0.99, Avg Steps: 139.19\n",
      "194\n",
      "Episode 23500/25000, Avg Reward: 32.2710, Epsilon: 0.095, Achieved: 0.99, Avg Steps: 170.29\n",
      "194\n",
      "Episode 23600/25000, Avg Reward: 35.9610, Epsilon: 0.094, Achieved: 0.99, Avg Steps: 133.39\n",
      "194\n",
      "Episode 23700/25000, Avg Reward: 14.7970, Epsilon: 0.093, Achieved: 0.98, Avg Steps: 339.03\n",
      "194\n",
      "Episode 23800/25000, Avg Reward: 30.9330, Epsilon: 0.093, Achieved: 0.98, Avg Steps: 177.67\n",
      "194\n",
      "Episode 23900/25000, Avg Reward: 28.8710, Epsilon: 0.092, Achieved: 0.98, Avg Steps: 198.29\n",
      "194\n",
      "Episode 24000/25000, Avg Reward: 37.2980, Epsilon: 0.091, Achieved: 0.99, Avg Steps: 120.02\n",
      "194\n",
      "Episode 24100/25000, Avg Reward: 30.4900, Epsilon: 0.090, Achieved: 0.99, Avg Steps: 188.10\n",
      "194\n",
      "Episode 24200/25000, Avg Reward: 23.4030, Epsilon: 0.089, Achieved: 0.98, Avg Steps: 252.97\n",
      "194\n",
      "Episode 24300/25000, Avg Reward: 17.8950, Epsilon: 0.088, Achieved: 0.99, Avg Steps: 314.05\n",
      "194\n",
      "Episode 24400/25000, Avg Reward: 33.7590, Epsilon: 0.087, Achieved: 0.99, Avg Steps: 155.41\n",
      "194\n",
      "Episode 24500/25000, Avg Reward: 20.0960, Epsilon: 0.086, Achieved: 0.96, Avg Steps: 274.04\n",
      "194\n",
      "Episode 24600/25000, Avg Reward: 31.9640, Epsilon: 0.085, Achieved: 1.00, Avg Steps: 179.36\n",
      "194\n",
      "Episode 24700/25000, Avg Reward: 42.3980, Epsilon: 0.085, Achieved: 1.00, Avg Steps: 75.02\n",
      "194\n",
      "Episode 24800/25000, Avg Reward: 16.5790, Epsilon: 0.084, Achieved: 0.97, Avg Steps: 315.21\n",
      "194\n",
      "Episode 24900/25000, Avg Reward: 43.4660, Epsilon: 0.083, Achieved: 1.00, Avg Steps: 64.34\n",
      "194\n",
      "Episode 25000/25000, Avg Reward: 24.1030, Epsilon: 0.082, Achieved: 0.97, Avg Steps: 239.97\n",
      "194\n",
      "Q-table 已儲存至 q_table_1.pkl\n"
     ]
    }
   ],
   "source": [
    "q_table = {}\n",
    "epsilon = hyperparameters[\"epsilon_start\"]\n",
    "\n",
    "def random_action(passenger_look, destination_look, has_picked_up, obs):\n",
    "\taction_probs = np.ones(action_nums) / action_nums\n",
    "\tif obstacle_south:\n",
    "\t\taction_probs[0] = 0\n",
    "\tif obstacle_north:\n",
    "\t\taction_probs[1] = 0\n",
    "\tif obstacle_east:\n",
    "\t\taction_probs[2] = 0\n",
    "\tif obstacle_west:\n",
    "\t\taction_probs[3] = 0\t\n",
    "\tif not passenger_look or has_picked_up or not is_in_station(obs):\n",
    "\t\taction_probs[4] = 0\n",
    "\tif not destination_look or not has_picked_up or not is_in_station(obs):\n",
    "\t\taction_probs[5] = 0\n",
    "\taction_probs = action_probs / np.sum(action_probs)\n",
    "\taction = np.random.choice(action_nums, p=action_probs)  # Random action\n",
    "\treturn action\n",
    "\n",
    "def get_state(obs, target_loc=None, has_picked_up=False):\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col, stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\t\n",
    "\n",
    "\tassert target_loc is not None\n",
    "\tx_dir = target_loc[0] - taxi_row\n",
    "\ty_dir = target_loc[1] - taxi_col\n",
    "\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\treturn (x_dir, y_dir, obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look, has_picked_up)\n",
    "\t# return (obstacle_north, obstacle_south, obstacle_east, obstacle_west)\n",
    "  \n",
    "def get_action(obs):\n",
    "\t\"\"\"\n",
    "\t# Selects the best action using the trained Q-table.\n",
    "\t\"\"\"\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = np.random.choice(action_nums)  # Random action\n",
    "\telse:\n",
    "\t\taction = np.argmax(q_table[get_state(obs)])  # Greedy action\n",
    "\treturn action\n",
    "\n",
    "def is_in_station(obs):\n",
    "\t\"\"\"\n",
    "\t# Checks if the taxi is in a station.\n",
    "\t\"\"\"\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col,stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\n",
    "\treturn (taxi_row, taxi_col) in stations\n",
    "\n",
    "env = SimpleTaxiEnv(**env_config)\n",
    "action_nums = 6\n",
    "rewards_per_episode = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "step_count = 0\n",
    "stations = [(0, 0), (0, 4), (4, 0), (4,4)]\n",
    "\n",
    "taxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\n",
    "if render:\n",
    "\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\taction=None, step=step_count, fuel=env.current_fuel)\n",
    "\ttime.sleep(0.5)\n",
    " \n",
    "\n",
    "achieved = []\n",
    "step_counts = []\n",
    "for episode in range(hyperparameters[\"episodes\"]):\n",
    "\tget_state.passenger_loc, get_state.destination_loc = None, None\n",
    "\tenv = SimpleTaxiEnv(grid_size=np.random.choice([5, 6, 7, 8, 9]), **env_config)\n",
    "\tobs, _ = env.reset()\n",
    "\tdone = False\n",
    "\ttotal_reward = 0\n",
    "\tstep_count = 0\n",
    " \n",
    "\tdestination = None\n",
    "\tvisited = []\n",
    "\thas_picked_up = False\n",
    "\tviolations = []\n",
    " \n",
    "\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [(obs[2], obs[3]), (obs[4], obs[5]), (obs[6], obs[7]), (obs[8], obs[9])]\n",
    "\ttarget_loc = random.choice(stations)\n",
    "\tstate = get_state(obs, target_loc, has_picked_up)\n",
    "\t\n",
    "\twhile not done:\t\n",
    "\t\tif state not in q_table:\n",
    "\t\t\tq_table[state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\t\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\t\taction = random_action(passenger_look, destination_look, has_picked_up, obs)\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(q_table[state])  # Greedy action\n",
    "\t\t\n",
    "\t\tshaped_reward = 0\n",
    "   \n",
    "\t\tx_dir = target_loc[0] - taxi_row\n",
    "\t\ty_dir = target_loc[1] - taxi_col\n",
    "\t\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\t\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\t\t# if action == 0 :  # Move Down\n",
    "        #     next_row += 1\n",
    "        # elif action == 1:  # Move Up\n",
    "        #     next_row -= 1\n",
    "        # elif action == 2:  # Move Right\n",
    "        #     next_col += 1\n",
    "        # elif action == 3:  # Move Left\n",
    "        #     next_col -= 1\n",
    "\t\t# if y_dir == 1 and action == 2 and not obstacle_east:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if y_dir == -1 and action == 3 and not obstacle_west:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if x_dir == 1 and action == 0 and not obstacle_south:\n",
    "\t\t# \t# print (target_loc, (taxi_row, taxi_col), (x_dir, y_dir), action)\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\t# if x_dir == -1 and action == 1 and not obstacle_north:\n",
    "\t\t# \tshaped_reward = 10\n",
    "\t\tif action == 4 and passenger_look and not has_picked_up and is_in_station(obs): \n",
    "\t\t\tshaped_reward += 100\n",
    "\t\t\thas_picked_up = True\n",
    "\t\tif action == 5 and destination_look and has_picked_up and is_in_station(obs):\n",
    "\t\t\tshaped_reward += 100\n",
    "\t\t\tdone = True\n",
    "   \n",
    "\t\tif obstacle_south and action == 0:\n",
    "\t\t\tviolations.append(0)\n",
    "\t\t\tshaped_reward += -20\n",
    "\t\t\t# print (\"south\")\n",
    "\t\t\t# print ((taxi_row, taxi_col), (x_dir, y_dir), obstacle_south)\n",
    "\t\tif obstacle_north and action == 1:\n",
    "\t\t\tviolations.append(1)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"north\")\n",
    "\t\tif obstacle_east and action == 2:\n",
    "\t\t\tviolations.append(2)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"east\")\n",
    "\t\tif obstacle_west and action == 3:\n",
    "\t\t\tviolations.append(3)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"west\")\n",
    "\t\tif (not passenger_look or has_picked_up or not is_in_station(obs)) and action == 4:\n",
    "\t\t\tviolations.append(4)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"not passenger\")\n",
    "\t\tif (not destination_look or not has_picked_up or not is_in_station(obs)) and action == 5:\n",
    "\t\t\tviolations.append(5)\n",
    "\t\t\tshaped_reward += -100\n",
    "\t\t\t# print (\"not destination\")\n",
    "\n",
    "\t\tprev_dist = abs(taxi_row - target_loc[0]) + abs(taxi_col - target_loc[1])\n",
    "  \n",
    "\t\tnext_obs, reward, done, _ = env.step(action)\n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = next_obs\n",
    "  \n",
    "\t\t# next_dist = abs(taxi_row - target_loc[0]) + abs(taxi_col - target_loc[1])\n",
    "\t\t# if next_dist < prev_dist:\n",
    "\t\t# \tshaped_reward += 10\n",
    "  \n",
    "\t\tif is_in_station(next_obs):\n",
    "\t\t\tvisited.append((taxi_row, taxi_col))\n",
    "\t\t\tif destination_look:\n",
    "\t\t\t\tdestination = (taxi_row, taxi_col)\n",
    "\t\t\tif has_picked_up and destination is not None:\n",
    "\t\t\t\ttarget_loc = destination\n",
    "\t\t\telse:\n",
    "\t\t\t\t# choose a station that has not been visited yet\n",
    "\t\t\t\t# print (visited, (taxi_row, taxi_col))\n",
    "\t\t\t\tfor station in stations:\n",
    "\t\t\t\t\tif station not in visited:\n",
    "\t\t\t\t\t\ttarget_loc = station\n",
    "\t\t\t\t\t\tbreak\t\n",
    "\t\t\t\t# print (target_loc)\n",
    "\t\t# print (destination, (taxi_row, taxi_col), action, shaped_reward)\n",
    "\t\tif done:\n",
    "\t\t\tif step_count == 4999:\n",
    "\t\t\t\tachieved.append(0)\n",
    "\t\t\telif step_count < 4999:\n",
    "\t\t\t\tachieved.append(1)\n",
    "\t\t\t\tshaped_reward += 200\n",
    "\t\t\tstep_counts.append(step_count)\n",
    "\t\t# if done:\n",
    "\t\t# \tif not has_picked_up:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not picked up\", next_obs, action, has_picked_up, destination)\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and not destination_look:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not destination\")\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and destination_look:\n",
    "\t\t# \t\t# print (\"done\")\n",
    "\t\t# \t\tshaped_reward = 100\n",
    "\t\t# \t\tstep_counts.append(step_count)\n",
    "   \n",
    "\t\ttotal_reward += reward\n",
    "\t\treward += shaped_reward\n",
    "\t\tnext_state = get_state(next_obs, target_loc, has_picked_up)\n",
    "  \n",
    "\t\tif next_state not in q_table:\n",
    "\t\t\tq_table[next_state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\tq_table[state][action] += hyperparameters[\"alpha\"] * (reward + hyperparameters[\"gamma\"] * np.max(q_table[next_state]) - q_table[state][action])\n",
    "\t\t\n",
    "\t\tstep_count += 1\n",
    "\t\tobs = next_obs\n",
    "\t\tstate = next_state\n",
    "\t\t\n",
    "\t\tif render:\n",
    "\t\t\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\t\t\taction=action, step=step_count, fuel=env.current_fuel)\n",
    "\t# print (np.sum(np.array(violations) == 0), np.sum(np.array(violations) == 1), np.sum(np.array(violations) == 2), np.sum(np.array(violations) == 3), np.sum(np.array(violations) == 4), np.sum(np.array(violations) == 5))\n",
    "\trewards_per_episode.append(total_reward)\n",
    "\tepsilon = max(hyperparameters[\"epsilon_end\"], epsilon * hyperparameters[\"decay_rate\"])\n",
    "\tif (episode + 1) % 100 == 0:\n",
    "\t\tavg_reward = np.mean(rewards_per_episode[-100:])\n",
    "\t\tprint(f'Episode {episode + 1}/{hyperparameters[\"episodes\"]}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.3f}, Achieved: {np.mean(achieved[-100:]):.2f}, Avg Steps: {np.mean(step_counts[-100:]):.2f}')\n",
    "\t\t# print ([np.argmax(i) for i in q_table.values()])\n",
    "\t\tprint (len(q_table))\n",
    "  \n",
    "filename = \"q_table_1.pkl\"\n",
    "\n",
    "# 儲存 Q-table\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "print(f\"Q-table 已儲存至 {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "[0, 0]\n",
      "obs= (1, 3, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 0, 0, 0)\n",
      "obs= (1, 2, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 0, 0, 0)\n",
      "obs= (1, 1, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 0, 0, 0)\n",
      "obs= (1, 0, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 1, 0, 0)\n",
      "obs= (0, 0, 0, 0, 0, 6, 6, 0, 6, 6, 1, 0, 0, 1, 0, 0)\n",
      "obs= (0, 1, 0, 0, 0, 6, 6, 0, 6, 6, 1, 0, 0, 0, 0, 0)\n",
      "obs= (0, 2, 0, 0, 0, 6, 6, 0, 6, 6, 1, 0, 0, 0, 0, 0)\n",
      "obs= (0, 3, 0, 0, 0, 6, 6, 0, 6, 6, 1, 0, 0, 0, 0, 0)\n",
      "obs= (0, 4, 0, 0, 0, 6, 6, 0, 6, 6, 1, 0, 0, 0, 0, 0)\n",
      "obs= (0, 5, 0, 0, 0, 6, 6, 0, 6, 6, 1, 0, 0, 0, 1, 0)\n",
      "obs= (0, 6, 0, 0, 0, 6, 6, 0, 6, 6, 1, 0, 1, 0, 1, 0)\n",
      "obs= (0, 6, 0, 0, 0, 6, 6, 0, 6, 6, 1, 0, 1, 0, 1, 0)\n",
      "obs= (1, 6, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 1, 0, 1, 0)\n",
      "obs= (1, 5, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 0, 1, 0)\n",
      "obs= (2, 5, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 0, 1, 0)\n",
      "obs= (3, 5, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 0, 1, 0)\n",
      "obs= (4, 5, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 0, 1, 0)\n",
      "obs= (5, 5, 0, 0, 0, 6, 6, 0, 6, 6, 0, 0, 0, 0, 1, 0)\n",
      "obs= (6, 5, 0, 0, 0, 6, 6, 0, 6, 6, 0, 1, 0, 0, 1, 0)\n",
      "obs= (6, 4, 0, 0, 0, 6, 6, 0, 6, 6, 0, 1, 0, 0, 1, 0)\n",
      "obs= (6, 3, 0, 0, 0, 6, 6, 0, 6, 6, 0, 1, 0, 0, 1, 0)\n",
      "obs= (6, 2, 0, 0, 0, 6, 6, 0, 6, 6, 0, 1, 0, 0, 1, 0)\n",
      "obs= (6, 1, 0, 0, 0, 6, 6, 0, 6, 6, 0, 1, 0, 0, 1, 1)\n",
      "obs= (6, 0, 0, 0, 0, 6, 6, 0, 6, 6, 0, 1, 0, 1, 1, 1)\n",
      "obs= (6, 0, 0, 0, 0, 6, 6, 0, 6, 6, 0, 1, 0, 1, 1, 1)\n",
      "Agent Finished in 25 steps, Score: 47.5\n",
      "Final Score: 47.5\n"
     ]
    }
   ],
   "source": [
    "env_config = {\n",
    "        \"grid_size\": np.random.choice([5, 6, 7, 8, 9]),\n",
    "        \"fuel_limit\": 5000\n",
    "    }\n",
    "    \n",
    "agent_score = run_agent(\"student_agent.py\", env_config, render=False)\n",
    "print(f\"Final Score: {agent_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
