{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\楊中\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
      "  fn()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from simple_custom_taxi_env import SimpleTaxiEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bool8 = np.bool_\n",
    "\n",
    "env_config = {\n",
    "    \"fuel_limit\": 5000\n",
    "}\n",
    "render = False\n",
    "hyperparameters = {\n",
    "    \"alpha\": 0.001,\n",
    "\t\"gamma\": 0.99,\n",
    "\t\"epsilon_start\": 1.0, \n",
    "\t\"epsilon_end\": 0.1,\n",
    "\t\"decay_rate\": 0.999,\n",
    "\t\"episodes\": 10000,\n",
    " \t\"max_steps\": 2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20/10000, Avg Reward: 20.5150, Epsilon: 0.980\n",
      "72\n",
      "Episode 40/10000, Avg Reward: 22.5600, Epsilon: 0.961\n",
      "72\n",
      "Episode 60/10000, Avg Reward: 3.0500, Epsilon: 0.942\n",
      "72\n",
      "Episode 80/10000, Avg Reward: -10.3000, Epsilon: 0.923\n",
      "72\n",
      "Episode 100/10000, Avg Reward: -22.9050, Epsilon: 0.905\n",
      "72\n",
      "Episode 120/10000, Avg Reward: -25.5950, Epsilon: 0.887\n",
      "72\n",
      "Episode 140/10000, Avg Reward: -19.0100, Epsilon: 0.869\n",
      "72\n",
      "Episode 160/10000, Avg Reward: -94.3000, Epsilon: 0.852\n",
      "72\n",
      "Episode 180/10000, Avg Reward: -127.8600, Epsilon: 0.835\n",
      "72\n",
      "Episode 200/10000, Avg Reward: -170.7500, Epsilon: 0.819\n",
      "72\n",
      "Episode 220/10000, Avg Reward: -141.4700, Epsilon: 0.802\n",
      "72\n",
      "Episode 240/10000, Avg Reward: -182.1500, Epsilon: 0.787\n",
      "72\n",
      "Episode 260/10000, Avg Reward: -201.7450, Epsilon: 0.771\n",
      "72\n",
      "Episode 280/10000, Avg Reward: -306.7100, Epsilon: 0.756\n",
      "72\n",
      "Episode 300/10000, Avg Reward: -331.4000, Epsilon: 0.741\n",
      "72\n",
      "Episode 320/10000, Avg Reward: -617.3950, Epsilon: 0.726\n",
      "72\n",
      "Episode 340/10000, Avg Reward: -804.6400, Epsilon: 0.712\n",
      "72\n",
      "Episode 360/10000, Avg Reward: -581.6300, Epsilon: 0.698\n",
      "72\n",
      "Episode 380/10000, Avg Reward: -679.8050, Epsilon: 0.684\n",
      "72\n",
      "Episode 400/10000, Avg Reward: -1048.2050, Epsilon: 0.670\n",
      "72\n",
      "Episode 420/10000, Avg Reward: -1226.9750, Epsilon: 0.657\n",
      "72\n",
      "Episode 440/10000, Avg Reward: -1145.7350, Epsilon: 0.644\n",
      "72\n",
      "Episode 460/10000, Avg Reward: -1559.5750, Epsilon: 0.631\n",
      "72\n",
      "Episode 480/10000, Avg Reward: -1026.9200, Epsilon: 0.619\n",
      "72\n",
      "Episode 500/10000, Avg Reward: -1866.2700, Epsilon: 0.606\n",
      "72\n",
      "Episode 520/10000, Avg Reward: -2119.7250, Epsilon: 0.594\n",
      "72\n",
      "Episode 540/10000, Avg Reward: -1639.2300, Epsilon: 0.583\n",
      "72\n",
      "Episode 560/10000, Avg Reward: -2784.7400, Epsilon: 0.571\n",
      "72\n",
      "Episode 580/10000, Avg Reward: -2644.3300, Epsilon: 0.560\n",
      "72\n",
      "Episode 600/10000, Avg Reward: -3177.0650, Epsilon: 0.549\n",
      "72\n",
      "Episode 620/10000, Avg Reward: -2369.9750, Epsilon: 0.538\n",
      "72\n",
      "Episode 640/10000, Avg Reward: -2510.3700, Epsilon: 0.527\n",
      "72\n",
      "Episode 660/10000, Avg Reward: -3672.4650, Epsilon: 0.517\n",
      "72\n",
      "Episode 680/10000, Avg Reward: -2317.3200, Epsilon: 0.506\n",
      "72\n",
      "Episode 700/10000, Avg Reward: -3023.5100, Epsilon: 0.496\n",
      "72\n",
      "Episode 720/10000, Avg Reward: -2550.5400, Epsilon: 0.487\n",
      "72\n",
      "Episode 740/10000, Avg Reward: -2558.0100, Epsilon: 0.477\n",
      "72\n",
      "Episode 760/10000, Avg Reward: -3773.2250, Epsilon: 0.467\n",
      "72\n",
      "Episode 780/10000, Avg Reward: -3414.0150, Epsilon: 0.458\n",
      "72\n",
      "Episode 800/10000, Avg Reward: -4217.6450, Epsilon: 0.449\n",
      "72\n",
      "Episode 820/10000, Avg Reward: -4207.7900, Epsilon: 0.440\n",
      "72\n",
      "Episode 840/10000, Avg Reward: -3712.2900, Epsilon: 0.432\n",
      "72\n",
      "Episode 860/10000, Avg Reward: -4575.5150, Epsilon: 0.423\n",
      "72\n",
      "Episode 880/10000, Avg Reward: -4116.0800, Epsilon: 0.415\n",
      "72\n",
      "Episode 900/10000, Avg Reward: -4846.2100, Epsilon: 0.406\n",
      "72\n",
      "Episode 920/10000, Avg Reward: -4934.9850, Epsilon: 0.398\n",
      "72\n",
      "Episode 940/10000, Avg Reward: -4853.0350, Epsilon: 0.390\n",
      "72\n",
      "Episode 960/10000, Avg Reward: -4850.2500, Epsilon: 0.383\n",
      "72\n",
      "Episode 980/10000, Avg Reward: -4389.9950, Epsilon: 0.375\n",
      "72\n",
      "Episode 1000/10000, Avg Reward: -4477.9100, Epsilon: 0.368\n",
      "72\n",
      "Episode 1020/10000, Avg Reward: -4398.0700, Epsilon: 0.360\n",
      "72\n",
      "Episode 1040/10000, Avg Reward: -4372.0850, Epsilon: 0.353\n",
      "72\n",
      "Episode 1060/10000, Avg Reward: -4202.0850, Epsilon: 0.346\n",
      "72\n",
      "Episode 1080/10000, Avg Reward: -5338.6250, Epsilon: 0.339\n",
      "72\n",
      "Episode 1100/10000, Avg Reward: -5169.2400, Epsilon: 0.333\n",
      "72\n",
      "Episode 1120/10000, Avg Reward: -5860.9950, Epsilon: 0.326\n",
      "72\n",
      "Episode 1140/10000, Avg Reward: -5898.7850, Epsilon: 0.320\n",
      "72\n",
      "Episode 1160/10000, Avg Reward: -5595.5450, Epsilon: 0.313\n",
      "72\n",
      "Episode 1180/10000, Avg Reward: -5033.4500, Epsilon: 0.307\n",
      "72\n",
      "Episode 1200/10000, Avg Reward: -5157.0400, Epsilon: 0.301\n",
      "72\n",
      "Episode 1220/10000, Avg Reward: -5724.6050, Epsilon: 0.295\n",
      "72\n",
      "Episode 1240/10000, Avg Reward: -5911.0450, Epsilon: 0.289\n",
      "72\n",
      "Episode 1260/10000, Avg Reward: -5527.4450, Epsilon: 0.283\n",
      "72\n",
      "Episode 1280/10000, Avg Reward: -5898.9300, Epsilon: 0.278\n",
      "72\n",
      "Episode 1300/10000, Avg Reward: -5929.2600, Epsilon: 0.272\n",
      "72\n",
      "Episode 1320/10000, Avg Reward: -6641.7550, Epsilon: 0.267\n",
      "72\n",
      "Episode 1340/10000, Avg Reward: -6007.4000, Epsilon: 0.262\n",
      "72\n",
      "Episode 1360/10000, Avg Reward: -5755.2100, Epsilon: 0.256\n",
      "72\n",
      "Episode 1380/10000, Avg Reward: -6418.5900, Epsilon: 0.251\n",
      "72\n",
      "Episode 1400/10000, Avg Reward: -5113.7650, Epsilon: 0.246\n",
      "72\n",
      "Episode 1420/10000, Avg Reward: -6860.4700, Epsilon: 0.242\n",
      "72\n",
      "Episode 1440/10000, Avg Reward: -5540.7400, Epsilon: 0.237\n",
      "72\n",
      "Episode 1460/10000, Avg Reward: -6514.6400, Epsilon: 0.232\n",
      "72\n",
      "Episode 1480/10000, Avg Reward: -6244.7600, Epsilon: 0.227\n",
      "72\n",
      "Episode 1500/10000, Avg Reward: -6293.5550, Epsilon: 0.223\n",
      "72\n",
      "Episode 1520/10000, Avg Reward: -5379.7800, Epsilon: 0.219\n",
      "72\n",
      "Episode 1540/10000, Avg Reward: -6302.5750, Epsilon: 0.214\n",
      "72\n",
      "Episode 1560/10000, Avg Reward: -6014.2400, Epsilon: 0.210\n",
      "72\n",
      "Episode 1580/10000, Avg Reward: -6024.5400, Epsilon: 0.206\n",
      "72\n",
      "Episode 1600/10000, Avg Reward: -5800.3300, Epsilon: 0.202\n",
      "72\n",
      "Episode 1620/10000, Avg Reward: -5833.2150, Epsilon: 0.198\n",
      "72\n",
      "Episode 1640/10000, Avg Reward: -6144.1500, Epsilon: 0.194\n",
      "72\n",
      "Episode 1660/10000, Avg Reward: -6182.5900, Epsilon: 0.190\n",
      "72\n",
      "Episode 1680/10000, Avg Reward: -7277.1300, Epsilon: 0.186\n",
      "72\n",
      "Episode 1700/10000, Avg Reward: -6204.7900, Epsilon: 0.183\n",
      "72\n",
      "Episode 1720/10000, Avg Reward: -6189.9450, Epsilon: 0.179\n",
      "72\n",
      "Episode 1740/10000, Avg Reward: -7326.4350, Epsilon: 0.175\n",
      "72\n",
      "Episode 1760/10000, Avg Reward: -5957.1650, Epsilon: 0.172\n",
      "72\n",
      "Episode 1780/10000, Avg Reward: -7389.9500, Epsilon: 0.168\n",
      "72\n",
      "Episode 1800/10000, Avg Reward: -7099.0450, Epsilon: 0.165\n",
      "72\n",
      "Episode 1820/10000, Avg Reward: -6660.4600, Epsilon: 0.162\n",
      "72\n",
      "Episode 1840/10000, Avg Reward: -5230.0300, Epsilon: 0.159\n",
      "72\n",
      "Episode 1860/10000, Avg Reward: -6128.1950, Epsilon: 0.156\n",
      "72\n",
      "Episode 1880/10000, Avg Reward: -6796.9950, Epsilon: 0.152\n",
      "72\n",
      "Episode 1900/10000, Avg Reward: -6827.0950, Epsilon: 0.149\n",
      "72\n",
      "Episode 1920/10000, Avg Reward: -6455.2800, Epsilon: 0.146\n",
      "72\n",
      "Episode 1940/10000, Avg Reward: -7154.4100, Epsilon: 0.144\n",
      "72\n",
      "Episode 1960/10000, Avg Reward: -6895.6750, Epsilon: 0.141\n",
      "72\n",
      "Episode 1980/10000, Avg Reward: -7318.2150, Epsilon: 0.138\n",
      "72\n",
      "Episode 2000/10000, Avg Reward: -6506.3600, Epsilon: 0.135\n",
      "72\n",
      "Episode 2020/10000, Avg Reward: -7257.2750, Epsilon: 0.133\n",
      "72\n",
      "Episode 2040/10000, Avg Reward: -6886.9100, Epsilon: 0.130\n",
      "72\n",
      "Episode 2060/10000, Avg Reward: -6896.3700, Epsilon: 0.127\n",
      "72\n",
      "Episode 2080/10000, Avg Reward: -6845.7950, Epsilon: 0.125\n",
      "72\n",
      "Episode 2100/10000, Avg Reward: -7374.4150, Epsilon: 0.122\n",
      "72\n",
      "Episode 2120/10000, Avg Reward: -7341.7300, Epsilon: 0.120\n",
      "72\n",
      "Episode 2140/10000, Avg Reward: -7400.4600, Epsilon: 0.118\n",
      "72\n",
      "Episode 2160/10000, Avg Reward: -7341.2050, Epsilon: 0.115\n",
      "72\n",
      "Episode 2180/10000, Avg Reward: -6064.5650, Epsilon: 0.113\n",
      "72\n",
      "Episode 2200/10000, Avg Reward: -7741.9850, Epsilon: 0.111\n",
      "72\n",
      "Episode 2220/10000, Avg Reward: -7160.0650, Epsilon: 0.108\n",
      "72\n",
      "Episode 2240/10000, Avg Reward: -6499.6800, Epsilon: 0.106\n",
      "72\n",
      "Episode 2260/10000, Avg Reward: -7393.3700, Epsilon: 0.104\n",
      "72\n",
      "Episode 2280/10000, Avg Reward: -7404.8950, Epsilon: 0.102\n",
      "72\n",
      "Episode 2300/10000, Avg Reward: -7934.7350, Epsilon: 0.100\n",
      "72\n",
      "Episode 2320/10000, Avg Reward: -7380.8350, Epsilon: 0.100\n",
      "72\n",
      "Episode 2340/10000, Avg Reward: -7008.8700, Epsilon: 0.100\n",
      "72\n",
      "Episode 2360/10000, Avg Reward: -6126.7650, Epsilon: 0.100\n",
      "72\n",
      "Episode 2380/10000, Avg Reward: -7162.5600, Epsilon: 0.100\n",
      "72\n",
      "Episode 2400/10000, Avg Reward: -6783.7600, Epsilon: 0.100\n",
      "72\n",
      "Episode 2420/10000, Avg Reward: -7001.7100, Epsilon: 0.100\n",
      "72\n",
      "Episode 2440/10000, Avg Reward: -7123.9400, Epsilon: 0.100\n",
      "72\n",
      "Episode 2460/10000, Avg Reward: -7158.0150, Epsilon: 0.100\n",
      "72\n",
      "Episode 2480/10000, Avg Reward: -5769.0550, Epsilon: 0.100\n",
      "72\n",
      "Episode 2500/10000, Avg Reward: -6741.8550, Epsilon: 0.100\n",
      "72\n",
      "Episode 2520/10000, Avg Reward: -7606.6900, Epsilon: 0.100\n",
      "72\n",
      "Episode 2540/10000, Avg Reward: -7314.6500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2560/10000, Avg Reward: -6898.4500, Epsilon: 0.100\n",
      "72\n",
      "Episode 2580/10000, Avg Reward: -7468.4300, Epsilon: 0.100\n",
      "72\n",
      "Episode 2600/10000, Avg Reward: -7084.2350, Epsilon: 0.100\n",
      "72\n",
      "Episode 2620/10000, Avg Reward: -7398.5950, Epsilon: 0.100\n",
      "72\n",
      "Episode 2640/10000, Avg Reward: -6079.8600, Epsilon: 0.100\n",
      "72\n",
      "Episode 2660/10000, Avg Reward: -6749.5100, Epsilon: 0.100\n",
      "72\n",
      "Episode 2680/10000, Avg Reward: -8172.5000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2700/10000, Avg Reward: -7222.0400, Epsilon: 0.100\n",
      "72\n",
      "Episode 2720/10000, Avg Reward: -6772.9650, Epsilon: 0.100\n",
      "72\n",
      "Episode 2740/10000, Avg Reward: -7422.2550, Epsilon: 0.100\n",
      "72\n",
      "Episode 2760/10000, Avg Reward: -6964.4750, Epsilon: 0.100\n",
      "72\n",
      "Episode 2780/10000, Avg Reward: -7135.3650, Epsilon: 0.100\n",
      "72\n",
      "Episode 2800/10000, Avg Reward: -6601.0450, Epsilon: 0.100\n",
      "72\n",
      "Episode 2820/10000, Avg Reward: -6347.2000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2840/10000, Avg Reward: -7950.2750, Epsilon: 0.100\n",
      "72\n",
      "Episode 2860/10000, Avg Reward: -8197.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2880/10000, Avg Reward: -7136.8600, Epsilon: 0.100\n",
      "72\n",
      "Episode 2900/10000, Avg Reward: -5966.6800, Epsilon: 0.100\n",
      "72\n",
      "Episode 2920/10000, Avg Reward: -7251.0350, Epsilon: 0.100\n",
      "72\n",
      "Episode 2940/10000, Avg Reward: -8181.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 2960/10000, Avg Reward: -6741.7650, Epsilon: 0.100\n",
      "72\n",
      "Episode 2980/10000, Avg Reward: -6678.5450, Epsilon: 0.100\n",
      "72\n",
      "Episode 3000/10000, Avg Reward: -7097.1300, Epsilon: 0.100\n",
      "72\n",
      "Episode 3020/10000, Avg Reward: -7152.6850, Epsilon: 0.100\n",
      "72\n",
      "Episode 3040/10000, Avg Reward: -7482.1100, Epsilon: 0.100\n",
      "72\n",
      "Episode 3060/10000, Avg Reward: -7128.3700, Epsilon: 0.100\n",
      "72\n",
      "Episode 3080/10000, Avg Reward: -7317.9000, Epsilon: 0.100\n",
      "72\n",
      "Episode 3100/10000, Avg Reward: -6057.4950, Epsilon: 0.100\n",
      "72\n",
      "Episode 3120/10000, Avg Reward: -6727.0350, Epsilon: 0.100\n",
      "72\n",
      "Episode 3140/10000, Avg Reward: -6338.0100, Epsilon: 0.100\n",
      "72\n",
      "Episode 3160/10000, Avg Reward: -7449.2100, Epsilon: 0.100\n",
      "72\n",
      "Episode 3180/10000, Avg Reward: -7501.8400, Epsilon: 0.100\n",
      "72\n",
      "Episode 3200/10000, Avg Reward: -6358.9300, Epsilon: 0.100\n",
      "72\n",
      "Episode 3220/10000, Avg Reward: -6344.0850, Epsilon: 0.100\n",
      "72\n",
      "Episode 3240/10000, Avg Reward: -7455.0350, Epsilon: 0.100\n",
      "72\n",
      "Episode 3260/10000, Avg Reward: -6872.8650, Epsilon: 0.100\n",
      "72\n",
      "Episode 3280/10000, Avg Reward: -7188.6250, Epsilon: 0.100\n",
      "72\n",
      "Episode 3300/10000, Avg Reward: -7235.4100, Epsilon: 0.100\n",
      "72\n",
      "Episode 3320/10000, Avg Reward: -7522.1600, Epsilon: 0.100\n",
      "72\n",
      "Episode 3340/10000, Avg Reward: -6738.6450, Epsilon: 0.100\n",
      "72\n",
      "Episode 3360/10000, Avg Reward: -7254.2200, Epsilon: 0.100\n",
      "72\n",
      "Episode 3380/10000, Avg Reward: -7072.5300, Epsilon: 0.100\n",
      "72\n",
      "Episode 3400/10000, Avg Reward: -6859.7650, Epsilon: 0.100\n",
      "72\n",
      "Episode 3420/10000, Avg Reward: -7202.9000, Epsilon: 0.100\n",
      "72\n",
      "Episode 3440/10000, Avg Reward: -7451.3450, Epsilon: 0.100\n",
      "72\n",
      "Episode 3460/10000, Avg Reward: -7075.3300, Epsilon: 0.100\n",
      "72\n",
      "Episode 3480/10000, Avg Reward: -7533.8250, Epsilon: 0.100\n",
      "72\n",
      "Episode 3500/10000, Avg Reward: -8200.7500, Epsilon: 0.100\n",
      "72\n",
      "Episode 3520/10000, Avg Reward: -7637.5150, Epsilon: 0.100\n",
      "72\n",
      "Episode 3540/10000, Avg Reward: -6553.6000, Epsilon: 0.100\n",
      "72\n",
      "Episode 3560/10000, Avg Reward: -7445.1500, Epsilon: 0.100\n",
      "72\n",
      "Episode 3580/10000, Avg Reward: -7051.4750, Epsilon: 0.100\n",
      "72\n",
      "Episode 3600/10000, Avg Reward: -6485.6050, Epsilon: 0.100\n",
      "72\n",
      "Episode 3620/10000, Avg Reward: -6373.6600, Epsilon: 0.100\n",
      "72\n",
      "Episode 3640/10000, Avg Reward: -6833.0800, Epsilon: 0.100\n",
      "72\n",
      "Episode 3660/10000, Avg Reward: -7901.3600, Epsilon: 0.100\n",
      "72\n",
      "Episode 3680/10000, Avg Reward: -6819.2850, Epsilon: 0.100\n",
      "72\n",
      "Episode 3700/10000, Avg Reward: -6827.5600, Epsilon: 0.100\n",
      "72\n",
      "Episode 3720/10000, Avg Reward: -6840.0700, Epsilon: 0.100\n",
      "72\n",
      "Episode 3740/10000, Avg Reward: -6110.0850, Epsilon: 0.100\n",
      "72\n",
      "Episode 3760/10000, Avg Reward: -6848.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 3780/10000, Avg Reward: -7408.3000, Epsilon: 0.100\n",
      "72\n",
      "Episode 3800/10000, Avg Reward: -7156.0100, Epsilon: 0.100\n",
      "72\n",
      "Episode 3820/10000, Avg Reward: -6848.2550, Epsilon: 0.100\n",
      "72\n",
      "Episode 3840/10000, Avg Reward: -7227.4800, Epsilon: 0.100\n",
      "72\n",
      "Episode 3860/10000, Avg Reward: -7037.8650, Epsilon: 0.100\n",
      "72\n",
      "Episode 3880/10000, Avg Reward: -7483.7950, Epsilon: 0.100\n",
      "72\n",
      "Episode 3900/10000, Avg Reward: -7047.1800, Epsilon: 0.100\n",
      "72\n",
      "Episode 3920/10000, Avg Reward: -7401.9350, Epsilon: 0.100\n",
      "72\n",
      "Episode 3940/10000, Avg Reward: -6389.2300, Epsilon: 0.100\n",
      "72\n",
      "Episode 3960/10000, Avg Reward: -7101.3750, Epsilon: 0.100\n",
      "72\n",
      "Episode 3980/10000, Avg Reward: -6857.0050, Epsilon: 0.100\n",
      "72\n",
      "Episode 4000/10000, Avg Reward: -7807.8300, Epsilon: 0.100\n",
      "72\n",
      "Episode 4020/10000, Avg Reward: -7423.8150, Epsilon: 0.100\n",
      "72\n",
      "Episode 4040/10000, Avg Reward: -6389.5150, Epsilon: 0.100\n",
      "72\n",
      "Episode 4060/10000, Avg Reward: -7627.8450, Epsilon: 0.100\n",
      "72\n",
      "Episode 4080/10000, Avg Reward: -6582.7700, Epsilon: 0.100\n",
      "72\n",
      "Episode 4100/10000, Avg Reward: -7044.3200, Epsilon: 0.100\n",
      "72\n",
      "Episode 4120/10000, Avg Reward: -6783.1650, Epsilon: 0.100\n",
      "72\n",
      "Episode 4140/10000, Avg Reward: -7177.4000, Epsilon: 0.100\n",
      "72\n",
      "Episode 4160/10000, Avg Reward: -7835.1850, Epsilon: 0.100\n",
      "72\n",
      "Episode 4180/10000, Avg Reward: -6398.6850, Epsilon: 0.100\n",
      "72\n",
      "Episode 4200/10000, Avg Reward: -7153.6900, Epsilon: 0.100\n",
      "72\n",
      "Episode 4220/10000, Avg Reward: -6454.4100, Epsilon: 0.100\n",
      "72\n",
      "Episode 4240/10000, Avg Reward: -6619.4900, Epsilon: 0.100\n",
      "72\n",
      "Episode 4260/10000, Avg Reward: -6013.7950, Epsilon: 0.100\n",
      "72\n",
      "Episode 4280/10000, Avg Reward: -7823.1100, Epsilon: 0.100\n",
      "72\n",
      "Episode 4300/10000, Avg Reward: -6269.9700, Epsilon: 0.100\n",
      "72\n",
      "Episode 4320/10000, Avg Reward: -6254.9550, Epsilon: 0.100\n",
      "72\n",
      "Episode 4340/10000, Avg Reward: -7483.9050, Epsilon: 0.100\n",
      "72\n",
      "Episode 4360/10000, Avg Reward: -7250.6650, Epsilon: 0.100\n",
      "72\n",
      "Episode 4380/10000, Avg Reward: -7206.4450, Epsilon: 0.100\n",
      "72\n",
      "Episode 4400/10000, Avg Reward: -6868.1150, Epsilon: 0.100\n",
      "72\n",
      "Episode 4420/10000, Avg Reward: -5921.3400, Epsilon: 0.100\n",
      "72\n",
      "Episode 4440/10000, Avg Reward: -7724.2150, Epsilon: 0.100\n",
      "72\n",
      "Episode 4460/10000, Avg Reward: -7588.6600, Epsilon: 0.100\n",
      "72\n",
      "Episode 4480/10000, Avg Reward: -7828.5150, Epsilon: 0.100\n",
      "72\n",
      "Episode 4500/10000, Avg Reward: -6369.7600, Epsilon: 0.100\n",
      "72\n",
      "Episode 4520/10000, Avg Reward: -7526.7300, Epsilon: 0.100\n",
      "72\n",
      "Episode 4540/10000, Avg Reward: -7496.7700, Epsilon: 0.100\n",
      "72\n",
      "Episode 4560/10000, Avg Reward: -7298.8950, Epsilon: 0.100\n",
      "72\n",
      "Episode 4580/10000, Avg Reward: -7124.4700, Epsilon: 0.100\n",
      "72\n",
      "Episode 4600/10000, Avg Reward: -4853.8300, Epsilon: 0.100\n",
      "72\n",
      "Episode 4620/10000, Avg Reward: -6425.0650, Epsilon: 0.100\n",
      "72\n",
      "Episode 4640/10000, Avg Reward: -6425.8300, Epsilon: 0.100\n",
      "72\n",
      "Episode 4660/10000, Avg Reward: -7833.9000, Epsilon: 0.100\n",
      "72\n",
      "Episode 4680/10000, Avg Reward: -7630.2300, Epsilon: 0.100\n",
      "72\n",
      "Episode 4700/10000, Avg Reward: -7241.1150, Epsilon: 0.100\n",
      "72\n",
      "Episode 4720/10000, Avg Reward: -6913.5800, Epsilon: 0.100\n",
      "72\n",
      "Episode 4740/10000, Avg Reward: -7173.5850, Epsilon: 0.100\n",
      "72\n",
      "Episode 4760/10000, Avg Reward: -7435.1250, Epsilon: 0.100\n",
      "72\n",
      "Episode 4780/10000, Avg Reward: -6127.2700, Epsilon: 0.100\n",
      "72\n",
      "Episode 4800/10000, Avg Reward: -7457.8900, Epsilon: 0.100\n",
      "72\n",
      "Episode 4820/10000, Avg Reward: -7448.1650, Epsilon: 0.100\n",
      "72\n",
      "Episode 4840/10000, Avg Reward: -7598.8600, Epsilon: 0.100\n",
      "72\n",
      "Episode 4860/10000, Avg Reward: -6742.6700, Epsilon: 0.100\n",
      "72\n",
      "Episode 4880/10000, Avg Reward: -7487.2500, Epsilon: 0.100\n",
      "72\n",
      "Episode 4900/10000, Avg Reward: -6747.5400, Epsilon: 0.100\n",
      "72\n",
      "Episode 4920/10000, Avg Reward: -7084.6900, Epsilon: 0.100\n",
      "72\n",
      "Episode 4940/10000, Avg Reward: -6658.1400, Epsilon: 0.100\n",
      "72\n",
      "Episode 4960/10000, Avg Reward: -7156.5400, Epsilon: 0.100\n",
      "72\n",
      "Episode 4980/10000, Avg Reward: -7372.0150, Epsilon: 0.100\n",
      "72\n",
      "Episode 5000/10000, Avg Reward: -6146.7100, Epsilon: 0.100\n",
      "72\n",
      "Episode 5020/10000, Avg Reward: -7580.7900, Epsilon: 0.100\n",
      "72\n",
      "Episode 5040/10000, Avg Reward: -7018.6750, Epsilon: 0.100\n",
      "72\n",
      "Episode 5060/10000, Avg Reward: -7441.5600, Epsilon: 0.100\n",
      "72\n",
      "Episode 5080/10000, Avg Reward: -7103.7950, Epsilon: 0.100\n",
      "72\n",
      "Episode 5100/10000, Avg Reward: -6306.8550, Epsilon: 0.100\n",
      "72\n",
      "Episode 5120/10000, Avg Reward: -6067.1450, Epsilon: 0.100\n",
      "72\n",
      "Episode 5140/10000, Avg Reward: -7089.4300, Epsilon: 0.100\n",
      "72\n",
      "Episode 5160/10000, Avg Reward: -7521.0550, Epsilon: 0.100\n",
      "72\n",
      "Episode 5180/10000, Avg Reward: -6909.3700, Epsilon: 0.100\n",
      "72\n",
      "Episode 5200/10000, Avg Reward: -6849.8300, Epsilon: 0.100\n",
      "72\n",
      "Episode 5220/10000, Avg Reward: -6739.4850, Epsilon: 0.100\n",
      "72\n",
      "Episode 5240/10000, Avg Reward: -5287.3550, Epsilon: 0.100\n",
      "72\n",
      "Episode 5260/10000, Avg Reward: -6831.3800, Epsilon: 0.100\n",
      "72\n",
      "Episode 5280/10000, Avg Reward: -6455.1150, Epsilon: 0.100\n",
      "72\n",
      "Episode 5300/10000, Avg Reward: -6728.0900, Epsilon: 0.100\n",
      "72\n",
      "Episode 5320/10000, Avg Reward: -6721.6900, Epsilon: 0.100\n",
      "72\n",
      "Episode 5340/10000, Avg Reward: -7243.7850, Epsilon: 0.100\n",
      "72\n",
      "Episode 5360/10000, Avg Reward: -7468.6250, Epsilon: 0.100\n",
      "72\n",
      "Episode 5380/10000, Avg Reward: -6759.4550, Epsilon: 0.100\n",
      "72\n",
      "Episode 5400/10000, Avg Reward: -6112.8450, Epsilon: 0.100\n",
      "72\n",
      "Episode 5420/10000, Avg Reward: -6007.3800, Epsilon: 0.100\n",
      "72\n",
      "Episode 5440/10000, Avg Reward: -7680.1650, Epsilon: 0.100\n",
      "72\n",
      "Episode 5460/10000, Avg Reward: -7824.8750, Epsilon: 0.100\n",
      "72\n",
      "Episode 5480/10000, Avg Reward: -7030.9000, Epsilon: 0.100\n",
      "72\n",
      "Episode 5500/10000, Avg Reward: -7872.5000, Epsilon: 0.100\n",
      "72\n",
      "Episode 5520/10000, Avg Reward: -6738.5500, Epsilon: 0.100\n",
      "72\n",
      "Episode 5540/10000, Avg Reward: -6468.0250, Epsilon: 0.100\n",
      "72\n",
      "Episode 5560/10000, Avg Reward: -7652.9300, Epsilon: 0.100\n",
      "72\n",
      "Episode 5580/10000, Avg Reward: -6394.6400, Epsilon: 0.100\n",
      "72\n",
      "Episode 5600/10000, Avg Reward: -6586.3000, Epsilon: 0.100\n",
      "72\n",
      "Episode 5620/10000, Avg Reward: -7155.7650, Epsilon: 0.100\n",
      "72\n",
      "Episode 5640/10000, Avg Reward: -7309.7450, Epsilon: 0.100\n",
      "72\n",
      "Episode 5660/10000, Avg Reward: -7863.2700, Epsilon: 0.100\n",
      "72\n",
      "Episode 5680/10000, Avg Reward: -6473.4400, Epsilon: 0.100\n",
      "72\n",
      "Episode 5700/10000, Avg Reward: -6042.7450, Epsilon: 0.100\n",
      "72\n",
      "Episode 5720/10000, Avg Reward: -5783.0900, Epsilon: 0.100\n",
      "72\n",
      "Episode 5740/10000, Avg Reward: -6917.7350, Epsilon: 0.100\n",
      "72\n",
      "Episode 5760/10000, Avg Reward: -6661.0700, Epsilon: 0.100\n",
      "72\n",
      "Episode 5780/10000, Avg Reward: -6796.0700, Epsilon: 0.100\n",
      "72\n",
      "Episode 5800/10000, Avg Reward: -6832.4550, Epsilon: 0.100\n",
      "72\n",
      "Episode 5820/10000, Avg Reward: -7215.0100, Epsilon: 0.100\n",
      "72\n",
      "Episode 5840/10000, Avg Reward: -6749.6050, Epsilon: 0.100\n",
      "72\n",
      "Episode 5860/10000, Avg Reward: -6282.3900, Epsilon: 0.100\n",
      "72\n",
      "Episode 5880/10000, Avg Reward: -7176.5700, Epsilon: 0.100\n",
      "72\n",
      "Episode 5900/10000, Avg Reward: -7481.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 5920/10000, Avg Reward: -7533.3300, Epsilon: 0.100\n",
      "72\n",
      "Episode 5940/10000, Avg Reward: -7808.7350, Epsilon: 0.100\n",
      "72\n",
      "Episode 5960/10000, Avg Reward: -7159.3850, Epsilon: 0.100\n",
      "72\n",
      "Episode 5980/10000, Avg Reward: -6585.8750, Epsilon: 0.100\n",
      "72\n",
      "Episode 6000/10000, Avg Reward: -7197.5850, Epsilon: 0.100\n",
      "72\n",
      "Episode 6020/10000, Avg Reward: -7805.0800, Epsilon: 0.100\n",
      "72\n",
      "Episode 6040/10000, Avg Reward: -6722.8950, Epsilon: 0.100\n",
      "72\n",
      "Episode 6060/10000, Avg Reward: -7444.3850, Epsilon: 0.100\n",
      "72\n",
      "Episode 6080/10000, Avg Reward: -7587.9350, Epsilon: 0.100\n",
      "72\n",
      "Episode 6100/10000, Avg Reward: -7810.1100, Epsilon: 0.100\n",
      "72\n",
      "Episode 6120/10000, Avg Reward: -7193.3250, Epsilon: 0.100\n",
      "72\n",
      "Episode 6140/10000, Avg Reward: -7023.3200, Epsilon: 0.100\n",
      "72\n",
      "Episode 6160/10000, Avg Reward: -6157.8000, Epsilon: 0.100\n",
      "72\n",
      "Episode 6180/10000, Avg Reward: -6705.6750, Epsilon: 0.100\n",
      "72\n",
      "Episode 6200/10000, Avg Reward: -6829.8350, Epsilon: 0.100\n",
      "72\n",
      "Episode 6220/10000, Avg Reward: -7116.0950, Epsilon: 0.100\n",
      "72\n",
      "Episode 6240/10000, Avg Reward: -6463.1150, Epsilon: 0.100\n",
      "72\n",
      "Episode 6260/10000, Avg Reward: -7446.6150, Epsilon: 0.100\n",
      "72\n",
      "Episode 6280/10000, Avg Reward: -6731.5350, Epsilon: 0.100\n",
      "72\n",
      "Episode 6300/10000, Avg Reward: -7069.3850, Epsilon: 0.100\n",
      "72\n",
      "Episode 6320/10000, Avg Reward: -6788.0500, Epsilon: 0.100\n",
      "72\n",
      "Episode 6340/10000, Avg Reward: -6357.4750, Epsilon: 0.100\n",
      "72\n",
      "Episode 6360/10000, Avg Reward: -6493.6250, Epsilon: 0.100\n",
      "72\n",
      "Episode 6380/10000, Avg Reward: -6864.7850, Epsilon: 0.100\n",
      "72\n",
      "Episode 6400/10000, Avg Reward: -5741.5050, Epsilon: 0.100\n",
      "72\n",
      "Episode 6420/10000, Avg Reward: -6614.2400, Epsilon: 0.100\n",
      "72\n",
      "Episode 6440/10000, Avg Reward: -7149.7600, Epsilon: 0.100\n",
      "72\n",
      "Episode 6460/10000, Avg Reward: -7619.3400, Epsilon: 0.100\n",
      "72\n",
      "Episode 6480/10000, Avg Reward: -7156.9650, Epsilon: 0.100\n",
      "72\n",
      "Episode 6500/10000, Avg Reward: -7835.7150, Epsilon: 0.100\n",
      "72\n",
      "Episode 6520/10000, Avg Reward: -7292.9000, Epsilon: 0.100\n",
      "72\n",
      "Episode 6540/10000, Avg Reward: -7294.3950, Epsilon: 0.100\n",
      "72\n",
      "Episode 6560/10000, Avg Reward: -7913.4150, Epsilon: 0.100\n",
      "72\n",
      "Episode 6580/10000, Avg Reward: -8193.2500, Epsilon: 0.100\n",
      "72\n",
      "Episode 6600/10000, Avg Reward: -6360.7850, Epsilon: 0.100\n",
      "72\n",
      "Episode 6620/10000, Avg Reward: -7182.0600, Epsilon: 0.100\n",
      "72\n",
      "Episode 6640/10000, Avg Reward: -7809.8650, Epsilon: 0.100\n",
      "72\n",
      "Episode 6660/10000, Avg Reward: -6963.4150, Epsilon: 0.100\n",
      "72\n",
      "Episode 6680/10000, Avg Reward: -7507.1800, Epsilon: 0.100\n",
      "72\n",
      "Episode 6700/10000, Avg Reward: -5669.4100, Epsilon: 0.100\n",
      "72\n",
      "Episode 6720/10000, Avg Reward: -7451.3450, Epsilon: 0.100\n",
      "72\n",
      "Episode 6740/10000, Avg Reward: -6017.6650, Epsilon: 0.100\n",
      "72\n",
      "Episode 6760/10000, Avg Reward: -6734.3350, Epsilon: 0.100\n",
      "72\n",
      "Episode 6780/10000, Avg Reward: -5829.8200, Epsilon: 0.100\n",
      "72\n",
      "Episode 6800/10000, Avg Reward: -7128.2750, Epsilon: 0.100\n",
      "72\n",
      "Episode 6820/10000, Avg Reward: -7523.3300, Epsilon: 0.100\n",
      "72\n",
      "Episode 6840/10000, Avg Reward: -7623.9150, Epsilon: 0.100\n",
      "72\n",
      "Episode 6860/10000, Avg Reward: -6398.0800, Epsilon: 0.100\n",
      "72\n",
      "Episode 6880/10000, Avg Reward: -6587.1300, Epsilon: 0.100\n",
      "72\n",
      "Episode 6900/10000, Avg Reward: -7214.0100, Epsilon: 0.100\n",
      "72\n",
      "Episode 6920/10000, Avg Reward: -6840.1850, Epsilon: 0.100\n",
      "72\n",
      "Episode 6940/10000, Avg Reward: -6791.8700, Epsilon: 0.100\n",
      "72\n",
      "Episode 6960/10000, Avg Reward: -7534.4550, Epsilon: 0.100\n",
      "72\n",
      "Episode 6980/10000, Avg Reward: -5886.1950, Epsilon: 0.100\n",
      "72\n",
      "Episode 7000/10000, Avg Reward: -7103.4550, Epsilon: 0.100\n",
      "72\n",
      "Episode 7020/10000, Avg Reward: -7250.3050, Epsilon: 0.100\n",
      "72\n",
      "Episode 7040/10000, Avg Reward: -6741.8100, Epsilon: 0.100\n",
      "72\n",
      "Episode 7060/10000, Avg Reward: -7175.9850, Epsilon: 0.100\n",
      "72\n",
      "Episode 7080/10000, Avg Reward: -6995.2850, Epsilon: 0.100\n",
      "72\n",
      "Episode 7100/10000, Avg Reward: -6852.2650, Epsilon: 0.100\n",
      "72\n",
      "Episode 7120/10000, Avg Reward: -7209.9400, Epsilon: 0.100\n",
      "72\n",
      "Episode 7140/10000, Avg Reward: -6786.3300, Epsilon: 0.100\n",
      "72\n",
      "Episode 7160/10000, Avg Reward: -7648.8900, Epsilon: 0.100\n",
      "72\n",
      "Episode 7180/10000, Avg Reward: -7555.1000, Epsilon: 0.100\n",
      "72\n",
      "Episode 7200/10000, Avg Reward: -7522.3250, Epsilon: 0.100\n",
      "72\n",
      "Episode 7220/10000, Avg Reward: -7328.9600, Epsilon: 0.100\n",
      "72\n",
      "Episode 7240/10000, Avg Reward: -7802.0550, Epsilon: 0.100\n",
      "72\n",
      "Episode 7260/10000, Avg Reward: -7229.3050, Epsilon: 0.100\n",
      "72\n",
      "Episode 7280/10000, Avg Reward: -7314.4600, Epsilon: 0.100\n",
      "72\n",
      "Episode 7300/10000, Avg Reward: -7247.5150, Epsilon: 0.100\n",
      "72\n",
      "Episode 7320/10000, Avg Reward: -6482.9700, Epsilon: 0.100\n",
      "72\n",
      "Episode 7340/10000, Avg Reward: -6842.3200, Epsilon: 0.100\n",
      "72\n",
      "Episode 7360/10000, Avg Reward: -6345.3750, Epsilon: 0.100\n",
      "72\n",
      "Episode 7380/10000, Avg Reward: -7614.7050, Epsilon: 0.100\n",
      "72\n",
      "Episode 7400/10000, Avg Reward: -7389.8650, Epsilon: 0.100\n",
      "72\n",
      "Episode 7420/10000, Avg Reward: -6799.7350, Epsilon: 0.100\n",
      "72\n",
      "Episode 7440/10000, Avg Reward: -7532.0550, Epsilon: 0.100\n",
      "72\n",
      "Episode 7460/10000, Avg Reward: -6314.4500, Epsilon: 0.100\n",
      "72\n",
      "Episode 7480/10000, Avg Reward: -6589.5050, Epsilon: 0.100\n",
      "72\n",
      "Episode 7500/10000, Avg Reward: -6474.0050, Epsilon: 0.100\n",
      "72\n",
      "Episode 7520/10000, Avg Reward: -7502.5350, Epsilon: 0.100\n",
      "72\n",
      "Episode 7540/10000, Avg Reward: -6830.6200, Epsilon: 0.100\n",
      "72\n",
      "Episode 7560/10000, Avg Reward: -7111.1650, Epsilon: 0.100\n",
      "72\n",
      "Episode 7580/10000, Avg Reward: -7537.8200, Epsilon: 0.100\n",
      "72\n",
      "Episode 7600/10000, Avg Reward: -6979.6150, Epsilon: 0.100\n",
      "72\n",
      "Episode 7620/10000, Avg Reward: -6438.7700, Epsilon: 0.100\n",
      "72\n",
      "Episode 7640/10000, Avg Reward: -7167.3100, Epsilon: 0.100\n",
      "72\n",
      "Episode 7660/10000, Avg Reward: -6497.7150, Epsilon: 0.100\n",
      "72\n",
      "Episode 7680/10000, Avg Reward: -6731.1850, Epsilon: 0.100\n",
      "72\n",
      "Episode 7700/10000, Avg Reward: -7417.7250, Epsilon: 0.100\n",
      "72\n",
      "Episode 7720/10000, Avg Reward: -7179.6450, Epsilon: 0.100\n",
      "72\n",
      "Episode 7740/10000, Avg Reward: -6706.9100, Epsilon: 0.100\n",
      "72\n",
      "Episode 7760/10000, Avg Reward: -6282.1750, Epsilon: 0.100\n",
      "72\n",
      "Episode 7780/10000, Avg Reward: -6828.8200, Epsilon: 0.100\n",
      "72\n",
      "Episode 7800/10000, Avg Reward: -7188.4250, Epsilon: 0.100\n",
      "72\n",
      "Episode 7820/10000, Avg Reward: -5483.4300, Epsilon: 0.100\n",
      "72\n",
      "Episode 7840/10000, Avg Reward: -7355.2100, Epsilon: 0.100\n",
      "72\n",
      "Episode 7860/10000, Avg Reward: -7503.3400, Epsilon: 0.100\n",
      "72\n",
      "Episode 7880/10000, Avg Reward: -7574.6000, Epsilon: 0.100\n",
      "72\n",
      "Episode 7900/10000, Avg Reward: -5966.0050, Epsilon: 0.100\n",
      "72\n",
      "Episode 7920/10000, Avg Reward: -7508.2400, Epsilon: 0.100\n",
      "72\n",
      "Episode 7940/10000, Avg Reward: -6123.8550, Epsilon: 0.100\n",
      "72\n",
      "Episode 7960/10000, Avg Reward: -7829.7400, Epsilon: 0.100\n",
      "72\n",
      "Episode 7980/10000, Avg Reward: -5451.3900, Epsilon: 0.100\n",
      "72\n",
      "Episode 8000/10000, Avg Reward: -8030.3600, Epsilon: 0.100\n",
      "72\n",
      "Episode 8020/10000, Avg Reward: -7226.4500, Epsilon: 0.100\n",
      "72\n",
      "Episode 8040/10000, Avg Reward: -6805.8350, Epsilon: 0.100\n",
      "72\n",
      "Episode 8060/10000, Avg Reward: -7653.1350, Epsilon: 0.100\n",
      "72\n",
      "Episode 8080/10000, Avg Reward: -7112.6400, Epsilon: 0.100\n",
      "72\n",
      "Episode 8100/10000, Avg Reward: -7458.8750, Epsilon: 0.100\n",
      "72\n",
      "Episode 8120/10000, Avg Reward: -6711.0250, Epsilon: 0.100\n",
      "72\n",
      "Episode 8140/10000, Avg Reward: -6838.3000, Epsilon: 0.100\n",
      "72\n",
      "Episode 8160/10000, Avg Reward: -7154.3600, Epsilon: 0.100\n",
      "72\n",
      "Episode 8180/10000, Avg Reward: -7109.2050, Epsilon: 0.100\n",
      "72\n",
      "Episode 8200/10000, Avg Reward: -6724.8400, Epsilon: 0.100\n",
      "72\n",
      "Episode 8220/10000, Avg Reward: -6779.1750, Epsilon: 0.100\n",
      "72\n",
      "Episode 8240/10000, Avg Reward: -6292.1350, Epsilon: 0.100\n",
      "72\n",
      "Episode 8260/10000, Avg Reward: -6255.4650, Epsilon: 0.100\n",
      "72\n",
      "Episode 8280/10000, Avg Reward: -7562.2600, Epsilon: 0.100\n",
      "72\n",
      "Episode 8300/10000, Avg Reward: -6815.9150, Epsilon: 0.100\n",
      "72\n",
      "Episode 8320/10000, Avg Reward: -7428.0000, Epsilon: 0.100\n",
      "72\n",
      "Episode 8340/10000, Avg Reward: -7368.4200, Epsilon: 0.100\n",
      "72\n",
      "Episode 8360/10000, Avg Reward: -7124.5250, Epsilon: 0.100\n",
      "72\n",
      "Episode 8380/10000, Avg Reward: -7156.5450, Epsilon: 0.100\n",
      "72\n",
      "Episode 8400/10000, Avg Reward: -7439.3350, Epsilon: 0.100\n",
      "72\n",
      "Episode 8420/10000, Avg Reward: -7088.1200, Epsilon: 0.100\n",
      "72\n",
      "Episode 8440/10000, Avg Reward: -6716.8700, Epsilon: 0.100\n",
      "72\n",
      "Episode 8460/10000, Avg Reward: -7097.4150, Epsilon: 0.100\n",
      "72\n",
      "Episode 8480/10000, Avg Reward: -7660.2700, Epsilon: 0.100\n",
      "72\n",
      "Episode 8500/10000, Avg Reward: -7478.6650, Epsilon: 0.100\n",
      "72\n",
      "Episode 8520/10000, Avg Reward: -7515.5950, Epsilon: 0.100\n",
      "72\n",
      "Episode 8540/10000, Avg Reward: -7834.9500, Epsilon: 0.100\n",
      "72\n",
      "Episode 8560/10000, Avg Reward: -6464.4600, Epsilon: 0.100\n",
      "72\n",
      "Episode 8580/10000, Avg Reward: -6276.6350, Epsilon: 0.100\n",
      "72\n",
      "Episode 8600/10000, Avg Reward: -7790.5900, Epsilon: 0.100\n",
      "72\n",
      "Episode 8620/10000, Avg Reward: -7231.7350, Epsilon: 0.100\n",
      "72\n",
      "Episode 8640/10000, Avg Reward: -6722.7400, Epsilon: 0.100\n",
      "72\n",
      "Episode 8660/10000, Avg Reward: -7069.5650, Epsilon: 0.100\n",
      "72\n",
      "Episode 8680/10000, Avg Reward: -7842.3600, Epsilon: 0.100\n",
      "72\n",
      "Episode 8700/10000, Avg Reward: -7151.3700, Epsilon: 0.100\n",
      "72\n",
      "Episode 8720/10000, Avg Reward: -6826.4050, Epsilon: 0.100\n",
      "72\n",
      "Episode 8740/10000, Avg Reward: -7151.2700, Epsilon: 0.100\n",
      "72\n",
      "Episode 8760/10000, Avg Reward: -6463.3250, Epsilon: 0.100\n",
      "72\n",
      "Episode 8780/10000, Avg Reward: -7515.8650, Epsilon: 0.100\n",
      "72\n",
      "Episode 8800/10000, Avg Reward: -6613.9400, Epsilon: 0.100\n",
      "72\n",
      "Episode 8820/10000, Avg Reward: -6568.4200, Epsilon: 0.100\n",
      "72\n",
      "Episode 8840/10000, Avg Reward: -5503.6400, Epsilon: 0.100\n",
      "72\n",
      "Episode 8860/10000, Avg Reward: -7179.2000, Epsilon: 0.100\n",
      "72\n",
      "Episode 8880/10000, Avg Reward: -6695.8700, Epsilon: 0.100\n",
      "72\n",
      "Episode 8900/10000, Avg Reward: -6093.6700, Epsilon: 0.100\n",
      "72\n",
      "Episode 8920/10000, Avg Reward: -6303.3300, Epsilon: 0.100\n",
      "72\n",
      "Episode 8940/10000, Avg Reward: -6703.6200, Epsilon: 0.100\n",
      "72\n",
      "Episode 8960/10000, Avg Reward: -6439.1350, Epsilon: 0.100\n",
      "72\n",
      "Episode 8980/10000, Avg Reward: -6808.1300, Epsilon: 0.100\n",
      "72\n",
      "Episode 9000/10000, Avg Reward: -6110.0950, Epsilon: 0.100\n",
      "72\n",
      "Episode 9020/10000, Avg Reward: -5810.4150, Epsilon: 0.100\n",
      "72\n",
      "Episode 9040/10000, Avg Reward: -7817.8600, Epsilon: 0.100\n",
      "72\n",
      "Episode 9060/10000, Avg Reward: -7558.2500, Epsilon: 0.100\n",
      "72\n",
      "Episode 9080/10000, Avg Reward: -7183.7900, Epsilon: 0.100\n",
      "72\n",
      "Episode 9100/10000, Avg Reward: -7756.2550, Epsilon: 0.100\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "q_table = {}\n",
    "epsilon = hyperparameters[\"epsilon_start\"]\n",
    "\n",
    "def get_state(obs, target_loc=None, has_picked_up=False):\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col, stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\t\n",
    "\n",
    "\tassert target_loc is not None\n",
    "\tx_dir = target_loc[0] - taxi_row\n",
    "\ty_dir = target_loc[1] - taxi_col\n",
    "\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\treturn (x_dir, y_dir, obstacle_north, obstacle_south, obstacle_east, obstacle_west, has_picked_up)\n",
    "  \n",
    "def get_action(obs):\n",
    "\t\"\"\"\n",
    "\t# Selects the best action using the trained Q-table.\n",
    "\t\"\"\"\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = np.random.choice(action_nums)  # Random action\n",
    "\telse:\n",
    "\t\taction = np.argmax(q_table[get_state(obs)])  # Greedy action\n",
    "\treturn action\n",
    "\n",
    "def is_in_station(obs):\n",
    "\t\"\"\"\n",
    "\t# Checks if the taxi is in a station.\n",
    "\t\"\"\"\n",
    "\tstations = [[0, 0], [0, 4], [4, 0], [4,4]]\n",
    "\ttaxi_row, taxi_col,stations[0][0],stations[0][1] ,stations[1][0],stations[1][1],stations[2][0],stations[2][1],stations[3][0],stations[3][1],obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [tuple(i) for i in stations]\n",
    "\treturn (taxi_row, taxi_col) in stations\n",
    "\n",
    "env = SimpleTaxiEnv(**env_config)\n",
    "action_nums = 6\n",
    "rewards_per_episode = []\n",
    "\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "step_count = 0\n",
    "stations = [(0, 0), (0, 4), (4, 0), (4,4)]\n",
    "\n",
    "taxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\n",
    "if render:\n",
    "\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\taction=None, step=step_count, fuel=env.current_fuel)\n",
    "\ttime.sleep(0.5)\n",
    " \n",
    "\n",
    "for episode in range(hyperparameters[\"episodes\"]):\n",
    "\tget_state.passenger_loc, get_state.destination_loc = None, None\n",
    "\tobs, _ = env.reset()\n",
    "\tdone = False\n",
    "\ttotal_reward = 0\n",
    "\tstep_count = 0\n",
    " \n",
    "\tdestination = None\n",
    "\tvisited = []\n",
    "\thas_picked_up = False\n",
    " \n",
    "\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\tstations = [(obs[2], obs[3]), (obs[4], obs[5]), (obs[6], obs[7]), (obs[8], obs[9])]\n",
    "\ttarget_loc = stations[0]\n",
    "\tstate = get_state(obs, target_loc, has_picked_up)\n",
    "\t\n",
    "\twhile not done:\t\n",
    "\t\tif state not in q_table:\n",
    "\t\t\tq_table[state] = np.zeros(action_nums)\n",
    "   \n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = obs\n",
    "\t\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\t\taction_probs = np.ones(action_nums) / action_nums\n",
    "\t\t\tif obstacle_south:\n",
    "\t\t\t\taction_probs[0] = 0\n",
    "\t\t\tif obstacle_north:\n",
    "\t\t\t\taction_probs[1] = 0\n",
    "\t\t\tif obstacle_east:\n",
    "\t\t\t\taction_probs[2] = 0\n",
    "\t\t\tif obstacle_west:\n",
    "\t\t\t\taction_probs[3] = 0\t\n",
    "\t\t\tif not passenger_look or has_picked_up or not is_in_station(obs):\n",
    "\t\t\t\taction_probs[4] = 0\n",
    "\t\t\tif not destination_look or not has_picked_up or not is_in_station(obs):\n",
    "\t\t\t\taction_probs[5] = 0\n",
    "\t\t\taction_probs = action_probs / np.sum(action_probs)\n",
    "\t\t\taction = np.random.choice(action_nums, p=action_probs)  # Random action\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(q_table[state])  # Greedy action\n",
    "\t\t\n",
    "\t\tif not has_picked_up and passenger_look and is_in_station(obs) and action == 4:\n",
    "\t\t\thas_picked_up = True\n",
    "\t\tif has_picked_up and destination_look and is_in_station(obs) and action == 5:\n",
    "\t\t\tdone = True\n",
    "   \n",
    "\t\tshaped_reward = 0\n",
    "\t\tx_dir = target_loc[0] - taxi_row\n",
    "\t\ty_dir = target_loc[1] - taxi_col\n",
    "\t\tx_dir = 0 if x_dir == 0 else x_dir // abs(x_dir)\n",
    "\t\ty_dir = 0 if y_dir == 0 else y_dir // abs(y_dir)\n",
    "\t\tif x_dir == 1 and action == 1 and not obstacle_north:\n",
    "\t\t\tshaped_reward = 1000\n",
    "\t\tif x_dir == -1 and action == 0 and not obstacle_south:\n",
    "\t\t\tshaped_reward = 1000\n",
    "\t\tif y_dir == 1 and action == 3 and not obstacle_west:\n",
    "\t\t\tshaped_reward = 1000\n",
    "\t\tif y_dir == -1 and action == 2 and not obstacle_east:\n",
    "\t\t\tshaped_reward = 1000\n",
    "\t\tif action == 4 and passenger_look and not has_picked_up:\n",
    "\t\t\tshaped_reward = 5000\n",
    "\t\tif action == 5 and destination_look and has_picked_up:\n",
    "\t\t\tshaped_reward = 10000\n",
    "\t\tif obstacle_south and action == 0:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\t\t# print (\"south\")\n",
    "\t\tif obstacle_north and action == 1:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\t\t# print (\"north\")\n",
    "\t\tif obstacle_east and action == 2:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\t\t# print (\"east\")\n",
    "\t\tif obstacle_west and action == 3:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\t\t# print (\"west\")\n",
    "\t\tif (not passenger_look or has_picked_up or not is_in_station(obs)) and action == 4:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\t\t# print (\"not passenger\")\n",
    "\t\tif (not destination_look or not has_picked_up or not is_in_station(obs)) and action == 5:\n",
    "\t\t\tshaped_reward = -10000\n",
    "\t\t\t# print (\"not destination\")\n",
    "   \n",
    "\t\tnext_obs, reward, done, _ = env.step(action)\n",
    "\t\ttaxi_row, taxi_col, _,_,_,_,_,_,_,_,obstacle_north, obstacle_south, obstacle_east, obstacle_west, passenger_look, destination_look = next_obs\n",
    "  \n",
    "\t\tif is_in_station(next_obs) and (taxi_row, taxi_col) not in visited:\n",
    "\t\t\tvisited.append((taxi_row, taxi_col))\n",
    "\t\t\tif destination_look:\n",
    "\t\t\t\tdestination = (taxi_row, taxi_col)\n",
    "\t\t\tif has_picked_up and destination is not None:\n",
    "\t\t\t\ttarget_loc = destination\n",
    "\t\t\telse:\n",
    "\t\t\t\t# choose a station that has not been visited yet\n",
    "\t\t\t\t# print (visited, (taxi_row, taxi_col))\n",
    "\t\t\t\tfor station in stations:\n",
    "\t\t\t\t\tif station not in visited:\n",
    "\t\t\t\t\t\ttarget_loc = station\n",
    "\t\t\t\t\t\tbreak\t\n",
    "\t\t\t\t# print (target_loc)\n",
    "\n",
    "\t\t# if done:\n",
    "\t\t# \tif not has_picked_up:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not picked up\", next_obs, action, has_picked_up, destination)\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and not destination_look:\n",
    "\t\t# \t\t# shaped_reward = -100\n",
    "\t\t# \t\t# print (\"not destination\")\n",
    "\t\t# \t\tdone = False\n",
    "\t\t# \tif has_picked_up and destination_look:\n",
    "\t\t# \t\t# print (\"done\")\n",
    "\t\t# \t\tshaped_reward = 2000\n",
    "\t\t\n",
    "\t\ttotal_reward += reward\n",
    "\t\treward += shaped_reward\n",
    "\t\tnext_state = get_state(next_obs, target_loc, has_picked_up)\n",
    "\t\tif next_obs not in q_table:\n",
    "\t\t\tq_table[next_state] = np.zeros(action_nums)\n",
    "\t\tq_table[state][action] += hyperparameters[\"alpha\"] * (reward + hyperparameters[\"gamma\"] * np.max(q_table[next_state]) - q_table[state][action])\n",
    "\t\t\n",
    "\t\tstep_count += 1\n",
    "\t\tobs = next_obs\n",
    "\t\tstate = next_state\n",
    "   \n",
    "\t\n",
    "\t\tif render:\n",
    "\t\t\tenv.render_env((taxi_row, taxi_col),\n",
    "\t\t\t\t\t\t\taction=action, step=step_count, fuel=env.current_fuel)\n",
    "\t# print (step_count)\n",
    "\trewards_per_episode.append(total_reward)\n",
    "\tepsilon = max(hyperparameters[\"epsilon_end\"], epsilon * hyperparameters[\"decay_rate\"])\n",
    "\tif (episode + 1) % 20 == 0:\n",
    "\t\tavg_reward = np.mean(rewards_per_episode[-20:])\n",
    "\t\tprint(f'Episode {episode + 1}/{hyperparameters[\"episodes\"]}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.3f}')\n",
    "\t\t# print ([np.argmax(i) for i in q_table.values()])\n",
    "\t\tprint (len(q_table))\n",
    "  \n",
    "filename = \"q_table.pkl\"\n",
    "\n",
    "# 儲存 Q-table\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "print(f\"Q-table 已儲存至 {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
